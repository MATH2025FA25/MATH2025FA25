---
title: "SLR: Conditions"
author: "Prof. Eric Friedlander"
footer: "[ðŸ”— MAT 212 - Winter 2025 -  Schedule](https://mat212wi25.netlify.app/schedule)"
logo: "../images/logo.png"
format: 
  revealjs:
    theme: slides.scss
    multiplex: false
    transition: fade
    slide-number: false
    incremental: false 
    chalkboard: true
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
execute:
  freeze: auto
  echo: true
  cache: false
knitr:
  opts_chunk: 
    R.options:      
    width: 200
bibliography: references.bib  
---

```{r setup}
#| include: false

library(countdown)

knitr::opts_chunk$set(
  fig.width = 8,
  fig.asp = 0.618,
  fig.retina = 3,
  dpi = 300,
  out.width = "80%",
  fig.align = "center"
)
```

# Application exercise

::: appex
ðŸ“‹ [AE 05 - Model Conditions](/ae/ae-05-conditions.qmd)
:::

Complete Exercise 0.

## Computational set up

```{r packages}
#| echo: true
#| message: false

# load packages
library(tidyverse)   # for data wrangling and visualization
library(ggformula)   # for plotting using formulas
library(broom)       # for formatting model output
library(scales)      # for pretty axis labels
library(knitr)       # for pretty tables
library(kableExtra)  # also for pretty tables
library(patchwork)   # arrange plots

# Spotify Dataset
spotify <- read_csv("../data/spotify-popular.csv")

# set default theme and larger font size for ggplot2
ggplot2::theme_set(ggplot2::theme_bw(base_size = 20))
```

## Quick Data Cleaning

```{r}
#| echo: true

spotify <- spotify |> 
  mutate(duration_min = duration_ms / 60000)
```

::: question
- What is this code doing?
- Why might I be doing it?
:::

## The regression model, revisited {.smaller}

```{r}
#| echo: true
spotify_fit <- lm(danceability ~ duration_min, data = spotify)

tidy(spotify_fit, conf.int = TRUE, conf.level = 0.95) |>
   kable(digits = 3)
```

- There is strong statistical evidence that there is a linear relationship between the duration of a song and it's danceability.

- We are 95% confidence that as the length of a song increases by one minute the danceability will derease by between 0.009 and 0.039 units.

## Mathematical representation, visualized {.midi}

$$
Y|X \sim N(\beta_0 + \beta_1 X, \sigma_\epsilon^2)
$$

![Image source: *Introduction to the Practice of Statistics (5th ed)*](images/04/regression.png){fig-align="center"}

# Model conditions

## Model conditions

1.  **Linearity:** There is a linear relationship between the outcome and predictor variables
2.  **Constant variance:** The variability of the errors is equal for all values of the predictor variable
3.  **Normality:** The errors follow a normal distribution
4.  **Independence:** The errors are independent from each other

## WARNING {.smaller}

- All of these assumptions are for the population
- We want to determine whether they are met from your data
- In real life, these conditions are *almost always* violated in one way or another
- Questions you should ask yourself:
    + Are my conditions close enough to being satisfied that I can trust the results of my inference
    + Do I have reason to believe that my conditions are *GROSSLY* violated?
    + Based on what I see, how trustworthy do I think the results of my inference are.
    
## ENGAGE: SOAP BOX {.smaller}

Statistics and numbers are often used to make arguments seem more "rigorous" or infallable. I'm sure you've heard the phrase "the numbers are the numbers" or "you can't argue with the numbers". More often than not, this is **BULLSHIT**. Most data analyses involve making decision which are subjective. The interpretability of any form of statistical inference is heavily influenced by whether the assumptions and conditions of that inference is met, which is almost never is. It is up to the practitioner to determine whether those conditions are met and what impact those conditions have on the results of those analyses. In my work, I rarely encounter practicitions who even know what the conditions are, let alone understand why they are important. **FURTHERMORE!!!** the quality of your analysis is only as good as the quality of your data. Remember a crap study design will yield crap data which will yield crappy analysis.


<!-- ## Linearity -->

<!-- -   If the linear model, $\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1x_i$, adequately describes the relationship between $X$ and $Y$, then the residuals should reflect random (chance) error -->

<!-- -   To assess this, we can look at a plot of the residuals vs. the fitted values -->

<!-- -   **Linearity satisfied** if there is no distinguishable pattern in the residuals plot, i.e. the residuals should be randomly scattered -->

<!-- -   A non-random pattern (e.g. a parabola) suggests a linear model does not adequately describe the relationship between $X$ and $Y$ -->

<!-- ## Linearity -->

<!-- âœ… The residuals vs. fitted values plot should show a random scatter of residuals (no distinguishable pattern or structure) -->

<!-- ```{r res-vs-fit} -->
<!-- #| echo: false -->
<!-- heb_aug <- augment(heb_fit) -->

<!-- gf_point(.resid ~ .fitted, data = heb_aug) |>  -->
<!--   gf_hline(yintercept = 0, linetype = "dashed") |>  -->
<!--   gf_labs( -->
<!--     x = "Fitted value", y = "Residual", -->
<!--     title = "Residuals vs. fitted values" -->
<!--   ) -->
<!-- ``` -->


<!-- ## The augment function {.smaller} -->

<!-- ```{r} -->
<!-- heb_aug <- augment(heb_fit) -->

<!-- head(heb_aug) |> kable() -->
<!-- ``` -->

<!-- ## Residuals vs. fitted values (code) -->

<!-- ```{r} -->
<!-- #| echo: true -->
<!-- #| ref.label: "res-vs-fit" -->
<!-- #| fig.show: "hide" -->
<!-- ``` -->

<!-- ## Non-linear relationships -->

<!-- ```{r} -->
<!-- #| echo: false -->
<!-- set.seed(1234) -->

<!-- n = 100 -->

<!-- df <- tibble( -->
<!--   x = -49:50, -->
<!--   e_curved = rnorm(n, 0, 150), -->
<!--   y_curved = x^2 + e_curved, -->
<!--   e_slight_curve = sort(rbeta(n, 5, 1) * 200) + rnorm(n, 0, 5), -->
<!--   y_slight_curve = x + e_slight_curve, -->
<!--   x_fan = seq(0, 3.99, 4 / n), -->
<!--   y_fan = c(rnorm(n / 8, 3, 1), rnorm(n / 8, 3.5, 2), rnorm(n / 8, 4, 2.5), rnorm(n / 8, 4.5, 3), rnorm(n / 4, 5, 4), rnorm((n / 4) + 2, 6, 5)) -->
<!-- ) -->
<!-- ``` -->

<!-- ::: columns -->
<!-- ::: {.column width="50%"} -->
<!-- ```{r} -->
<!-- #| out.width: "100%" -->
<!-- #| fig.asp: 1.2 -->
<!-- #| echo: false -->

<!-- p1 <- ggplot(df, aes(x = x, y = y_curved)) + -->
<!--   geom_point() + -->
<!--   geom_smooth(method = "lm", se = FALSE, color = "#8F2D56") + -->
<!--   labs( -->
<!--     x = "X", y = "Y", -->
<!--     title = "Observed data + model" -->
<!--     ) -->

<!-- curved_fit <- lm(y_curved ~ x, data = df) -->

<!-- curved_aug <- augment(curved_fit) -->

<!-- p2 <- ggplot(curved_aug, aes(x = .fitted, y = .resid)) + -->
<!--   geom_point() + -->
<!--   geom_hline(yintercept = 0, linetype = "dashed") + -->
<!--   labs( -->
<!--     x = "Fitted value", y = "Residual", -->
<!--     title = "Residuals vs. fitted values" -->
<!--   ) + -->
<!--   ylim(-2000, 2000) -->

<!-- p1 / p2 + -->
<!--   plot_annotation(title = "Obviously curved") -->
<!-- ``` -->
<!-- ::: -->

<!-- ::: {.column width="50%"} -->
<!-- ```{r} -->
<!-- #| out.width: "100%" -->
<!-- #| fig.asp: 1.2 -->
<!-- #| echo: false -->

<!-- p1 <- ggplot(df, aes(x = x, y = y_slight_curve)) + -->
<!--   geom_point() + -->
<!--   geom_smooth(method = "lm", se = FALSE, color = "#8F2D56") + -->
<!--   labs( -->
<!--     x = "X", y = "Y", -->
<!--     title = "Observed data + model" -->
<!--     ) -->

<!-- slight_curve_fit <- lm(y_slight_curve ~ x, data = df) -->

<!-- slight_curve_aug <- augment(slight_curve_fit) -->

<!-- p2 <- ggplot(slight_curve_aug, aes(x = .fitted, y = .resid)) + -->
<!--   geom_point() + -->
<!--   geom_hline(yintercept = 0, linetype = "dashed") + -->
<!--   labs( -->
<!--     x = "Fitted value", y = "Residual", -->
<!--     title = "Residuals vs. fitted values" -->
<!--   ) -->

<!-- p1 / p2 + -->
<!--   plot_annotation(title = "Not so obviously curved") -->
<!-- ``` -->
<!-- ::: -->
<!-- ::: -->

<!-- ## Constant variance {.midi} -->

<!-- -   If the spread of the distribution of $Y$ is equal for all values of $X$ then the spread of the residuals should be approximately equal for each value of $X$ -->

<!-- -   To assess this, we can look at a plot of the residuals vs. the fitted values -->

<!-- -   **Constant variance satisfied** if the vertical spread of the residuals is approximately equal as you move from left to right (i.e. there is no "fan" pattern) -->

<!-- -   A fan pattern suggests the constant variance assumption is not satisfied and transformation or some other remedy is required (more on this later in the semester) -->

<!-- -   **CAREFUL**: Inconsistent distribution of $X$s can make it seem as if there is non-constant variance -->

<!-- ## Constant variance -->

<!-- âœ… The vertical spread of the residuals is relatively constant across the plot -->

<!-- ```{r} -->
<!-- #| ref.label: "res-vs-fit" -->
<!-- #| echo: false -->
<!-- ``` -->

<!-- ## Non-constant variance -->

<!-- ::: columns -->
<!-- ::: {.column width="50%"} -->
<!-- ```{r} -->
<!-- #| out.width: "100%" -->
<!-- #| echo: false -->
<!-- ggplot(df, aes(x = x_fan, y = y_fan)) + -->
<!--   geom_point() + -->
<!--   geom_smooth(method = "lm", se = FALSE, color = "#8F2D56") + -->
<!--   labs( -->
<!--     x = "X", y = "Y", -->
<!--     title = "Observed data + model" -->
<!--     ) -->
<!-- ``` -->
<!-- ::: -->

<!-- ::: {.column width="50%"} -->
<!-- ```{r} -->
<!-- #| out.width: "100%" -->
<!-- #| echo: false -->

<!-- fan_fit <- lm(y_fan ~ x_fan, data = df) -->

<!-- fan_aug <- augment(fan_fit) -->

<!-- ggplot(fan_aug, aes(x = .fitted, y = .resid)) + -->
<!--   geom_point() + -->
<!--   geom_hline(yintercept = 0, linetype = "dashed") + -->
<!--   labs( -->
<!--     x = "Fitted value", y = "Residual", -->
<!--     title = "Residuals vs. fitted values" -->
<!--   ) + -->
<!--   ylim(-15, 15) -->
<!-- ``` -->
<!-- ::: -->
<!-- ::: -->

<!-- - Think: Is my error/variance proportional to the thing I'm predicting? -->


<!-- ## Normality {.midi} -->

<!-- -   The linear model assumes that the distribution of $Y$ is Normal for every value of $X$ -->

<!-- -   This is impossible to check in practice, so we will look at the overall distribution of the residuals to assess if the normality assumption is satisfied -->

<!-- -   **Normality satisfied** if a histogram of the residuals is approximately normal -->

<!--     -   Can also check that the points on a normal QQ-plot falls along a diagonal line -->

<!-- -   Most inferential methods for regression are robust to some departures from normality, so we can proceed with inference if the sample size is sufficiently large, $n > 30$ -->

<!-- ## Normality -->

<!-- ```{r} -->
<!-- #| echo: false -->

<!-- ggplot(heb_aug, aes(x = .resid)) + -->
<!--   geom_histogram(bins = 7, color = "white") + -->
<!--   labs( -->
<!--     x = "Residual", -->
<!--     y = "Count", -->
<!--     title = "Histogram of residuals" -->
<!--   ) -->
<!-- ``` -->

<!-- ## Check normality using a QQ-plot -->

<!-- ::: columns -->
<!-- ::: {.column width="50%"} -->
<!-- ```{r} -->
<!-- #| code-fold: true -->
<!-- gf_histogram(~.resid, data = heb_aug, -->
<!--              bins=7, color = "white") |>  -->
<!--   gf_labs( -->
<!--     x = "Residual", -->
<!--     y = "Count", -->
<!--     title = "Histogram of residuals" -->
<!--   ) -->
<!-- ``` -->

<!-- ```          -->
<!-- ``` -->
<!-- ::: -->

<!-- ::: {.column width="50%"} -->
<!-- ```{r} -->
<!-- #| code-fold: true -->
<!-- gf_qq(~.resid, data = heb_aug) |>  -->
<!--   gf_qqline() |>   -->
<!--   gf_labs(x = "Theoretical quantile",  -->
<!--        y = "Observed quantile",  -->
<!--        title = "Normal QQ-plot of residuals") -->
<!-- ``` -->
<!-- ::: -->
<!-- ::: -->

<!-- -   Assess whether residuals lie along the diagonal line of the Quantile-quantile plot (QQ-plot). -->

<!-- -   If so, the residuals are normally distributed. -->

<!-- ## Normality -->

<!-- ```{r} -->
<!-- #| echo: false -->
<!-- #| out.width: "80%" -->

<!-- ggplot(heb_aug, aes(sample = .resid)) + -->
<!--   stat_qq()+ -->
<!--   stat_qq_line() +  -->
<!--   labs(x = "Theoretical quantile",  -->
<!--        y = "Observed quantile",  -->
<!--        title = "Normal QQ-plot of residuals") -->
<!-- ``` -->

<!-- âŒ The residuals do not appear to follow a normal distribution, because the points do not lie on the diagonal line, so normality is not satisfied. -->

<!-- âœ… The sample size  $n =  37 > 30$, so the sample size is large enough to relax this condition and proceed with inference. -->


<!-- ## Independence {.midi} -->

<!-- -   We can often check the independence assumption based on the context of the data and how the observations were collected -->

<!-- -   Two common violations of the independence assumption: -->

<!--     -   **Serial Effect**: If the data were collected over time, plot the residuals in time order to see if there is a pattern (serial correlation) -->

<!--     -   **Cluster Effect**: If there are subgroups represented in the data that are not accounted for in the model (e.g., type of supermarket), you can color the points in the residual plots by group to see if the model systematically over or under predicts for a particular subgroup -->

<!-- ## Independence {.nonincremental} -->

<!-- Recall the description of the data: -->

<!-- > -   Average household income (per zip code) and number of organic vegetable offerings in San Antonio, TX -->
<!-- > -->
<!-- > -   Data from HEB website, compiled by high school student Linda Saucedo, Fall 2019 -->

<!-- <br> -->

<!-- âŒ Based on the information we have,  it's  unclear if the data are independent. In fact, I'd guess that they are likely geographically correlated. -->

<!-- ## Recap -->

<!-- Used residual plots to check conditions for SLR: -->

<!-- ::: columns -->
<!-- ::: {.column width="50%"} -->
<!-- ::: nonincremental -->
<!-- -   Linearity -->
<!-- -   Constant variance -->
<!-- ::: -->
<!-- ::: -->

<!-- ::: {.column width="50%"} -->
<!-- ::: nonincremental -->
<!-- -   Normality -->
<!-- -   Independence -->
<!-- ::: -->
<!-- ::: -->
<!-- ::: -->

<!-- . . . -->

<!-- ::: question -->
<!-- 1. Which of these conditions are required for fitting a SLR (and not doing any inference)?  -->
<!-- 2. Which for simulation-based inference for the slope for an SLR?  -->
<!-- 3. Which for inference with mathematical models? -->
<!-- ::: -->

<!-- ```{r} -->
<!-- #| echo: false -->
<!-- countdown(minutes = 3, font_size = "2em") -->
<!-- ``` -->

