---
title: "Model comparison"
author: "Prof. Eric Friedlander"
footer: "[ðŸ”— MAT 212 - Winter 2025 -  Schedule](https://mat212wi25.netlify.app/schedule)"
logo: "../images/logo.png"
format: 
  revealjs:
    theme: slides.scss
    multiplex: false
    transition: fade
    slide-number: false
    incremental: false 
    chalkboard: true
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
execute:
  freeze: auto
  echo: true
  cache: false
knitr:
  opts_chunk: 
    R.options:      
    width: 200
bibliography: references.bib  
---

```{r}
#| include: false

# figure options
knitr::opts_chunk$set(
  fig.width = 10, fig.asp = 0.618,
  fig.retina = 3, dpi = 300, fig.align = "center"
)
```

## Announcements

::: appex
ðŸ“‹ [AE 12 - Model Comparison](/ae/ae-12-comparison.qmd)

- Open up AE 12 and complete Exercises 0-2
:::


## Topics

::: nonincremental
-   ANOVA for multiple linear regression and sum of squares
-   Comparing models with $R^2$ vs. $R^2_{ajd}$ 
-   Comparing models with AIC and BIC
-   Occam's razor and parsimony
:::


## Computational setup

```{r}
#| echo: true

# load packages
library(tidyverse)
library(broom)
library(yardstick)
library(ggformula)
library(supernova)
library(tidymodels)
library(patchwork)
library(knitr)
library(janitor)
library(kableExtra)

# set default theme and larger font size for ggplot2
ggplot2::theme_set(ggplot2::theme_bw(base_size = 20))
```

# Introduction

## Data: Restaurant tips

Which variables help us predict the amount customers tip at a restaurant?

```{r}
#| echo: false
#| message: false
tips <- read_csv("../data/tip-data.csv") |>
  drop_na(Party)
```

```{r}
#| echo: false
tips |>
  select(Tip, Party, Meal, Age)
```

## Variables

**Predictors**:

::: nonincremental
-   `Party`: Number of people in the party
-   `Meal`: Time of day (`Lunch`, `Dinner`, `Late Night`)
-   `Age`: Age category of person paying the bill (`Yadult`, `Middle`, `SenCit`)
:::

**Outcome**: `Tip`: Amount of tip

## Outcome: `Tip`

```{r}
#| echo: false
ggplot(tips, aes(x = Tip)) +
  geom_histogram(binwidth = 1) +
  labs(title = "Distribution of tips")
```

## Predictors

```{r}
#| echo: false
p1 <- ggplot(tips, aes(x = Party)) +
  geom_histogram(binwidth = 1) +
  labs(title = "Number of people in party")

p2 <- ggplot(tips, aes(x = Meal)) +
  geom_bar() +
  labs(title = "Meal type")

p3 <- ggplot(tips, aes(x = Age)) +
  geom_bar() +
  labs(title = "Age of payer")

p1 + (p2 / p3)
```

## Relevel categorical predictors

```{r}
#| echo: true

tips <- tips |>
  mutate(
    Meal = fct_relevel(Meal, "Lunch", "Dinner", "Late Night"),
    Age  = fct_relevel(Age, "Yadult", "Middle", "SenCit")
  )
```

## Predictors, again

```{r}
#| echo: false
p1 <- ggplot(tips, aes(x = Party)) +
  geom_histogram(binwidth = 1) +
  labs(title = "Number of people in party")

p2 <- ggplot(tips, aes(x = Meal, fill = Meal)) +
  geom_bar() +
  labs(title = "Meal type") +
  scale_fill_viridis_d(end = 0.8)

p3 <- ggplot(tips, aes(x = Age, fill = Age)) +
  geom_bar() +
  labs(title = "Age of payer") +
  scale_fill_viridis_d(option = "E", end = 0.8)

p1 + (p2 / p3)

```

## Outcome vs. predictors

```{r}
#| echo: false
#| fig.width: 12
#| fig.height: 4

p4 <- ggplot(tips, aes(x = Party, y = Tip)) +
  geom_point(color = "#5B888C")

p5 <- ggplot(tips, aes(x = Meal, y = Tip, fill = Meal)) +
  geom_boxplot(show.legend = FALSE) +
  scale_fill_viridis_d(end = 0.8)

p6 <- ggplot(tips, aes(x = Age, y = Tip, fill = Age)) +
  geom_boxplot(show.legend = FALSE) +
  scale_fill_viridis_d(option = "E", end = 0.8)

p4 + p5 + p6
```

## Fit and summarize model {.midi}

```{r}
#| echo: false

tip_fit <- lm(Tip ~ Party + Age, data = tips)

tidy(tip_fit) |>
  kable(digits = 3)
```

. . .

<br>

::: question
Is this model good?
:::

## Another model summary

```{r}
anova(tip_fit) |>
  tidy() |>
  kable(digits = 2)
```

# Analysis of variance (ANOVA)

## Analysis of variance (ANOVA)

<br>

![](images/12/model-anova.png){fig-align="center"}

## ANOVA {.smaller}

-   **Main Idea:** Decompose the total variation of the outcome into:
    -   the variation that can be explained by the each of the variables in the model
    -   the variation that **can't** be explained by the model (left in the residuals)
-   $SS_{Total}$: Total sum of squares, variability of outcome, $\sum_{i = 1}^n (y_i - \bar{y})^2$
-   $SS_{Error}$: Residual sum of squares, variability of residuals, $\sum_{i = 1}^n (y_i - \hat{y}_i)^2$
-   $SS_{Model} = SS_{Total} - SS_{Error}$: Variability explained by the model, $\sum_{i = 1}^n (\hat{y}_i - \bar{y})^2$

 . . .
 
 Complete Exercise 3.

## ANOVA output in R[^1]

[^1]: [Click here](anova-table.html) for explanation about the way R calculates sum of squares for each variable.

```{r}
#| echo: false
anova(tip_fit) |>
  tidy() |>
  kable()
```

## ANOVA output, with totals

```{r}
#| echo: FALSE
anova(tip_fit) |>
  tidy() |>
  mutate(across(where(is.numeric), round, 2)) |>
  janitor::adorn_totals(where = "row", cols = 1:3, fill = "") |>
  mutate(
    statistic = if_else(is.na(statistic), "", statistic),
    p.value = if_else(is.na(p.value), "", p.value)
    ) |>
  kable()
```

## Sum of squares {.smaller}

::: columns
::: {.column width="50%"}
```{r}
#| echo: false
anova(tip_fit) |>
  tidy() |>
  mutate(across(where(is.numeric), round, 2)) |>
  select(term, df, sumsq) |>
  janitor::adorn_totals(where = "row", cols = 1:3, fill = "") |>
  kable() |>
  column_spec(3, background = "#D9E3E4")
```
:::

::: {.column width="50%"}
-   $SS_{Total}$: Total sum of squares, variability of outcome, $\sum_{i = 1}^n (y_i - \bar{y})^2$
-   $SS_{Error}$: Residual sum of squares, variability of residuals, $\sum_{i = 1}^n (y_i - \hat{y}_i)^2$
-   $SS_{Model} = SS_{Total} - SS_{Error}$: Variability explained by the model, $\sum_{i = 1}^n (\hat{y}_i - \bar{y})^2$
:::
:::


## Sum of squares: $SS_{Total}$

```{r}
#| echo: false
anova_df <- anova(tip_fit) |>
  tidy() |>
  mutate(across(where(is.numeric), round, 2)) |>
  select(term, df, sumsq) 

anova_df |>
  janitor::adorn_totals(where = "row", cols = 1:3, fill = "") |>
  kable() |>
  row_spec(4, background = "#D9E3E4")
```

<br>

<center>

$SS_{Total}$: Total sum of squares, variability of outcome

<br>

$\sum_{i = 1}^n (y_i - \bar{y})^2$ = `r sum(anova_df$sumsq[1:3])`

</center>

## Sum of squares: $SS_{Error}$

```{r}
#| echo: false
anova_df <- anova(tip_fit) |>
  tidy() |>
  mutate(across(where(is.numeric), round, 2)) |>
  select(term, df, sumsq) 

anova_df |>
  janitor::adorn_totals(where = "row", cols = 1:3, fill = "") |>
  kable() |>
  row_spec(3, background = "#D9E3E4")
```

<br>

<center>

$SS_{Error}$: Residual sum of squares, variability of residuals

<br>

$\sum_{i = 1}^n (y_i - \hat{y}_i)^2$ = `r anova_df$sumsq[3]`

</center>

## Sum of squares: $SS_{Model}$

```{r}
#| echo: false
anova_df <- anova(tip_fit) |>
  tidy() |>
  mutate(across(where(is.numeric), round, 2)) |>
  select(term, df, sumsq) 

anova_df |>
  janitor::adorn_totals(where = "row", cols = 1:3, fill = "") |>
  kable() |>
  row_spec(c(1,2), background = "#D9E3E4")
```

<br>

<center>

$SS_{Model}$: Variability explained by the model

<br>

$\sum_{i = 1}^n (\hat{y}_i - \bar{y})^2 = SS_{Model} = SS_{Total} - SS_{Error} =$ `r sum(anova_df$sumsq[1:2])`

</center>

## F-Test: Testing the whole model at once {.smaller}

**Hypotheses:** 

$H_0: \beta_1 = \beta_2 = \cdots = \beta_k = 0$ vs. $H_A:$ at least one $\beta_i \neq 0$ 

. . .

**Test statistic:** F-statistics

$$
F = \frac{MSModel}{MSE} = \frac{SSModel/k}{SSE/(n-k-1)} \\
$$

. . .

**p-value:** Probability of observing a test statistic at least as extreme (in the direction of the alternative hypothesis) from the null value as the one observed

$$
\text{p-value} = P(F > \text{test statistic}),
$$

calculated from an $F$ distribution with $k$ and $n - k - 1$ degrees of freedom.

## F-test in R

- Use `glance` function from `broom` package
  - `statistic`: F-statistic
  - `p.value`: p-value from F-test

## R-squared, $R^2$

**Recall**: $R^2$ is the proportion of the variation in the response variable explained by the regression model.

. . .

$$
R^2 = \frac{SS_{Model}}{SS_{Total}} = 1 - \frac{SS_{Error}}{SS_{Total}}
$$

Complete Exercises 4-7.

# Model comparison

## R-squared, $R^2$, Overfitting

-   $R^2$ will always increase as we add more variables to the model 
    +   If we add enough variables, we can usually achieve $R^2=100\%$
    +   Eventually our model will over-align to the noise in our data and become worse at predicting new data... this is called [overfitting](https://en.wikipedia.org/wiki/Overfitting#/media/File:Pyplot_overfitting.png)   
-   If we only use $R^2$ to choose a best fit model, we will be prone to choosing the model with the most predictor variables

## Adjusted $R^2$

-   **Adjusted** $R^2$: measure that includes a penalty for unnecessary predictor variables
-   Similar to $R^2$, it is a measure of the amount of variation in the response that is explained by the regression model
-   Differs from $R^2$ by using the mean squares (sum of squares/degrees of freedom) rather than sums of squares and therefore adjusting for the number of predictor variables

## $R^2$ and Adjusted $R^2$

$$R^2 = \frac{SS_{Model}}{SS_{Total}} = 1 - \frac{SS_{Error}}{SS_{Total}}$$

<br>

. . .

$$R^2_{adj} = 1 - \frac{SS_{Error}/(n-p-1)}{SS_{Total}/(n-1)}$$

where

-   $n$ is the number of observations used to fit the model

-   $p$ is the number of terms (not including the intercept) in the model

## Using $R^2$ and Adjusted $R^2$

-   $R^2_{adj}$ can be used as a quick assessment to compare the fit of multiple models; however, it should not be the only assessment!
-   Use $R^2$ when describing the relationship between the response and predictor variables

Complete Exercises 8-10.

## Comparing models with $R^2_{adj}$ {.smaller}

::: columns
::: {.column width="50%"}
`tip_fit_1`:

```{r}
#| echo: false

tip_fit_1 <- lm(Tip ~ Party + Age + Meal, data = tips)

glance(tip_fit_1) |> 
  kable()
```
:::

::: {.column width="50%"}

`tip_fit_2`:

```{r}
#| echo: false

tip_fit_2 <- lm(Tip ~ Party + Age + Meal + Day, data = tips)

glance(tip_fit_2) |> 
  kable()
```
:::
:::

::: question
1.  Which model would we choose based on $R^2$?
2.  Which model would we choose based on Adjusted $R^2$?
3.  Which statistic should we use to choose the final model - $R^2$ or Adjusted $R^2$? Why?
:::

## AIC & BIC

Estimators of prediction error and *relative* quality of models:

. . .

**Akaike's Information Criterion (AIC)**: $$AIC = n\log(SS_\text{Error}) - n \log(n) + 2(p+1)$$ <br>

. . .

**Schwarz's Bayesian Information Criterion (BIC)**: $$BIC = n\log(SS_\text{Error}) - n\log(n) + log(n)\times(p+1)$$


## AIC & BIC

$$
\begin{aligned} 
& AIC = \color{blue}{n\log(SS_\text{Error})} - n \log(n) + 2(p+1) \\
& BIC = \color{blue}{n\log(SS_\text{Error})} - n\log(n) + \log(n)\times(p+1) 
\end{aligned}
$$

. . .

<br>

First Term: Decreases as *p* increases... why?

## AIC & BIC

$$
\begin{aligned} 
& AIC = n\log(SS_\text{Error}) - \color{blue}{n \log(n)} + 2(p+1) \\
& BIC = n\log(SS_\text{Error}) - \color{blue}{n\log(n)} + \log(n)\times(p+1) 
\end{aligned}
$$

<br>

Second Term: Fixed for a given sample size *n*

## AIC & BIC

$$
\begin{aligned} & AIC = n\log(SS_\text{Error}) - n\log(n) + \color{blue}{2(p+1)} \\
& BIC = n\log(SS_\text{Error}) - n\log(n) + \color{blue}{\log(n)\times(p+1)} 
\end{aligned}
$$

<br>

Third Term: Increases as *p* increases

## Using AIC & BIC

$$
\begin{aligned} & AIC = n\log(SS_{Error}) - n \log(n) + \color{red}{2(p+1)} \\
& BIC = n\log(SS_{Error}) - n\log(n) + \color{red}{\log(n)\times(p+1)} 
\end{aligned}
$$

-   Choose model with the smaller value of AIC or BIC

-   If $n \geq 8$, the **penalty** for BIC is larger than that of AIC, so BIC tends to favor *more parsimonious* models (i.e. models with fewer terms)

Complete Exercise 11 and 12.

## Comparing models with AIC and BIC

::: columns
::: {.column width="50%"}
`tip_fit_1`

```{r}
#| echo: false

tip_fit_1 <- lm(Tip ~ Party + 
            Age + 
            Meal,
      
      data = tips)

glance(tip_fit_1) |> 
  select(AIC, BIC) |> 
  kable()
```
:::

::: {.column width="50%"}
`tip_fit_2`

```{r}
#| echo: false

tip_fit_2 <- lm(Tip ~ Party + 
            Age + 
            Meal + 
            Day, 
      data = tips)

glance(tip_fit_2) |> 
  select(AIC, BIC) |> 
  kable()
```
:::
:::

::: question
1.  Which model would we choose based on AIC?

2.  Which model would we choose based on BIC?
:::

## Commonalities between criteria

-   $R^2_{adj}$, AIC, and BIC all apply a penalty for more predictors
-   The penalty for added model complexity attempts to strike a balance between underfitting (too few predictors in the model) and overfitting (too many predictors in the model)
-   Goal: **Parsimony**

## Parsimony and Occam's razor {.small}

-   The principle of **parsimony** is attributed to William of Occam (early 14th-century English nominalist philosopher), who insisted that, given a set of equally good explanations for a given phenomenon, *the correct explanation is the simplest explanation*[^2]

-   Called **Occam's razor** because he "shaved" his explanations down to the bare minimum

-   Parsimony in modeling:

    ::: nonincremental
    -   models should have as few parameters as possible
    -   linear models should be preferred to non-linear models
    -   experiments relying on few assumptions should be preferred to those relying on many
    -   models should be pared down until they are *minimal adequate* (i.e. contain the minimum number of predictors required to meet some critereon)
    -   simple explanations should be preferred to complex explanations
    :::

[^2]: Source: The R Book by Michael J. Crawley.

## In pursuit of Occam's razor

-   Occam's razor states that among competing hypotheses that predict equally well, the one with the fewest assumptions should be selected

-   Model selection follows this principle

-   We only want to add another variable to the model if the addition of that variable brings something valuable in terms of predictive power to the model

-   In other words, we prefer the simplest best model, i.e. **parsimonious** model

## Alternate views {.midi}

> Sometimes a simple model will outperform a more complex model . . . Nevertheless, I believe that deliberately limiting the complexity of the model is not fruitful when the problem is evidently complex. Instead, if a simple model is found that outperforms some particular complex model, the appropriate response is to define a different complex model that captures whatever aspect of the problem led to the simple model performing well.
>
> <br>
>
> Radford Neal - Bayesian Learning for Neural Networks[^3]

[^3]: Suggested blog post: [Occam](https://statmodeling.stat.columbia.edu/2012/06/26/occam-2/) by Andrew Gelman

## Other concerns with our approach {.midi}

-   All criteria we considered for model comparison require making predictions for our data and then uses the prediction error ($SS_{Error}$) somewhere in the formula
-   But we're making prediction for the data we used to build the model (estimate the coefficients), which can lead to **overfitting**

## Recap

-   ANOVA for multiple linear regression and sum of squares
-   $R^2$ for multiple linear regression
-   Comparing models with
    -   $R^2$ vs.Â $R^2_{Adj}$
    -   AIC and BIC
-   Occam's razor and parsimony
