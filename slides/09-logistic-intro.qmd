---
title: "Logistic regression"
subtitle: "Introduction"
author: "Prof. Eric Friedlander"
footer: "[ðŸ”— MAT 212 - Winter 2025 -  Schedule](https://mat212wi25.netlify.app/schedule)"
logo: "../images/logo.png"
format: 
  revealjs:
    theme: slides.scss
    multiplex: false
    transition: fade
    slide-number: false
    incremental: false 
    chalkboard: true
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
execute:
  freeze: auto
  echo: true
  cache: false
knitr:
  opts_chunk: 
    R.options:      
    width: 200
bibliography: references.bib  
---

```{r}
#| include: false

# figure options
knitr::opts_chunk$set(
  fig.width = 10, fig.asp = 0.618, out.width = "90%",
  fig.retina = 3, dpi = 300, fig.align = "center"
)

library(countdown)
```

## Application Exercise

::: appex
ðŸ“‹ [AE 09 - Intro to Logistic Regression](/ae/ae-09-logistic-intro.qmd)

- Complete Exercises 0-2.
:::


# Logistic regression

## Topics

-   Introduction to modeling categorical data

-   Logistic regression for *binary* response variable

-   Relationship between odds and probabilities

## Computational setup

```{r}
#| warning: false

# load packages
library(tidyverse)
library(ggformula)
library(broom)
library(knitr)
library(ggforce)

# set default theme and larger font size for ggplot2
ggplot2::theme_set(ggplot2::theme_bw(base_size = 20))
```

# Predicting categorical outcomes

## Types of outcome variables

**Quantitative outcome variable**:

-   Sales price of a house
-   **Model**: Expected sales price given the number of bedrooms, lot size, etc.

. . .

**Categorical outcome variable**:

-   Indicator for developing coronary heart disease in the next 10 years
-   **Model**: Probability an adult is high risk of heart disease in the next 10 years given their age, total cholesterol, etc.

## Models for categorical outcomes

::: columns
::: {.column width="50%"}
**Logistic regression**

2 Outcomes

- 1: "Success" (models probability of this category...)
- 0: "Failure"
:::

::: {.column width="50%"}
**Multinomial logistic regression**

3+ Outcomes

- 1: Democrat
- 2: Republican
- 3: Independent
:::
:::

## 2024 election forecasts

[The Economist](https://www.economist.com/interactive/us-2024-election/prediction-model/president)


## 2020 NBA finals predictions

![](images/19/nba-predictions.png){fig-align="center"}

Source: [FiveThirtyEight 2019-20 NBA Predictions](https://projects.fivethirtyeight.com/2020-nba-predictions/games/?ex_cid=rrpromo)

## Data: Framingham Study {.smaller}

This data set is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to use the patients age to predict if a randomly selected adult is high risk for heart disease in the next 10 years.

```{r}
heart_disease <- read_csv("../data/framingham.csv") |>
  select(totChol, TenYearCHD, age, BMI, cigsPerDay, heartRate) |>
  drop_na()
```

## Variables

- Response: 
    -   `TenYearCHD`:
        -   1: Patient developed heart disease within 10 years of exam
        -   0: Patient did not develop heart disease within 10 years of exam
- Predictor: 
  -   `age`: age in years at time of visit    


## Plot the data

```{r}
#| echo: FALSE
heart_disease |> 
  gf_point(TenYearCHD ~ age)  |>
  gf_labs(y = "CHD Risk", x = "Age")
```

## Let's fit a linear regression model

```{r}
#| echo: false
heart_disease |> 
  gf_point(TenYearCHD ~ age)  |>
  gf_lm()  |> 
  gf_labs(y = "CHD Risk", x = "Age")
```

ðŸ›‘ *This model produces predictions outside of 0 and 1.*

## Let's try another model

```{r}
#| label: logistic-model-plot
#| echo: false

heart_disease |> 
  gf_point(TenYearCHD ~ age)  |>
  gf_hline(yintercept = c(0,1), lty = 2) |> 
  gf_labs(y = "CHD Risk", x = "Age") |> 
  gf_refine(stat_smooth(method ="glm", method.args = list(family = "binomial"), 
              fullrange = TRUE, se = FALSE))
```

## Let's try another model: Zooming Out

```{r}
#| label: logistic-model-plot-2
#| echo: false

heart_disease |> 
  gf_point(TenYearCHD ~ age)  |>
  gf_hline(yintercept = c(0,1), lty = 2) |> 
  gf_lims(x = c(0, 300)) |> 
  gf_labs(y = "CHD Risk", x = "Age") |> 
  gf_refine(stat_smooth(method ="glm", method.args = list(family = "binomial"), 
              fullrange = TRUE, se = FALSE))
```

*âœ… This model (called a **logistic regression model**) only produces predictions between 0 and 1.*

## The code

```{r}
#| ref.label: logistic-model-plot
#| echo: true
#| fig-show: hide
```

## Different types of models

| Method                          | Outcome      | Model                                                     |
|---------------------------------|--------------|-----------------------------------------------------------|
| Linear regression               | Quantitative | $Y = \beta_0 + \beta_1~ X$                                |
| Linear regression (transform Y) | Quantitative | $\log(Y) = \beta_0 + \beta_1~ X$                          |
| Logistic regression             | Binary       | $\log\big(\frac{\pi}{1-\pi}\big) = \beta_0 + \beta_1 ~ X$ |

**Note:** In this class (and in most college level math classes) ((and and in R))  $\log$ means log base $e$ (i.e. natural log)

## Linear vs. logistic regression

Complete Exercise 3.

## Linear vs. logistic regression

::: question
State whether a linear regression model or logistic regression model is more appropriate for each scenario.

1.  Use age and education to predict if a randomly selected person will vote in the next election.

2.  Use budget and run time (in minutes) to predict a movie's total revenue.

3.  Use age and sex to calculate the probability a randomly selected adult will visit St. Lukes in the next year.
:::

# Odds and probabilities

## Binary response variable

::: incremental
-   $Y$: 
    +   1: "success" (not necessarily a good thing)
    +   0: "failure"
-   $\pi$: **probability** that $Y=1$, i.e., $P(Y = 1)$
-   $\frac{\pi}{1-\pi}$: **odds** that $Y = 1$
-   $\log\big(\frac{\pi}{1-\pi}\big)$: **log-odds**
-   Go from $\pi$ to $\log\big(\frac{\pi}{1-\pi}\big)$ using the **logit transformation**
:::

## Odds {.smaller}

::: incremental
Suppose there is a **70% chance** it will rain tomorrow

-   Probability it will rain is $\mathbf{p = 0.7}$
-   Probability it won't rain is $\mathbf{1 - p = 0.3}$
-   Odds it will rain are **7 to 3**, **7:3**, $\mathbf{\frac{0.7}{0.3} \approx 2.33}$
    +   For every 3 times it doesn't rain, it will rain 7 times
    +   For every time it *doesn't* rain, it will rain 2.33 times
-   Log-Odds it will rain is $\log\mathbf{\frac{0.7}{0.3} \approx \log(2.33) \approx 0.847}$
    +   Negative $\Rightarrow$ probability of success less than 50-50 (0.5)
    +   Positive $\Rightarrow$ probability of success greater than 50-50 (0.5)
    +   What are the log-odds of of a probability of 0? What about 1?
:::


## From log-odds to probabilities

::: columns
::: {.column width="50%"}
**log-odds**

$$\omega = \log \frac{\pi}{1-\pi}$$

**odds**

$$e^\omega = \frac{\pi}{1-\pi}$$
:::

::: {.column width="50%"}
**probability**

$$\pi = \frac{e^\omega}{1 + e^\omega}$$
:::
:::

Complete Exercise 4-5.

# Logistic regression

## From odds to probabilities {.incremental}

(1) **Logistic model**: log-odds = $\log\big(\frac{\pi}{1-\pi}\big) = \beta_0 + \beta_1~X$
(2) **Odds =** $\exp\big\{\log\big(\frac{\pi}{1-\pi}\big)\big\} = \frac{\pi}{1-\pi}$
(3) Combining (1) and (2) with what we saw earlier

. . .

$$\text{probability} = \pi = \frac{\exp\{\beta_0 + \beta_1~X\}}{1 + \exp\{\beta_0 + \beta_1~X\}}$$

## Logistic regression model

**Logit form**: $$\log\big(\frac{\pi}{1-\pi}\big) = \beta_0 + \beta_1~X$$

. . .

**Probability form**:

$$
\pi = \frac{\exp\{\beta_0 + \beta_1~X\}}{1 + \exp\{\beta_0 + \beta_1~X\}}
$$

## Variables

- Response: 
    -   `TenYearCHD`:
        -   1: Patient developed heart disease within 10 years of exam
        -   0: Patient did not develop heart disease within 10 years of exam
- Predictors: 
  -   `age`: age in years


# Logistic regression

## Logistic regression model

**Logit form**: $$\log\big(\frac{\pi}{1-\pi}\big) = \beta_0 + \beta_1~X$$

. . .

**Probability form**:

$$
\pi = \frac{\exp\{\beta_0 + \beta_1~X\}}{1 + \exp\{\beta_0 + \beta_1~X\}}
$$

. . .

Today: Using R to fit this model.



## TenYearCHD vs. age

```{r}
#| echo: true

heart_disease |> 
gf_sina(age ~ factor(TenYearCHD)) |> 
  gf_labs(x = "TenYearCHD - 1: yes, 0: no",
       y = "Age", 
       title = "Age vs. Ten YearCHD")
```

## TenYearCHD vs. age

```{r}
#| echo: true

heart_disease |> 
gf_violin(age ~ factor(TenYearCHD), fill = "steelblue") |> 
  gf_labs(x = "TenYearCHD - 1: yes, 0: no",
       y = "Age", 
       title = "Age vs. TenYearCHD")
```

## TenYearCHD vs. age

```{r}
#| echo: true

heart_disease |> 
gf_boxplot(age ~ factor(TenYearCHD), fill = "steelblue") |> 
  gf_sina(size = 0.75, alpha=0.25) |> 
  gf_labs(x = "TenYearCHD - 1: yes, 0: no",
       y = "Age", 
       title = "Age vs. TenYearCHD")
```

## Let's fit a model

```{r}
#| echo: true
#| code-line-numbers: "1|3"
heart_disease_fit <- glm(TenYearCHD ~ age, data = heart_disease, family = "binomial")

tidy(heart_disease_fit) |> kable()
```

## The model {.smaller}

```{r}
#| echo: true

tidy(heart_disease_fit) |> kable(digits = 3)
```


$$\textbf{Logit form:}\qquad\log\Big(\frac{\hat{\pi}}{1-\hat{\pi}}\Big) = -5.561 + 0.076 \times \text{age}$$ 

$$\textbf{Probability form:}\qquad\hat{\pi} = \frac{\exp(-5.561 + 0.076 \times \text{age})}{1+\exp(-5.561 + 0.075 \times \text{age})}$$

where $\hat{\pi}$ is the predicted probability of developing heart disease in the next 10 years.

## Interpreting $\hat{\beta}$'s

```{r}
tidy(heart_disease_fit) |> kable(digits = 3)
```

For every addition year of age, the **log-odds** of developing heart disease in the next 10 years, increases by 0.076.

. . .


Complete Exercises 6-8.

## Interpretability of $\beta$ for predicted probabilities

```{r}
#| echo: false

augment(heart_disease_fit) |> 
  mutate(predicted_prob = exp(.fitted)/(1+exp(.fitted))) |> 
  gf_point(predicted_prob ~ age) |> 
  gf_labs( x = "Age", y = "Predicted Probability")
```

-   SLOPE IS CHANGING!
-   Increase in $\hat{\pi}$ due to increase of 1 year of Age *depends on what starting age is*

## `glm` and `augment` {.smaller}

The `.fitted` values in `augment` correspond to predictions from the logistic form of the model (i.e. the log-odds):

```{r}
augment(heart_disease_fit)  |> head()
```

. . .

Note: The residuals do not make sense here!

**For observation 1**

$$\text{predicted probability} = \hat{\pi} = \frac{\exp\{-2.680\}}{1 + \exp\{-2.680\}} = 0.0733$$

## Using `predict` with `glm`

Default output is log-odds:

```{r}
predict(heart_disease_fit, new_data = heart_disease) |> head() |> kable(digits = 3)
```

## Using `predict` with `glm`

More commonly you want the predicted probability:

```{r}
predict(heart_disease_fit, newdata = heart_disease, type = "response") |> head() |> kable(digits = 3)
```

. . .

Complete Exercise 9


## Recap

-   Introduced logistic regression for binary response variable
-   Described relationship between odds and probabilities
-   Fit logistic regression models using `glm`
-   Interpreted coefficients in logistic regression models
-   Used logistic regression model to calculate predicted odds and probabilities
-   Use `predict` to make predictions using `glm`
