{
  "hash": "91e065ae20c137dafdbd230c899933b3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Model comparison continued\"\nauthor: \"Prof. Eric Friedlander\"\ndate: \"2024-10-25\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— MAT 212 - Fall 2024 -  Schedule](https://mat212fa24.netlify.app/schedule)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: false\n    incremental: false \n    chalkboard: true\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\nexecute:\n  freeze: auto\n  echo: true\n  cache: false\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n\n\n\n\n\n## Announcements\n\n-   Project: EDA Due Wednesday, October 30th\n-   Oral R Quiz\n\n::: appex\nðŸ“‹ [AE 15 - Model Comparison 2](https://mat212fa24.netlify.app/ae/ae-15-comparison2)\n\n- Open up AE 15\n:::\n\n\n## Topics\n\n::: nonincremental\n-   Comparing models with $R^2$ vs. $R^2_{adj}$\n-   Comparing models with AIC and BIC\n-   Occam's razor and parsimony\n:::\n\n\n## Computational setup\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# load packages\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(yardstick)\nlibrary(ggformula)\nlibrary(supernova)\nlibrary(tidymodels)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(janitor)\nlibrary(kableExtra)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))\n```\n:::\n\n\n\n\n# Introduction\n\n## Data: Restaurant tips\n\nWhich variables help us predict the amount customers tip at a restaurant?\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 169 Ã— 4\n     Tip Party Meal   Age   \n   <dbl> <dbl> <chr>  <chr> \n 1  2.99     1 Dinner Yadult\n 2  2        1 Dinner Yadult\n 3  5        1 Dinner SenCit\n 4  4        3 Dinner Middle\n 5 10.3      2 Dinner SenCit\n 6  4.85     2 Dinner Middle\n 7  5        4 Dinner Yadult\n 8  4        3 Dinner Middle\n 9  5        2 Dinner Middle\n10  1.58     1 Dinner SenCit\n# â„¹ 159 more rows\n```\n\n\n:::\n:::\n\n\n\n\n## Variables\n\n**Predictors**:\n\n::: nonincremental\n-   `Party`: Number of people in the party\n-   `Meal`: Time of day (`Lunch`, `Dinner`, `Late Night`)\n-   `Age`: Age category of person paying the bill (`Yadult`, `Middle`, `SenCit`)\n:::\n\n**Outcome**: `Tip`: Amount of tip\n\n## Outcome: `Tip`\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](21-comparison2_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\n\n\n## Predictors\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](21-comparison2_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\n\n\n## Outcome vs. predictors\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](21-comparison2_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=3600}\n:::\n:::\n\n\n\n\n## Fit and summarize model {.midi}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|term        | estimate| std.error| statistic| p.value|\n|:-----------|--------:|---------:|---------:|-------:|\n|(Intercept) |    0.838|     0.397|     2.112|   0.036|\n|Party       |    1.837|     0.124|    14.758|   0.000|\n|AgeSenCit   |    0.379|     0.410|     0.925|   0.356|\n|AgeYadult   |   -1.009|     0.408|    -2.475|   0.014|\n\n\n:::\n:::\n\n\n\n\n. . .\n\n<br>\n\n::: question\nIs this model good?\n:::\n\n# Model comparison\n\n## R-squared, $R^2$, Overfitting\n\n-   $R^2$ will always increase as we add more variables to the model \n    +   If we add enough variables, we can usually achieve $R^2=100\\%$\n    +   Eventually our model will over-align to the noise in our data and become worse at predicting new data... this is called [overfitting](https://en.wikipedia.org/wiki/Overfitting#/media/File:Pyplot_overfitting.png)   \n-   If we only use $R^2$ to choose a best fit model, we will be prone to choosing the model with the most predictor variables\n\n## Adjusted $R^2$\n\n-   **Adjusted** $R^2$: measure that includes a penalty for unnecessary predictor variables\n-   Similar to $R^2$, it is a measure of the amount of variation in the response that is explained by the regression model\n-   Differs from $R^2$ by using the mean squares (sum of squares/degrees of freedom) rather than sums of squares and therefore adjusting for the number of predictor variables\n\n## $R^2$ and Adjusted $R^2$\n\n$$R^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Error}}{SS_{Total}}$$\n\n<br>\n\n. . .\n\n$$R^2_{adj} = 1 - \\frac{SS_{Error}/(n-p-1)}{SS_{Total}/(n-1)}$$\n\nwhere\n\n-   $n$ is the number of observations used to fit the model\n\n-   $p$ is the number of terms (not including the intercept) in the model\n\n## Using $R^2$ and Adjusted $R^2$\n\n-   $R^2_{adj}$ can be used as a quick assessment to compare the fit of multiple models; however, it should not be the only assessment!\n-   Use $R^2$ when describing the relationship between the response and predictor variables\n\nComplete Exercises 1-3.\n\n## Comparing models with $R^2_{adj}$ {.smaller}\n\n::: columns\n::: {.column width=\"50%\"}\n`tip_fit_1`:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n| r.squared| adj.r.squared|    sigma| statistic| p.value| df|    logLik|      AIC|      BIC| deviance| df.residual| nobs|\n|---------:|-------------:|--------:|---------:|-------:|--:|---------:|--------:|--------:|--------:|-----------:|----:|\n| 0.6743626|     0.6643738| 1.954983|  67.51136|       0|  5| -350.0405| 714.0811| 735.9904| 622.9793|         163|  169|\n\n\n:::\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n`tip_fit_2`:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n| r.squared| adj.r.squared|   sigma| statistic| p.value| df|   logLik|      AIC|      BIC| deviance| df.residual| nobs|\n|---------:|-------------:|-------:|---------:|-------:|--:|--------:|--------:|--------:|--------:|-----------:|----:|\n| 0.6825157|     0.6624218| 1.96066|  33.96625|       0| 10| -347.898| 719.7959| 757.3547| 607.3815|         158|  169|\n\n\n:::\n:::\n\n\n\n:::\n:::\n\n::: question\n1.  Which model would we choose based on $R^2$?\n2.  Which model would we choose based on Adjusted $R^2$?\n3.  Which statistic should we use to choose the final model - $R^2$ or Adjusted $R^2$? Why?\n:::\n\n## AIC & BIC\n\nEstimators of prediction error and *relative* quality of models:\n\n. . .\n\n**Akaike's Information Criterion (AIC)**: $$AIC = n\\log(SS_\\text{Error}) - n \\log(n) + 2(p+1)$$ <br>\n\n. . .\n\n**Schwarz's Bayesian Information Criterion (BIC)**: $$BIC = n\\log(SS_\\text{Error}) - n\\log(n) + log(n)\\times(p+1)$$\n\n\n## AIC & BIC\n\n$$\n\\begin{aligned} \n& AIC = \\color{blue}{n\\log(SS_\\text{Error})} - n \\log(n) + 2(p+1) \\\\\n& BIC = \\color{blue}{n\\log(SS_\\text{Error})} - n\\log(n) + \\log(n)\\times(p+1) \n\\end{aligned}\n$$\n\n. . .\n\n<br>\n\nFirst Term: Decreases as *p* increases... why?\n\n## AIC & BIC\n\n$$\n\\begin{aligned} \n& AIC = n\\log(SS_\\text{Error}) - \\color{blue}{n \\log(n)} + 2(p+1) \\\\\n& BIC = n\\log(SS_\\text{Error}) - \\color{blue}{n\\log(n)} + \\log(n)\\times(p+1) \n\\end{aligned}\n$$\n\n<br>\n\nSecond Term: Fixed for a given sample size *n*\n\n## AIC & BIC\n\n$$\n\\begin{aligned} & AIC = n\\log(SS_\\text{Error}) - n\\log(n) + \\color{blue}{2(p+1)} \\\\\n& BIC = n\\log(SS_\\text{Error}) - n\\log(n) + \\color{blue}{\\log(n)\\times(p+1)} \n\\end{aligned}\n$$\n\n<br>\n\nThird Term: Increases as *p* increases\n\n## Using AIC & BIC\n\n$$\n\\begin{aligned} & AIC = n\\log(SS_{Error}) - n \\log(n) + \\color{red}{2(p+1)} \\\\\n& BIC = n\\log(SS_{Error}) - n\\log(n) + \\color{red}{\\log(n)\\times(p+1)} \n\\end{aligned}\n$$\n\n-   Choose model with the smaller value of AIC or BIC\n\n-   If $n \\geq 8$, the **penalty** for BIC is larger than that of AIC, so BIC tends to favor *more parsimonious* models (i.e. models with fewer terms)\n\nComplete Exercise 4.\n\n## Comparing models with AIC and BIC\n\n::: columns\n::: {.column width=\"50%\"}\n`tip_fit_1`\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|      AIC|      BIC|\n|--------:|--------:|\n| 714.0811| 735.9904|\n\n\n:::\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n`tip_fit_2`\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|      AIC|      BIC|\n|--------:|--------:|\n| 719.7959| 757.3547|\n\n\n:::\n:::\n\n\n\n:::\n:::\n\n::: question\n1.  Which model would we choose based on AIC?\n\n2.  Which model would we choose based on BIC?\n:::\n\n## Commonalities between criteria\n\n-   $R^2_{adj}$, AIC, and BIC all apply a penalty for more predictors\n-   The penalty for added model complexity attempts to strike a balance between underfitting (too few predictors in the model) and overfitting (too many predictors in the model)\n-   Goal: **Parsimony**\n\n## Parsimony and Occam's razor {.small}\n\n-   The principle of **parsimony** is attributed to William of Occam (early 14th-century English nominalist philosopher), who insisted that, given a set of equally good explanations for a given phenomenon, *the correct explanation is the simplest explanation*[^2]\n\n-   Called **Occam's razor** because he \"shaved\" his explanations down to the bare minimum\n\n-   Parsimony in modeling:\n\n    ::: nonincremental\n    -   models should have as few parameters as possible\n    -   linear models should be preferred to non-linear models\n    -   experiments relying on few assumptions should be preferred to those relying on many\n    -   models should be pared down until they are *minimal adequate* (i.e. contain the minimum number of predictors required to meet some critereon)\n    -   simple explanations should be preferred to complex explanations\n    :::\n\n[^2]: Source: The R Book by Michael J. Crawley.\n\n## In pursuit of Occam's razor\n\n-   Occam's razor states that among competing hypotheses that predict equally well, the one with the fewest assumptions should be selected\n\n-   Model selection follows this principle\n\n-   We only want to add another variable to the model if the addition of that variable brings something valuable in terms of predictive power to the model\n\n-   In other words, we prefer the simplest best model, i.e. **parsimonious** model\n\n## Alternate views {.midi}\n\n> Sometimes a simple model will outperform a more complex model . . . Nevertheless, I believe that deliberately limiting the complexity of the model is not fruitful when the problem is evidently complex. Instead, if a simple model is found that outperforms some particular complex model, the appropriate response is to define a different complex model that captures whatever aspect of the problem led to the simple model performing well.\n>\n> <br>\n>\n> Radford Neal - Bayesian Learning for Neural Networks[^3]\n\n[^3]: Suggested blog post: [Occam](https://statmodeling.stat.columbia.edu/2012/06/26/occam-2/) by Andrew Gelman\n\n## Other concerns with our approach {.midi}\n\n-   All criteria we considered for model comparison require making predictions for our data and then uses the prediction error ($SS_{Error}$) somewhere in the formula\n-   But we're making prediction for the data we used to build the model (estimate the coefficients), which can lead to **overfitting**\n\n\n## Recap\n\n-   Comparing models with\n    -   $R^2$ vs.Â $R^2_{Adj}$\n    -   AIC and BIC\n-   Occam's razor and parsimony\n\n-   Complete Exercise 5.\n\n",
    "supporting": [
      "21-comparison2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}