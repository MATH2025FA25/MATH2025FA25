{
  "hash": "8accf57a4088cf77c1e84ab953a0103f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"SLR: Conditions\"\nauthor: \"Prof. Eric Friedlander\"\nfooter: \"[üîó MAT 212 - Winter 2025 -  Schedule](https://mat212wi25.netlify.app/schedule)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: false\n    incremental: false \n    chalkboard: true\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\nexecute:\n  freeze: auto\n  echo: true\n  cache: false\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib  \n---\n\n\n\n\n\n# Application exercise\n\n::: appex\nüìã [AE 05 - Model Conditions](/ae/ae-05-conditions.qmd)\n:::\n\nComplete Exercise 0.\n\n## Computational set up\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(ggformula)   # for plotting using formulas\nlibrary(broom)       # for formatting model output\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(kableExtra)  # also for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# Spotify Dataset\nspotify <- read_csv(\"../data/spotify-popular.csv\")\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))\n```\n:::\n\n\n\n## Quick Data Cleaning\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nspotify <- spotify |> \n  mutate(duration_min = duration_ms / 60000)\n```\n:::\n\n\n\n::: question\n- What is this code doing?\n- Why might I be doing it?\n:::\n\n## The regression model, revisited {.smaller}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nspotify_fit <- lm(danceability ~ duration_min, data = spotify)\n\ntidy(spotify_fit, conf.int = TRUE, conf.level = 0.95) |>\n   kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|term         | estimate| std.error| statistic| p.value| conf.low| conf.high|\n|:------------|--------:|---------:|---------:|-------:|--------:|---------:|\n|(Intercept)  |    0.781|     0.028|    28.351|   0.000|    0.727|     0.835|\n|duration_min |   -0.024|     0.008|    -3.151|   0.002|   -0.039|    -0.009|\n\n\n:::\n:::\n\n\n\n- There is strong statistical evidence that there is a linear relationship between the duration of a song and it's danceability.\n\n- We are 95% confidence that as the length of a song increases by one minute the danceability will decrease by between 0.009 and 0.039 units.\n\n## Mathematical representation, visualized {.midi}\n\n$$\nY|X \\sim N(\\beta_0 + \\beta_1 X, \\sigma_\\epsilon^2)\n$$\n\n![Image source: *Introduction to the Practice of Statistics (5th ed)*](images/04/regression.png){fig-align=\"center\"}\n\n# Model conditions\n\n## Model conditions\n\n1.  **Linearity:** There is a linear relationship between the outcome and predictor variables\n2.  **Constant variance:** The variability of the errors is equal for all values of the predictor variable\n3.  **Normality:** The errors follow a normal distribution\n4.  **Independence:** The errors are independent from each other\n\n## WARNING {.smaller}\n\n- Many of these assumptions are for the population\n- We want to determine whether they are met from your data\n- In real life, these conditions are *almost always* violated in one way or another\n- Questions you should ask yourself:\n    + Are my conditions close enough to being satisfied that I can trust the results of my inference\n    + Do I have reason to believe that my conditions are *GROSSLY* violated?\n    + Based on what I see, how trustworthy do I think the results of my inference are.\n    \n## ENGAGE: SOAP BOX {.smaller}\n\nStatistics and numbers are often used to make arguments seem more \"rigorous\" or infallible. I'm sure you've heard the phrase \"the numbers are the numbers\" or \"you can't argue with the numbers\". More often than not, this is **BULLSHIT**. Most data analyses involve making decision which are subjective. The interpretability of any form of statistical inference is heavily influenced by whether the assumptions and conditions of that inference is met, which they almost never are. It is up to the practitioner to determine whether those conditions are met and what impact those conditions have on the results of those analyses. In my work, I rarely encounter practitioners who even know what the conditions are, let alone understand why they are important. **FURTHERMORE!!!** the quality of your analysis is only as good as the quality of your data. Remember a crap study design will yield crap data which will yield crappy analysis. Statistical analyses yield one important form of evidence which should be combined with other forms of evidence when making an argument.\n\n## Linearity\n\n-   If the linear model, $\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1X$, adequately describes the relationship between $X$ and $Y$, then the residuals should reflect random (chance) error\n\n-   To assess this, we can look at a plot of the residuals ($e_i$'s) vs. the fitted values ($\\hat{y}_i$'s) or predictors(or $x_i$'s)\n\n-   There should be no distinguishable pattern in the residuals plot, i.e. the residuals should be randomly scattered\n\n-   A non-random pattern (e.g. a parabola) suggests a linear model that does not adequately describe the relationship between $X$ and $Y$\n\n## Linearity\n\n‚úÖ The residuals vs. fitted values plot should show a random scatter of residuals (no distinguishable pattern or structure)\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slr-conditions_files/figure-revealjs/res-vs-fit-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n## The augment function {.smaller}\n\n`augment` is from the `broom` package:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nspotify_aug <- augment(spotify_fit)\n\nhead(spotify_aug) |> kable()\n```\n\n::: {.cell-output-display}\n\n\n| danceability| duration_min|   .fitted|     .resid|      .hat|    .sigma|   .cooksd| .std.resid|\n|------------:|------------:|---------:|----------:|---------:|---------:|---------:|----------:|\n|        0.733|     2.464000| 0.7212126|  0.0117874| 0.0057228| 0.1304132| 0.0000237|  0.0907334|\n|        0.630|     3.258650| 0.7019569| -0.0719569| 0.0021749| 0.1303749| 0.0003332| -0.5529036|\n|        0.877|     3.864133| 0.6872849|  0.1897151| 0.0024253| 0.1301401| 0.0025838|  1.4579193|\n|        0.831|     3.674783| 0.6918732|  0.1391268| 0.0020725| 0.1302669| 0.0011866|  1.0689702|\n|        0.668|     2.650183| 0.7167011| -0.0487011| 0.0044968| 0.1303962| 0.0003170| -0.3746465|\n|        0.626|     3.377017| 0.6990886| -0.0730886| 0.0020230| 0.1303736| 0.0003196| -0.5615572|\n\n\n:::\n:::\n\n\n\n## Residuals vs. fitted values (code)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nspotify_aug <- augment(spotify_fit)\n\ngf_point(.resid ~ .fitted, data = spotify_aug) |>\n  gf_hline(yintercept = 0, linetype = \"dashed\") |>\n  gf_labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Residuals vs. fitted values\"\n  )\n```\n:::\n\n\n\n## Non-linear relationships\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n::: columns\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slr-conditions_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slr-conditions_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n:::\n\n## Violations of Linearity\n\n- Impact: inference relies on estimates of $\\sigma_\\epsilon$ computed from residuals:\n  + Residuals will be larger in certain places so estimates will be inaccurate\n  + Therefore, inference (i.e. CIs and p-values) will be inaccurate\n  + Most importantly... your predictions will be wrong most of the time\n- Remedy: transform your data (to come)\n\n. . .\n\nComplete Exercises 1-3\n\n## Constant variance {.midi}\n\n-   If the spread of the distribution of $Y$ is equal for all values of $X$ then the spread of the residuals should be approximately equal for each value of $X$\n\n-   To assess this, we can look at a plot of the residuals vs. the fitted values\n\n-   The vertical spread of the residuals should be approximately equal as you move from left to right\n\n-   **CAREFUL**: Inconsistent distribution of $X$s can make it seem as if there is non-constant variance\n\n## Constant variance\n\n‚úÖ The vertical spread of the residuals is relatively constant across the plot\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slr-conditions_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n## Non-constant variance: Fan-Pattern {.smaller}\n\n::: columns\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slr-conditions_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slr-conditions_files/figure-revealjs/unnamed-chunk-10-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n:::\n\n- Constant variance is frequently violated when the error/variance is proportional to the response variable\n- Whose wealth fluctuates more per day... Dr. Friedlander's or Elon Musk's?\n\n## Violations of Constant Variance\n\n- Impact: inference relies on estimates of $\\sigma_\\epsilon$ computed from residuals:\n  + Residuals will be larger in certain places so estimates will be inaccurate\n  + Therefore, inference (i.e. CIs and p-values) will be inaccurate\n- Remedy: transform your data (to come)\n\n. . .\n\nComplete Exercises 4\n\n## Normality {.smaller}\n\n-   The linear model assumes that the distribution of $Y$ is Normal for every value of $X$\n\n-   This is impossible to check in practice, so we will look at the overall distribution of the residuals to assess if the normality assumption is satisfied\n\n-   A histogram of the residuals should look approximately normal, symmetric, without any huge outliers\n\n-   A normal QQ-plot falls along a diagonal line\n\n-   Most inferential methods for regression are robust to some departures from normality, so we can proceed with inference if the sample size is sufficiently large, roughly $n > 30$ depending on how non-normal your residuals look\n\n    +   **Notable exception:** predictions intervals!\n\n## Check normality using a histogram\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slr-conditions_files/figure-revealjs/unnamed-chunk-11-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n## Check normality using a QQ-plot {.smaller}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ngf_qq(~.resid, data = spotify_aug) |>\n  gf_qqline() |>\n  gf_labs(x = \"Theoretical quantile\",\n       y = \"Observed quantile\",\n       title = \"Normal QQ-plot of residuals\")\n```\n\n::: {.cell-output-display}\n![](05-slr-conditions_files/figure-revealjs/unnamed-chunk-12-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n-   $x$-axis: quantile we would expect from a true normal distribution\n\n-   $y$-axis: quantile we observe in the data\n\n-   Bell-shaped does not necessarily equal normal... QQ-plot can detect distributions with heavier (i.e. more spread out) tails than a normal distribution\n\n## Check normality using a QQ-plot {.smaller}\n\n::: columns\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ngf_histogram(~.resid, data = spotify_aug,\n             bins=7, color = \"white\") |>\n  gf_labs(\n    x = \"Residual\",\n    y = \"Count\",\n    title = \"Histogram of residuals\"\n  )\n```\n\n::: {.cell-output-display}\n![](05-slr-conditions_files/figure-revealjs/unnamed-chunk-13-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ngf_qq(~.resid, data = spotify_aug) |>\n  gf_qqline() |>\n  gf_labs(x = \"Theoretical quantile\",\n       y = \"Observed quantile\",\n       title = \"Normal QQ-plot of residuals\")\n```\n\n::: {.cell-output-display}\n![](05-slr-conditions_files/figure-revealjs/unnamed-chunk-14-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n:::\n:::\n\n-   Assess whether residuals lie along the diagonal line of the Quantile-quantile plot (QQ-plot).\n\n-   If so, the residuals are normally distributed.\n\n-   Note: QQ-Plots are pretty sensitive so it doesn't take too much departure to conclude non-normality\n\n## Normality {.smaller}\n\n‚ùå The residuals do not appear to follow a normal distribution, because the points do not lie on the diagonal line, so normality is not satisfied.\n\n‚úÖ The sample size  $n =  508>  30$, so the sample size is large enough to relax this condition and proceed with inference (mostly).\n\n## Violations of Normality\n\n- Impact: depends what you want to do and how large your sample size is\n    + Your predictions intervals will be wrong... they will be symmetric when they shouldn't be...\n    + If you have a large sample size not a big deal for anything else\n- Remedy... depends on what you want to do\n    + If sample size is large enough and don't care about prediction intervals... do nothing\n    + Otherwise, transform data... (hard for small sample sizes)\n    \n. . .\n\nComplete Exercises 5 and 6.\n\n\n## Independence {.midi}\n\n-   We can often check the independence assumption based on the context of the data and how the observations were collected\n\n-   Two common violations of the independence assumption:\n\n    -   **Temporal Correlation**: If the data were collected over time, plot the residuals in time order to see if there is a pattern (serial correlation)\n\n    -   **Cluster Effect**: If there are subgroups represented in the data that are not accounted for in the model, you can color the points in the residual plots by group to see if the model systematically over or under predicts for a particular subgroup\n    \n    -   **Spatial Correlation**: If observations that were close to one another are more correlated than ones which are far apart then independent is violated\n\n## Independence \n\nComplete Exercise 7\n\n. . .\n\n‚ùå Based on the information we have,  it's  unlikely the data are independent. \n\n## Violations of Independence \n\n- Impact: depends on how it's violated\n    + In some cases you'll underestimate p-values (too many false positives) and make CIs which are too narrow\n    + In other cases you'll do the opposite\n- Remedy... depends on the source and type of dependence\n    + Add variable accounting for dependence to model\n    + Otherwise, beyond the scope of this class\n      + Time-Series Analysis\n      + Mixed-Effects Models\n      + Spacial Statistics\n\n# Model evaluation\n\n## Partitioning Variability\n\nLet's think about variation:\n\n:::{.incremental}\n- DATA = MODEL + ERROR\n- $\\substack{\\text{Variation} \\\\ \\text{in Y}} = \\substack{\\text{Variation explained} \\\\ \\text{by model}} + \\substack{\\text{Variation not explained} \\\\ \\text{by model}}$\n:::\n\n## Partitioning Variability (ANOVA)\n\n:::{.incremental}\n-   $y_i - \\bar{y} = (\\hat{y}_i - \\bar{y}) + (y_i-\\hat{y}_i)$\n-   Square and sum: $\\sum(y_i-\\bar{y})^2 = \\sum(\\hat{y} - \\bar{y})^2 + \\sum(y-\\hat{y})^2$\n-   $\\substack{\\text{Sum of squares} \\\\ \\text{Total}} = \\substack{\\text{Sum of squares} \\\\ \\text{model}} + \\substack{\\text{Sum of squares} \\\\ \\text{error}}$\n-   $SSTotal = SSModel + SSE$\n-   $SST = SSM + SSE$\n:::\n\n## ANOVA in R\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nspotify_fit |>\n  anova() |>\n  tidy() |>\n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|term         |  df|     sumsq|    meansq| statistic|   p.value|\n|:------------|---:|---------:|---------:|---------:|---------:|\n|duration_min |   1| 0.1685294| 0.1685294|  9.928516| 0.0017237|\n|Residuals    | 506| 8.5889829| 0.0169743|        NA|        NA|\n\n\n:::\n:::\n\n\n\n:::{.fragment}\n- More on this later in the semester\n- Complete Exercise 8.\n:::\n\n## Recall: Correlation Coefficient\n\n-   The **correlation coefficient**, $r$, is a number between -1 and +1 that measures how strong the linear relationship between two variables $x$ and $y$ is.\n\n$$\nr = \\frac{\\sum(x_i - \\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum(x_i-\\bar{x})^2\\sum(y_i-\\bar{y})^2}}\n= \\frac{\\sum(x_i - \\bar{x})(y_i-\\bar{y})}{s_xs_y}\n$$\n\n\n## Two statistics: $R^2$ {.smaller}\n\n::: incremental\n-   **R-squared**, $R^2$, **Coefficient of Determination** : Percentage of variability in the outcome explained by the regression model (in the context of SLR, the predictor)\n$$\nR^2 = Cor(y, \\hat{y})^2\n$$\n    -  Also called **PRE (Percent Reduction in Error)** because:\n$$\nR^2 = \\frac{SSModel}{SSTotal}\n$$\n:::\n\n\n## Two statistics: RMSE {.smaller}\n\n-   **Root mean square error, RMSE**: A measure of the average error (average difference between observed and predicted values of the outcome)\n$$\nRMSE = \\sqrt{\\frac{\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}{n}}\n$$\n    + Sometimes people just case about numerator (SSE) or version without the square-root (MSE)\n    + Sometimes the denominator may have $n-1$ instead\n\n. . .\n\n::: question\nWhat indicates a good model fit? Higher or lower $R^2$? Higher or lower RMSE?\n:::\n\n## $R^2$\n\n::: incremental\n-   Ranges between 0 (terrible predictor) and 1 (perfect predictor)\n-   Has no units\n-   Calculate with `rsq()` from `yardstick` package using the augmented data:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(yardstick)\nspotify_aug <- augment(spotify_fit)\n\nrsq(spotify_aug, truth = danceability, estimate = .fitted) |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric |.estimator | .estimate|\n|:-------|:----------|---------:|\n|rsq     |standard   |  0.019244|\n\n\n:::\n:::\n\n\n:::\n\n## Interpreting $R^2$ {.smaller}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n::: poll\nüó≥Ô∏è **Discussion**\n\n::: midi\n::: poll\nThe $R^2$ of the model for `danceability` from `Average_Income_K` is 1.9%. Which of the following is the correct interpretation of this value?\n:::\n\n1.  `duration_min` correctly predicts 1.9% of `danceability`.\n2.  1.9% of the variability in `danceability` can be explained by `duration_min`.\n3.  1.9% of the variability in `duration_min` can be explained by `danceability`.\n4.  1.9% of the time `danceability` can be predicted by `duration_min`.\n:::\n:::\n\nComplete Exercise 9.\n\n## Activity\n\n::: appex\nIn groups, at the board, design a simulation-based procedure for producing a p-value for the following hypothesis test.\n\n- $H_0: R^2 = 0$\n- $H_A: R^2 \\neq 0$\n:::\n\n## RMSE {.smaller}\n\n::: incremental\n-   Ranges between 0 (perfect predictor) and infinity (terrible predictor)\n\n-   Same units as the response variable\n\n-   Interpretation (kind of): how much does my model miss by, on average.\n\n-   Calculate with  `rmse()` from `yardstick` package using the augmented data:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrmse(spotify_aug, truth = danceability, estimate = .fitted)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       0.130\n```\n\n\n:::\n:::\n\n\n:::\n\n-   Complete Exercise 10.\n\n## Using the word \"Good\"\n\n-   There is no such thing as a \"Good\" $R^2$ or, especially, RMSE without context\n-   Whether your model is a \"Good\" model depends on many things:\n    +   What are you using your model for?\n    +   How good are other models?\n\n## Recap {.smaller}\n\nUsed residual plots to check conditions for SLR:\n\n::: columns\n::: {.column width=\"50%\"}\n::: nonincremental\n-   Linearity (residuals vs fitted vals)\n-   Constant variance (residuals vs fitted vals)\n:::\n:::\n\n::: {.column width=\"50%\"}\n::: nonincremental\n-   Normality (histogram/QQ-plot of residuals)\n-   Independence (knowledge of data collection)\n:::\n:::\n:::\n\n. . .\n\nNote: Predictions are still valid as long as linearity is met but p-values and CIs are not without other three\n\n. . .\n\n-   Can decompose total variation (SST) into variation explained by the model (SSM) and leftover variation (SSE)\n-   Two metrics for evaluating and comparing models: \n    +   $R^2$: What proportion of the variation in the response variable is explained by the model?\n    +   $RMSE$: How far is does my model miss by on average?\n\n\n",
    "supporting": [
      "05-slr-conditions_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}