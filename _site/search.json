[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Lecture\nMon-Fri 1:00 - 3:30pm\nCruzen-Murray Library (CML) 105\n\n\nOffice Hours\nMon-Fri 11:00am-12:00pm\nBoone 126B\n\n\n\nOffice hours are also available by appointment, just email me!\n\n\n\n\nInstructor: Professor Eric Friedlander\nOffice: Boone 126B\nEmail: efriedlander@collegeofidaho.edu",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-info",
    "href": "syllabus.html#course-info",
    "title": "Syllabus",
    "section": "",
    "text": "Lecture\nMon-Fri 1:00 - 3:30pm\nCruzen-Murray Library (CML) 105\n\n\nOffice Hours\nMon-Fri 11:00am-12:00pm\nBoone 126B\n\n\n\nOffice hours are also available by appointment, just email me!\n\n\n\n\nInstructor: Professor Eric Friedlander\nOffice: Boone 126B\nEmail: efriedlander@collegeofidaho.edu",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-learning-objectives",
    "href": "syllabus.html#course-learning-objectives",
    "title": "Syllabus",
    "section": "Course Learning Objectives",
    "text": "Course Learning Objectives\nBy the end of the semester, you will be able to…\n\nanalyze real-world data to answer questions about multivariable relationships.\nuse R to fit and evaluate linear and logistic regression models.\nassess whether a proposed model is appropriate and describe its limitations.\nuse Quarto to write reproducible reports.\neffectively communicate statistical results through writing and oral presentations.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-community",
    "href": "syllabus.html#course-community",
    "title": "Syllabus",
    "section": "Course community",
    "text": "Course community\n\nCollege of Idaho Honor Code\n\nThe College of Idaho maintains that academic honesty and integrity are essential values in the educational process. Operating under an Honor Code philosophy, the College expects conduct rooted in honesty, integrity, and understanding, allowing members of a diverse student body to live together and interact and learn from one another in ways that protect both personal freedom and community standards. Violations of academic honesty are addressed primarily by the instructor and may be referred to the Student Judicial Board.\n\nBy participating in this course, you are agreeing that all your work and conduct will be in accordance with the College of Idaho Honor Code.\n\n\nDisability Accommodation Statement\nThe College of Idaho seeks to provide an educational environment that is accessible to the needs of students with disabilities. The College provides reasonable services to enrolled students who have a documented permanent or temporary physical, psychological, learning, intellectual, or sensory disability that qualifies the student for accommodations under the Americans with Disabilities Act or section 504 of the Rehabilitation Act of 1973. If you have, or think you may have, a disability that impacts your performance as a student in this class, you are encouraged to arrange support services and/or accommodations through the Department of Accessibility and Learning Excellence located in McCain 201B and available via email at accessibility@collegeofidaho.edu. Reasonable academic accommodations may be provided to students who submit appropriate and current documentation of their disability. Accommodations can be arranged only through this process and are not retroactively applied. More information can be found on the DALE webpage (https://www.collegeofidaho.edu/accessibility).\n\n\nCommunication\nAll lecture notes, assignment instructions, an up-to-date schedule, and other course materials may be found on the course website, mat212wi25.netlify.app.\nPeriodic announcements will be sent via email and will also be available through Canvas and grades will be stored in the Canvas gradebook. Please check your email regularly to ensure you have the latest announcements for the course.\n\n\nIn class agreements\nIf we discuss/agree to something in class or office hours which requires action from me (e.g. “you may turn in your homework late due to a sporting event”), you MUST send me a follow-up message. If you don’t, I will almost certainly forget, and our agreement will be considered null and void.\n\n\nGetting help in the course\n\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.\nI am here to help you be successful in the course. You are encouraged to attend office hours to ask questions about the course content and assignments. Many questions are most effectively answered as you discuss them with others, so office hours are a valuable resource. You are encouraged to use them!\nOutside of class and office hours, any general questions about course content or assignments should be posted on the class discussion forum on the Teams Discussion Forum. There is a chance another student has already asked a similar question, so please check the other posts before adding a new question. If you know the answer to a question posted in the discussion forum, you are encouraged to respond!\n\n\n\nEmail\nIf you have questions about assignment extensions or accommodations, please email efriedlander@collegeofidaho.edu. Please see Late work policy for more information. If you email me, please include “MAT 212” in the subject line. Barring extenuating circumstances, I will respond to MAT 212 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday.\nCheck out the Support page for more resources.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#textbook",
    "href": "syllabus.html#textbook",
    "title": "Syllabus",
    "section": "Textbook",
    "text": "Textbook\nThe official textbook for this course is:\n\nStat2: Modeling with Regression and ANOVA, 2nd ed. by Cannon et al.\n\nIn addition, readings may be assigned from the following texts (all freely available online).\n\nR for Data Science by Garret Grolemund and Hadley Wickham\nIntroduction to Modern Statistics by Mine Çetinkaya-Rundel and Johanna Hardin\nTidy modeling with R by Max Kuhn and Julia Silge\nBeyond Multiple Linear Regression by Paul Roback and Julie Legler",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#lectures",
    "href": "syllabus.html#lectures",
    "title": "Syllabus",
    "section": "Lectures",
    "text": "Lectures\nLectures are designed to be interactive, so you gain experience applying new concepts and learning from each other. My role as instructor is to introduce you to new methods, tools, and techniques, but it is up to you to take them and make use of them. A lot of what you do in this course will involve writing code, and coding is a skill that is best learned by doing. Therefore, as much as possible, you will be working on a variety of tasks and activities throughout each lecture and lab. You are expected to prepare for class by completing assigned readings, attend all lecture sessions, and meaningfully contribute to in-class exercises and discussion. Additionally, some lectures will feature application exercises that will be graded based on completing what we do in class.\nYou are expected to bring a laptop, tablet, or Chromebook to each class so that you can participate in the in-class exercises. Please make sure your device is fully charged before you come to class, as the number of outlets in the classroom may not be sufficient to accommodate everyone.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#activities-assessment",
    "href": "syllabus.html#activities-assessment",
    "title": "Syllabus",
    "section": "Activities & Assessment",
    "text": "Activities & Assessment\nYou will be assessed based on four components: application exercises, homework, project, and oral exams.\n\nApplication Exercises\nMost lectures will have Application Exercises (AEs) that go along with them. These exercises will give you an opportunity to practice applying the statistical concepts and code introduced in the readings and lectures. Typically, students who are present will receive full credit on AEs and do not need to turn anything in. However, there will be times where you must complete an AE outside of class and submit it. Students who are late to class or miss class entirely must turn in a .qmd and .pdf file for their AE before the start of the next lecture. Specifically, AEs from Monday lectures are due Tuesday by 1:00pm MT. Students with an excused absence will be graded for completion, students without an excused absence will be graded for correctness.\n\n\nHomework\nIn homework, you will apply what you’ve learned during lecture to complete data analysis tasks. You may discuss homework assignments with other students; however, homework should be completed and submitted individually. Similar to lab assignments, homework must be typed up using Quarto and submitted as .qmd and .pdf files in Canvas.\n\n\nOral Exams\nThere will be two oral exams in this course. Each exam will include a closed-notes component and an applied component which uses R. Through these exams you have the opportunity to demonstrate what you’ve learned in the course thus far. The exams will focus on both conceptual understanding of the content and application through analysis and computational tasks. The content of the exam will be related to the content in reading assignments, lectures, application exercises, and homework assignments. More detail about the exams will be given during the semester. Please note the following course policies:\n\nYou MUST receive at least an average of 60% on your exams to pass the class.\nStudents will an average lower than 70% may retake one of their oral exams but will be capped at 70% for their exam grade.\nANY UNCITED CODE ON YOUR HOMEWORK WHICH IS NOT COVERED IN CLASS IS ELIGIBLE FOR INCLUSION IN YOUR ORAL EXAM.\nThe R used on the exams will be basic. The purpose of including R is to ensure you know the basic R functions we use regularly, R syntax, and your R “workflow”. Do you know the basic functions? If you run into a (common) error, can you debug it yourself? Can you interpret the R Help menu? Do you understand common terminology (e.g. “argument”, “function”, “output”)?\nThe specific questions asked during the Oral Exams will be different for every student and for every attempt, although similar in style and difficulty.\n\n\n\nProject\nThe purpose of the final project is to apply what you’ve learned throughout the semester to analyze an interesting data-driven research question. The project will be completed with your in pairs, and each team will present their work through a written report and poster presentation taking place during the last day of class. More information about the project will be provided during the semester.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\nThe final course grade will be calculated as follows:\n\n\n\n\nCategory\nPercentage\n\n\n\n\nHomework\n25%\n\n\nFinal Project\n25%\n\n\nExam 01\n20%\n\n\nExam 02\n20%\n\n\nApplication Exercises\n10%\n\n\n\nNote: You must receive at least a 60% on your two exams to pass the course.\n\n\nThe final letter grade will be determined based on the following thresholds:\n\n\n\n\nLetter Grade\nFinal Course Grade\n\n\n\n\nA\n&gt;= 93\n\n\nA-\n90 - 92.99\n\n\nB+\n87 - 89.99\n\n\nB\n83 - 86.99\n\n\nB-\n80 - 82.99\n\n\nC+\n77 - 79.99\n\n\nC\n73 - 76.99\n\n\nC-\n70 - 72.99\n\n\nD+\n67 - 69.99\n\n\nD\n63 - 66.99\n\n\nD-\n60 - 62.99\n\n\nF\n&lt; 60",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#five-tips-for-success",
    "href": "syllabus.html#five-tips-for-success",
    "title": "Syllabus",
    "section": "Five tips for success",
    "text": "Five tips for success\nYour success in this course depends very much on you and the effort you put into it. The course has been organized so that the burden of learning is on you. I will help you by providing you with materials and answering questions and setting a pace, but for this to work you must do the following:\n\nComplete all the preparation work before class.\nAsk questions. As often as you can. In class, out of class. Ask me, ask your friends, ask the person sitting next to you. This will help you more than anything else. If you get a question wrong on an assessment, ask why. If you’re not sure about the homework, ask. If you hear something on the news that sounds related to what we discussed, ask. If the reading is confusing, ask.\nDo the readings.\nDo the homework. The earlier you start, the better. It’s not enough to just mechanically plow through the exercises. You should ask yourself how these exercises relate to earlier material, and imagine how they might be changed (to make questions for an exam, for example.)\nDon’t procrastinate. The content builds upon what was taught in previous weeks, so if something is confusing to you on Day 2, Day 3 will become more confusing, Day 4 even worse, etc. Don’t let the week end with unanswered questions. But if you find yourself falling behind and not knowing where to begin asking, come to office hours and I can help you identify a good (re)starting point.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "Syllabus",
    "section": "Course policies",
    "text": "Course policies\n\nAcademic honesty\nTL;DR: Don’t cheat!\n\nThe homework assignments must be completed individually but you are welcome to discuss the assignment with classmates (e.g., discuss what’s the best way for approaching a problem, what functions are useful for accomplishing a particular task, etc.). However you may not directly share (i.e. via copy/paste or copying) answers to homework questions (including and especially any code) with anyone other than myself.\nFor the projects, collaboration within teams is not only allowed, but expected. Communication between teams at a high level is also allowed however you may not share code or components of the project across teams.\nReusing code: Unless explicitly stated otherwise, you may make use of online resources (e.g. StackOverflow) for coding examples on assignments. If you directly use code from an outside source (or use it as inspiration), you must explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. Furthermore, ANY UNCITED CODE OR CONTENT ON YOUR HOMEWORK WHICH IS NOT COVERED IN CLASS IS ELIGIBLE FOR INCLUSION IN YOUR ORAL EXAMS.\nUse of artificial intelligence (AI): You should treat AI tools, such as ChatGPT, the same as other online resources. There are two guiding principles that govern how you can use AI in this course:1 (1) Cognitive dimension: Working with AI should not reduce your ability to think clearly. We will practice using AI to facilitate—rather than hinder—learning. (2) Ethical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity.\n\nAI tools for code: You may make use of the technology for coding examples on assignments; if you do so, you must explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. You may use these guidelines for citing AI-generated content.\nNo AI tools for narrative: Unless instructed otherwise, AI is not permitted for writing narrative on assignments. In general, you may use AI as a resource as you complete assignments but not to answer the exercises for you. You are ultimately responsible for the work you turn in; it should reflect your understanding of the course content.\n\n\nIf you are unsure if the use of a particular resource complies with the academic honesty policy, please ask.\nRegardless of course delivery format, it is the responsibility of all students to understand and follow all College of Idaho policies, including academic integrity (e.g., completing one’s own work, following proper citation of sources, adhering to guidance around group work projects, and more). Ignoring these requirements is a violation of the Honor Code.\n\n\nLate work policy\nThe due dates for assignments are there to help you keep up with the course material and to ensure the teaching team can provide feedback within a timely manner. I understand that things come up periodically that could make it difficult to submit an assignment by the deadline.\n\nLate Homework: There will be a 5% deduction for each 24-hour period the assignment is late for the first two days. After 2 days, students will receive a 30% reduction. No homework will be accepted after it is returned to the class\nLate Application Exercises: AEs are due the day after the class they are assigned. No late work is accepted for application exercises, since these are designed as in-class activities to help you prepare for homework.\nSchool-Sponsored Events/Illness: If an application exercise, exam, or project must be missed due to a school-sponsored event, you must let me know at least a week ahead of time so that we can schedule a time for you to make up the work before you leave. If you must miss a exam or a project presentation due to illness, you must let me know before class that day so that we can schedule a time for you to take a make-up quiz or exam. Failure to adhere to this policy will result in a 35% penalty on the corresponding assignment.\n\n\n\nRegrade Requests\nRegrade requests must be submitted via email within a week of when an assignment is returned. Regrade requests will be considered if there was an error in the grade calculation or if you feel a correct answer was mistakenly marked as incorrect. Requests to dispute the number of points deducted for an incorrect response will not be considered. Note that by submitting a regrade request, the entire question will be graded which could potentially result in losing points.\nNo grades will be changed after the final project presentations.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#important-dates",
    "href": "syllabus.html#important-dates",
    "title": "Syllabus",
    "section": "Important dates",
    "text": "Important dates\n\nJan 3: Classes begin\nJan 7: Last day to drop\nJan 17: Last day to withdraw with W or elect Pass/Fail\nJan 20: Martin Luther King Day - NO CLASSES\nJan 24: Classes end",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThese guiding principles are based on Course Policies related to ChatGPT and other AI Tools developed by Joel Gladd, Ph.D.↩︎↩︎",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "slides/13-multicollinearity.html#first-things-first",
    "href": "slides/13-multicollinearity.html#first-things-first",
    "title": "Multicollinearity",
    "section": "First things first",
    "text": "First things first\n\nFinish-up AE-16"
  },
  {
    "objectID": "slides/13-multicollinearity.html#announcements",
    "href": "slides/13-multicollinearity.html#announcements",
    "title": "Multicollinearity",
    "section": "Announcements",
    "text": "Announcements\n\nProject: EDA Due Today\nProject: Paper Due November 18th\nOral R Quiz\n\n\n📋 AE 17 - Multicollinearity\n\nOpen up AE 17 and Complete Exercise 0"
  },
  {
    "objectID": "slides/13-multicollinearity.html#topics",
    "href": "slides/13-multicollinearity.html#topics",
    "title": "Multicollinearity",
    "section": "Topics",
    "text": "Topics\n\nDefining Multicollinearity\nDetecting Multicollinearity\nVariance Inflation Factors"
  },
  {
    "objectID": "slides/13-multicollinearity.html#computational-setup",
    "href": "slides/13-multicollinearity.html#computational-setup",
    "title": "Multicollinearity",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(mosaic)\nlibrary(mosaicData)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(scales)\nlibrary(countdown)\nlibrary(rms)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))"
  },
  {
    "objectID": "slides/13-multicollinearity.html#data-rail_trail",
    "href": "slides/13-multicollinearity.html#data-rail_trail",
    "title": "Multicollinearity",
    "section": "Data: rail_trail",
    "text": "Data: rail_trail\n\nThe Pioneer Valley Planning Commission (PVPC) collected data for ninety days from April 5, 2005 to November 15, 2005.\nData collectors set up a laser sensor, with breaks in the laser beam recording when a rail-trail user passed the data collection station.\n\n\n\n# A tibble: 90 × 7\n   volume hightemp avgtemp season cloudcover precip day_type\n    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   \n 1    501       83    66.5 Summer       7.60 0      Weekday \n 2    419       73    61   Summer       6.30 0.290  Weekday \n 3    397       74    63   Spring       7.5  0.320  Weekday \n 4    385       95    78   Summer       2.60 0      Weekend \n 5    200       44    48   Spring      10    0.140  Weekday \n 6    375       69    61.5 Spring       6.60 0.0200 Weekday \n 7    417       66    52.5 Spring       2.40 0      Weekday \n 8    629       66    52   Spring       0    0      Weekend \n 9    533       80    67.5 Summer       3.80 0      Weekend \n10    547       79    62   Summer       4.10 0      Weekday \n# ℹ 80 more rows\n\n\nSource: Pioneer Valley Planning Commission via the mosaicData package."
  },
  {
    "objectID": "slides/13-multicollinearity.html#full-model",
    "href": "slides/13-multicollinearity.html#full-model",
    "title": "Multicollinearity",
    "section": "Full model",
    "text": "Full model\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n17.622161\n76.582860\n0.2301058\n0.8185826\n\n\nhightemp\n7.070528\n2.420523\n2.9210743\n0.0045045\n\n\navgtemp\n-2.036685\n3.142113\n-0.6481896\n0.5186733\n\n\nseasonSpring\n35.914983\n32.992762\n1.0885716\n0.2795319\n\n\nseasonSummer\n24.153571\n52.810486\n0.4573632\n0.6486195\n\n\ncloudcover\n-7.251776\n3.843071\n-1.8869743\n0.0627025\n\n\nprecip\n-95.696525\n42.573359\n-2.2478030\n0.0272735\n\n\nday_typeWeekend\n35.903750\n22.429056\n1.6007696\n0.1132738"
  },
  {
    "objectID": "slides/13-multicollinearity.html#what-is-multicollinearity",
    "href": "slides/13-multicollinearity.html#what-is-multicollinearity",
    "title": "Multicollinearity",
    "section": "What is multicollinearity",
    "text": "What is multicollinearity\n\nMulticollinearity is the case when one or more predictor variables are strongly correlated with some combination of other predictors\nIntuition: if you could fit a good linear model with one of your predictors as the response and the rest of the predictors as your explanatory variables, then your predictors are exhibiting multicollinearity"
  },
  {
    "objectID": "slides/13-multicollinearity.html#example",
    "href": "slides/13-multicollinearity.html#example",
    "title": "Multicollinearity",
    "section": "Example",
    "text": "Example\nLet’s assume the true population regression equation is \\(y = 3 + 4x\\)\n\nSuppose we try estimating that equation using a model with variables \\(x\\) and \\(z = x/10\\)\n\\[\n\\begin{aligned}\\hat{y}&= \\hat{\\beta}_0 + \\hat{\\beta}_1x  + \\hat{\\beta}_2z\\\\\n&= \\hat{\\beta}_0 + \\hat{\\beta}_1x  + \\hat{\\beta}_2\\frac{x}{10}\\\\\n&= \\hat{\\beta}_0 + \\bigg(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10}\\bigg)x\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/13-multicollinearity.html#example-1",
    "href": "slides/13-multicollinearity.html#example-1",
    "title": "Multicollinearity",
    "section": "Example",
    "text": "Example\n\\[\\hat{y} = \\hat{\\beta}_0 + \\bigg(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10}\\bigg)x\\]\n\nWe can set \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\) to any two numbers such that \\(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10} = 4\\)\nTherefore, we are unable to choose the “best” combination of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\)\nIn statistics, we say this model is “unidentifiable” because different parameters combinations can result in the same model\nThis is also why we need to set a reference level for categorical variables\nComplete Exercises 1-2."
  },
  {
    "objectID": "slides/13-multicollinearity.html#why-multicollinearity-is-a-problem",
    "href": "slides/13-multicollinearity.html#why-multicollinearity-is-a-problem",
    "title": "Multicollinearity",
    "section": "Why multicollinearity is a problem",
    "text": "Why multicollinearity is a problem\n\nWhen we have perfect collinearities, we are unable to get estimates for the coefficients\n\nWhen we have almost perfect collinearities (i.e. highly correlated predictor variables), the standard errors for our regression coefficients inflate\nIn other words, we lose precision in our estimates of the regression coefficients\nThis impedes our ability to use the model for inference\nIt is also difficult to interpret the model coefficients"
  },
  {
    "objectID": "slides/13-multicollinearity.html#detecting-multicollinearity",
    "href": "slides/13-multicollinearity.html#detecting-multicollinearity",
    "title": "Multicollinearity",
    "section": "Detecting Multicollinearity",
    "text": "Detecting Multicollinearity\nMulticollinearity may occur when…\n\nThere are very high correlations \\((r &gt; 0.9)\\) among two or more predictor variables, especially when the sample size is small\nOne (or more) predictor variables is an almost perfect linear combination of the others\nThere are interactions between two or more continuous variables"
  },
  {
    "objectID": "slides/13-multicollinearity.html#detecting-multicollinearity-in-the-eda",
    "href": "slides/13-multicollinearity.html#detecting-multicollinearity-in-the-eda",
    "title": "Multicollinearity",
    "section": "Detecting multicollinearity in the EDA",
    "text": "Detecting multicollinearity in the EDA\n\nLook at a correlation matrix of the predictor variables, including all indicator variables\n\nLook out for values close to 1 or -1\n\nLook at a scatter plot matrix of the predictor variables\n\nLook out for plots that show a relatively linear relationship\n\nComplete Exercises 3-4."
  },
  {
    "objectID": "slides/13-multicollinearity.html#detecting-multicollinearity-vif",
    "href": "slides/13-multicollinearity.html#detecting-multicollinearity-vif",
    "title": "Multicollinearity",
    "section": "Detecting Multicollinearity (VIF)",
    "text": "Detecting Multicollinearity (VIF)\nVariance Inflation Factor (VIF): Measure of multicollinearity in the regression model\n\\[VIF(\\hat{\\beta}_j) = \\frac{1}{1-R^2_{X_j|X_{-j}}}\\]\nwhere \\(R^2_{X_j|X_{-j}}\\) is the proportion of variation in \\(X_j\\) that is explained by the linear combination of the other explanatory variables in the model."
  },
  {
    "objectID": "slides/13-multicollinearity.html#detecting-multicollinearity-vif-1",
    "href": "slides/13-multicollinearity.html#detecting-multicollinearity-vif-1",
    "title": "Multicollinearity",
    "section": "Detecting Multicollinearity (VIF)",
    "text": "Detecting Multicollinearity (VIF)\n\nTypically \\(VIF &gt; 10\\) indicates concerning multicollinearity\nVariables with similar values of VIF are typically the ones correlated with each other\nUse the vif() function in the rms R package to calculate VIF"
  },
  {
    "objectID": "slides/13-multicollinearity.html#vif-for-rail-trail-model",
    "href": "slides/13-multicollinearity.html#vif-for-rail-trail-model",
    "title": "Multicollinearity",
    "section": "VIF for rail trail model",
    "text": "VIF for rail trail model\nComplete Exercise 5.\n\n\nvif(rt_full_fit)\n\n       hightemp         avgtemp    seasonSpring    seasonSummer      cloudcover \n      10.259978       13.086175        2.751577        5.841985        1.587485 \n         precip day_typeWeekend \n       1.295352        1.125741 \n\n\n\n\n\nhightemp and avgtemp are correlated."
  },
  {
    "objectID": "slides/13-multicollinearity.html#what-to-do-about-multicollinearity",
    "href": "slides/13-multicollinearity.html#what-to-do-about-multicollinearity",
    "title": "Multicollinearity",
    "section": "What to do about Multicollinearity",
    "text": "What to do about Multicollinearity\n\nDrop some predictors.\n\nExample: Remove one of these variables and refit the model.\n\nCombine some predictors.\n\nExample: Create a new variable temp_comsite that is the average of avgtemp and hightemp.\n\nDiscount the individual coefficients and t-tests.\n\nExample: Think about avgtemp and hightemp together with their individual \\(\\beta\\)’s and p-values not having much meaning.\n\n\n\nComplete Exercises 6 & 7."
  },
  {
    "objectID": "slides/13-multicollinearity.html#model-without-hightemp",
    "href": "slides/13-multicollinearity.html#model-without-hightemp",
    "title": "Multicollinearity",
    "section": "Model without hightemp",
    "text": "Model without hightemp\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n76.071\n77.204\n0.985\n0.327\n\n\navgtemp\n6.003\n1.583\n3.792\n0.000\n\n\nseasonSpring\n34.555\n34.454\n1.003\n0.319\n\n\nseasonSummer\n13.531\n55.024\n0.246\n0.806\n\n\ncloudcover\n-12.807\n3.488\n-3.672\n0.000\n\n\nprecip\n-110.736\n44.137\n-2.509\n0.014\n\n\nday_typeWeekend\n48.420\n22.993\n2.106\n0.038"
  },
  {
    "objectID": "slides/13-multicollinearity.html#model-without-avgtemp",
    "href": "slides/13-multicollinearity.html#model-without-avgtemp",
    "title": "Multicollinearity",
    "section": "Model without avgtemp",
    "text": "Model without avgtemp\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.421\n74.992\n0.112\n0.911\n\n\nhightemp\n5.696\n1.164\n4.895\n0.000\n\n\nseasonSpring\n31.239\n32.082\n0.974\n0.333\n\n\nseasonSummer\n9.424\n47.504\n0.198\n0.843\n\n\ncloudcover\n-8.353\n3.435\n-2.431\n0.017\n\n\nprecip\n-98.904\n42.137\n-2.347\n0.021\n\n\nday_typeWeekend\n37.062\n22.280\n1.663\n0.100"
  },
  {
    "objectID": "slides/13-multicollinearity.html#model-without-temp_composite",
    "href": "slides/13-multicollinearity.html#model-without-temp_composite",
    "title": "Multicollinearity",
    "section": "Model without temp_composite",
    "text": "Model without temp_composite\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n18.823\n77.430\n0.243\n0.809\n\n\nseasonSpring\n28.458\n33.059\n0.861\n0.392\n\n\nseasonSummer\n-0.986\n51.234\n-0.019\n0.985\n\n\ncloudcover\n-10.367\n3.409\n-3.041\n0.003\n\n\nprecip\n-104.475\n42.725\n-2.445\n0.017\n\n\nday_typeWeekend\n40.914\n22.479\n1.820\n0.072\n\n\ntemp_composite\n6.292\n1.376\n4.571\n0.000"
  },
  {
    "objectID": "slides/13-multicollinearity.html#choosing-a-model",
    "href": "slides/13-multicollinearity.html#choosing-a-model",
    "title": "Multicollinearity",
    "section": "Choosing a model",
    "text": "Choosing a model\nModel without hightemp:\n\n\n\n\n\nadj.r.squared\nAIC\nBIC\n\n\n\n\n0.42\n1087.5\n1107.5\n\n\n\n\n\nModel without avgtemp:\n\n\n\n\n\nadj.r.squared\nAIC\nBIC\n\n\n\n\n0.47\n1079.05\n1099.05\n\n\n\n\n\nModel with temp_composite:\n\n\n\n\n\nadj.r.squared\nAIC\nBIC\n\n\n\n\n0.46\n1081.67\n1101.67\n\n\n\n\n\n\nBased on Adjusted \\(R^2\\), AIC, and BIC, the model without avgtemp is a better fit. Therefore, we choose to remove avgtemp from the model and leave hightemp in the model to deal with the multicollinearity."
  },
  {
    "objectID": "slides/13-multicollinearity.html#selected-model-for-now",
    "href": "slides/13-multicollinearity.html#selected-model-for-now",
    "title": "Multicollinearity",
    "section": "Selected model (for now)",
    "text": "Selected model (for now)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.421\n74.992\n0.112\n0.911\n\n\nhightemp\n5.696\n1.164\n4.895\n0.000\n\n\nseasonSpring\n31.239\n32.082\n0.974\n0.333\n\n\nseasonSummer\n9.424\n47.504\n0.198\n0.843\n\n\ncloudcover\n-8.353\n3.435\n-2.431\n0.017\n\n\nprecip\n-98.904\n42.137\n-2.347\n0.021\n\n\nday_typeWeekend\n37.062\n22.280\n1.663\n0.100"
  },
  {
    "objectID": "slides/11-comparison.html#announcements",
    "href": "slides/11-comparison.html#announcements",
    "title": "Model comparison",
    "section": "Announcements",
    "text": "Announcements\n\n📋 AE 11 - Model Comparison\n\nOpen up AE 11 and complete Exercises 0-2"
  },
  {
    "objectID": "slides/11-comparison.html#topics",
    "href": "slides/11-comparison.html#topics",
    "title": "Model comparison",
    "section": "Topics",
    "text": "Topics\n\nANOVA for multiple linear regression and sum of squares\nComparing models with \\(R^2\\) vs. \\(R^2_{ajd}\\)\nComparing models with AIC and BIC\nOccam’s razor and parsimony"
  },
  {
    "objectID": "slides/11-comparison.html#computational-setup",
    "href": "slides/11-comparison.html#computational-setup",
    "title": "Model comparison",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(yardstick)\nlibrary(ggformula)\nlibrary(supernova)\nlibrary(tidymodels)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(janitor)\nlibrary(kableExtra)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/11-comparison.html#data-restaurant-tips",
    "href": "slides/11-comparison.html#data-restaurant-tips",
    "title": "Model comparison",
    "section": "Data: Restaurant tips",
    "text": "Data: Restaurant tips\nWhich variables help us predict the amount customers tip at a restaurant?\n\n\n# A tibble: 169 × 4\n     Tip Party Meal   Age   \n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; \n 1  2.99     1 Dinner Yadult\n 2  2        1 Dinner Yadult\n 3  5        1 Dinner SenCit\n 4  4        3 Dinner Middle\n 5 10.3      2 Dinner SenCit\n 6  4.85     2 Dinner Middle\n 7  5        4 Dinner Yadult\n 8  4        3 Dinner Middle\n 9  5        2 Dinner Middle\n10  1.58     1 Dinner SenCit\n# ℹ 159 more rows"
  },
  {
    "objectID": "slides/11-comparison.html#variables",
    "href": "slides/11-comparison.html#variables",
    "title": "Model comparison",
    "section": "Variables",
    "text": "Variables\nPredictors:\n\nParty: Number of people in the party\nMeal: Time of day (Lunch, Dinner, Late Night)\nAge: Age category of person paying the bill (Yadult, Middle, SenCit)\n\nOutcome: Tip: Amount of tip"
  },
  {
    "objectID": "slides/11-comparison.html#outcome-tip",
    "href": "slides/11-comparison.html#outcome-tip",
    "title": "Model comparison",
    "section": "Outcome: Tip",
    "text": "Outcome: Tip"
  },
  {
    "objectID": "slides/11-comparison.html#predictors",
    "href": "slides/11-comparison.html#predictors",
    "title": "Model comparison",
    "section": "Predictors",
    "text": "Predictors"
  },
  {
    "objectID": "slides/11-comparison.html#relevel-categorical-predictors",
    "href": "slides/11-comparison.html#relevel-categorical-predictors",
    "title": "Model comparison",
    "section": "Relevel categorical predictors",
    "text": "Relevel categorical predictors\n\ntips &lt;- tips |&gt;\n  mutate(\n    Meal = fct_relevel(Meal, \"Lunch\", \"Dinner\", \"Late Night\"),\n    Age  = fct_relevel(Age, \"Yadult\", \"Middle\", \"SenCit\")\n  )"
  },
  {
    "objectID": "slides/11-comparison.html#predictors-again",
    "href": "slides/11-comparison.html#predictors-again",
    "title": "Model comparison",
    "section": "Predictors, again",
    "text": "Predictors, again"
  },
  {
    "objectID": "slides/11-comparison.html#outcome-vs.-predictors",
    "href": "slides/11-comparison.html#outcome-vs.-predictors",
    "title": "Model comparison",
    "section": "Outcome vs. predictors",
    "text": "Outcome vs. predictors"
  },
  {
    "objectID": "slides/11-comparison.html#fit-and-summarize-model",
    "href": "slides/11-comparison.html#fit-and-summarize-model",
    "title": "Model comparison",
    "section": "Fit and summarize model",
    "text": "Fit and summarize model\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.170\n0.366\n-0.465\n0.643\n\n\nParty\n1.837\n0.124\n14.758\n0.000\n\n\nAgeMiddle\n1.009\n0.408\n2.475\n0.014\n\n\nAgeSenCit\n1.388\n0.485\n2.862\n0.005\n\n\n\n\n\n\n\n\nIs this model good?"
  },
  {
    "objectID": "slides/11-comparison.html#another-model-summary",
    "href": "slides/11-comparison.html#another-model-summary",
    "title": "Model comparison",
    "section": "Another model summary",
    "text": "Another model summary\n\nanova(tip_fit) |&gt;\n  tidy() |&gt;\n  kable(digits = 2)\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nParty\n1\n1188.64\n1188.64\n285.71\n0.00\n\n\nAge\n2\n38.03\n19.01\n4.57\n0.01\n\n\nResiduals\n165\n686.44\n4.16\nNA\nNA"
  },
  {
    "objectID": "slides/11-comparison.html#analysis-of-variance-anova-1",
    "href": "slides/11-comparison.html#analysis-of-variance-anova-1",
    "title": "Model comparison",
    "section": "Analysis of variance (ANOVA)",
    "text": "Analysis of variance (ANOVA)"
  },
  {
    "objectID": "slides/11-comparison.html#anova",
    "href": "slides/11-comparison.html#anova",
    "title": "Model comparison",
    "section": "ANOVA",
    "text": "ANOVA\n\nMain Idea: Decompose the total variation of the outcome into:\n\nthe variation that can be explained by the each of the variables in the model\nthe variation that can’t be explained by the model (left in the residuals)\n\n\\(SS_{Total}\\): Total sum of squares, variability of outcome, \\(\\sum_{i = 1}^n (y_i - \\bar{y})^2\\)\n\\(SS_{Error}\\): Residual sum of squares, variability of residuals, \\(\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2\\)\n\\(SS_{Model} = SS_{Total} - SS_{Error}\\): Variability explained by the model, \\(\\sum_{i = 1}^n (\\hat{y}_i - \\bar{y})^2\\)\n\n\nComplete Exercise 3."
  },
  {
    "objectID": "slides/11-comparison.html#anova-output-in-r1",
    "href": "slides/11-comparison.html#anova-output-in-r1",
    "title": "Model comparison",
    "section": "ANOVA output in R1",
    "text": "ANOVA output in R1\n\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nParty\n1\n1188.63588\n1188.635880\n285.711511\n0.000000\n\n\nAge\n2\n38.02783\n19.013916\n4.570361\n0.011699\n\n\nResiduals\n165\n686.44389\n4.160266\nNA\nNA\n\n\n\n\n\nClick here for explanation about the way R calculates sum of squares for each variable."
  },
  {
    "objectID": "slides/11-comparison.html#anova-output-with-totals",
    "href": "slides/11-comparison.html#anova-output-with-totals",
    "title": "Model comparison",
    "section": "ANOVA output, with totals",
    "text": "ANOVA output, with totals\n\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nParty\n1\n1188.64\n1188.64\n285.71\n0\n\n\nAge\n2\n38.03\n19.01\n4.57\n0.01\n\n\nResiduals\n165\n686.44\n4.16\n\n\n\n\nTotal\n168\n1913.11"
  },
  {
    "objectID": "slides/11-comparison.html#sum-of-squares",
    "href": "slides/11-comparison.html#sum-of-squares",
    "title": "Model comparison",
    "section": "Sum of squares",
    "text": "Sum of squares\n\n\n\n\n\n\n\nterm\ndf\nsumsq\n\n\n\n\nParty\n1\n1188.64\n\n\nAge\n2\n38.03\n\n\nResiduals\n165\n686.44\n\n\nTotal\n168\n1913.11\n\n\n\n\n\n\n\n\\(SS_{Total}\\): Total sum of squares, variability of outcome, \\(\\sum_{i = 1}^n (y_i - \\bar{y})^2\\)\n\\(SS_{Error}\\): Residual sum of squares, variability of residuals, \\(\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2\\)\n\\(SS_{Model} = SS_{Total} - SS_{Error}\\): Variability explained by the model, \\(\\sum_{i = 1}^n (\\hat{y}_i - \\bar{y})^2\\)"
  },
  {
    "objectID": "slides/11-comparison.html#sum-of-squares-ss_total",
    "href": "slides/11-comparison.html#sum-of-squares-ss_total",
    "title": "Model comparison",
    "section": "Sum of squares: \\(SS_{Total}\\)",
    "text": "Sum of squares: \\(SS_{Total}\\)\n\n\n\n\n\nterm\ndf\nsumsq\n\n\n\n\nParty\n1\n1188.64\n\n\nAge\n2\n38.03\n\n\nResiduals\n165\n686.44\n\n\nTotal\n168\n1913.11\n\n\n\n\n\n\n\n\\(SS_{Total}\\): Total sum of squares, variability of outcome\n\n\\(\\sum_{i = 1}^n (y_i - \\bar{y})^2\\) = 1913.11"
  },
  {
    "objectID": "slides/11-comparison.html#sum-of-squares-ss_error",
    "href": "slides/11-comparison.html#sum-of-squares-ss_error",
    "title": "Model comparison",
    "section": "Sum of squares: \\(SS_{Error}\\)",
    "text": "Sum of squares: \\(SS_{Error}\\)\n\n\n\n\n\nterm\ndf\nsumsq\n\n\n\n\nParty\n1\n1188.64\n\n\nAge\n2\n38.03\n\n\nResiduals\n165\n686.44\n\n\nTotal\n168\n1913.11\n\n\n\n\n\n\n\n\\(SS_{Error}\\): Residual sum of squares, variability of residuals\n\n\\(\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2\\) = 686.44"
  },
  {
    "objectID": "slides/11-comparison.html#sum-of-squares-ss_model",
    "href": "slides/11-comparison.html#sum-of-squares-ss_model",
    "title": "Model comparison",
    "section": "Sum of squares: \\(SS_{Model}\\)",
    "text": "Sum of squares: \\(SS_{Model}\\)\n\n\n\n\n\nterm\ndf\nsumsq\n\n\n\n\nParty\n1\n1188.64\n\n\nAge\n2\n38.03\n\n\nResiduals\n165\n686.44\n\n\nTotal\n168\n1913.11\n\n\n\n\n\n\n\n\\(SS_{Model}\\): Variability explained by the model\n\n\\(\\sum_{i = 1}^n (\\hat{y}_i - \\bar{y})^2 = SS_{Model} = SS_{Total} - SS_{Error} =\\) 1226.67"
  },
  {
    "objectID": "slides/11-comparison.html#f-test-testing-the-whole-model-at-once",
    "href": "slides/11-comparison.html#f-test-testing-the-whole-model-at-once",
    "title": "Model comparison",
    "section": "F-Test: Testing the whole model at once",
    "text": "F-Test: Testing the whole model at once\nHypotheses:\n\\(H_0: \\beta_1 = \\beta_2 = \\cdots = \\beta_k = 0\\) vs. \\(H_A:\\) at least one \\(\\beta_i \\neq 0\\)\n\nTest statistic: F-statistics\n\\[\nF = \\frac{MSModel}{MSE} = \\frac{SSModel/k}{SSE/(n-k-1)} \\\\\n\\]\n\n\np-value: Probability of observing a test statistic at least as extreme (in the direction of the alternative hypothesis) from the null value as the one observed\n\\[\n\\text{p-value} = P(F &gt; \\text{test statistic}),\n\\]\ncalculated from an \\(F\\) distribution with \\(k\\) and \\(n - k - 1\\) degrees of freedom."
  },
  {
    "objectID": "slides/11-comparison.html#f-test-in-r",
    "href": "slides/11-comparison.html#f-test-in-r",
    "title": "Model comparison",
    "section": "F-test in R",
    "text": "F-test in R\n\nUse glance function from broom package\n\nstatistic: F-statistic\np.value: p-value from F-test\n\n\n\nComplete Exercise 4."
  },
  {
    "objectID": "slides/11-comparison.html#nested-models",
    "href": "slides/11-comparison.html#nested-models",
    "title": "Model comparison",
    "section": "Nested Models",
    "text": "Nested Models\n\nWe say one model is nested inside another model if all of its TERMS are present in the other model\n\n\n\nConsider three different models:\n\nModel 1: \\(Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_3 + \\epsilon\\)\nModel 2: \\(Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\epsilon\\)\nModel 3: \\(Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_1X_2 + \\epsilon\\)\n\n\n\n\n\nModel 2 is nested inside both Model 1 and Model 3.\nWhy isn’t Model 3 nested in Model 1?\nSmaller model is called the Reduced Model\nLarger model is called the Full Model (be careful, this term depends on context)\n\n\n\n\nComplete Exercises 5-7."
  },
  {
    "objectID": "slides/11-comparison.html#recall-anova-f-test",
    "href": "slides/11-comparison.html#recall-anova-f-test",
    "title": "Model comparison",
    "section": "Recall: ANOVA, F-Test",
    "text": "Recall: ANOVA, F-Test\nHypotheses:\n\\(H_0: \\beta_1 = \\beta_2 = \\cdots = \\beta_p = 0\\) vs. \\(H_A:\\) at least one \\(\\beta_i \\neq 0\\)\nTest statistic: F-statistic\n\\[\nF = \\frac{MSModel}{MSE} = \\frac{SSModel/p}{SSE/(n-p-1)} \\\\\n\\]\np-value: Probability of observing a test statistic at least as extreme (in the direction of the alternative hypothesis) from the null value as the one observed\n\\[\n\\text{p-value} = P(F &gt; \\text{test statistic}),\n\\]\ncalculated from an \\(F\\) distribution with \\(p\\) and \\(n - p - 1\\) degrees of freedom."
  },
  {
    "objectID": "slides/11-comparison.html#nested-f-test",
    "href": "slides/11-comparison.html#nested-f-test",
    "title": "Model comparison",
    "section": "Nested F-Test",
    "text": "Nested F-Test\nSuppose \\(k\\) is the number of \\(\\beta\\)’s in the nested model and \\(p\\) is the full number of predictors in the larger model. I.e. \\(\\beta_{k+1},\\ldots, \\beta_{p}\\) are the new \\(\\beta\\)’s\nHypotheses:\n\\(H_0: \\beta_{k+1} = \\beta_{k+2} = \\cdots = \\beta_p = 0\\) vs. \\(H_A:\\) at least one \\(\\beta_i \\neq 0\\) for \\(i&gt;k+1\\)\nTest statistic: F-statistic\n\\[\nF = \\frac{(SSModel_{full} - SSModel_{reduced})/(p-k)}{SSE_{full}/(n-p-1)} \\\\\n\\]\np-value: Probability of observing a test statistic at least as extreme (in the direction of the alternative hypothesis) from the null value as the one observed\n\\[\n\\text{p-value} = P(F &gt; \\text{test statistic}),\n\\]\ncalculated from an \\(F\\) distribution with \\(p-k\\) (the number of predictors being tested) and \\(n - p - 1\\) degrees of freedom.\n\nNote: Same as regular F-test if reduced model is just \\(Y= \\beta_0\\)."
  },
  {
    "objectID": "slides/11-comparison.html#nested-f-test-in-r",
    "href": "slides/11-comparison.html#nested-f-test-in-r",
    "title": "Model comparison",
    "section": "Nested F-Test in R",
    "text": "Nested F-Test in R\n\ntip_fit_1 &lt;- lm(Tip ~ Party + Age + Meal, data = tips)\ntip_fit_2 &lt;- lm(Tip ~ Party + Age + Meal + Day, data = tips)\n\nanova(tip_fit_1, tip_fit_2) |&gt; # Enter reduced model first\n  tidy() |&gt; \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\ndf.residual\nrss\ndf\nsumsq\nstatistic\np.value\n\n\n\n\nTip ~ Party + Age + Meal\n163\n622.9793\nNA\nNA\nNA\nNA\n\n\nTip ~ Party + Age + Meal + Day\n158\n607.3815\n5\n15.59778\n0.8114993\n0.543086\n\n\n\n\n\n\\[\nF = \\frac{(SSModel_{full} - SSModel_{reduced})/(p-k)}{SSE_{full}/(n-p-1)}\n= \\frac{15.59778/5}{6073815/158}\n= 0.8114993\n\\]\nLet’s interpret this together.\n\nComplete Exercise 8."
  },
  {
    "objectID": "slides/11-comparison.html#r-squared-r2",
    "href": "slides/11-comparison.html#r-squared-r2",
    "title": "Model comparison",
    "section": "R-squared, \\(R^2\\)",
    "text": "R-squared, \\(R^2\\)\nRecall: \\(R^2\\) is the proportion of the variation in the response variable explained by the regression model.\n\n\\[\nR^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Error}}{SS_{Total}}\n\\]\nComplete Exercises 9-11."
  },
  {
    "objectID": "slides/11-comparison.html#r-squared-r2-overfitting",
    "href": "slides/11-comparison.html#r-squared-r2-overfitting",
    "title": "Model comparison",
    "section": "R-squared, \\(R^2\\), Overfitting",
    "text": "R-squared, \\(R^2\\), Overfitting\n\n\\(R^2\\) will always increase as we add more variables to the model\n\nIf we add enough variables, we can usually achieve \\(R^2=100\\%\\)\nEventually our model will over-align to the noise in our data and become worse at predicting new data… this is called overfitting\n\n\nIf we only use \\(R^2\\) to choose a best fit model, we will be prone to choosing the model with the most predictor variables"
  },
  {
    "objectID": "slides/11-comparison.html#adjusted-r2",
    "href": "slides/11-comparison.html#adjusted-r2",
    "title": "Model comparison",
    "section": "Adjusted \\(R^2\\)",
    "text": "Adjusted \\(R^2\\)\n\nAdjusted \\(R^2\\): measure that includes a penalty for unnecessary predictor variables\nSimilar to \\(R^2\\), it is a measure of the amount of variation in the response that is explained by the regression model\nDiffers from \\(R^2\\) by using the mean squares (sum of squares/degrees of freedom) rather than sums of squares and therefore adjusting for the number of predictor variables"
  },
  {
    "objectID": "slides/11-comparison.html#r2-and-adjusted-r2",
    "href": "slides/11-comparison.html#r2-and-adjusted-r2",
    "title": "Model comparison",
    "section": "\\(R^2\\) and Adjusted \\(R^2\\)",
    "text": "\\(R^2\\) and Adjusted \\(R^2\\)\n\\[R^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Error}}{SS_{Total}}\\]\n\n\n\\[R^2_{adj} = 1 - \\frac{SS_{Error}/(n-p-1)}{SS_{Total}/(n-1)}\\]\nwhere\n\n\\(n\\) is the number of observations used to fit the model\n\\(p\\) is the number of terms (not including the intercept) in the model"
  },
  {
    "objectID": "slides/11-comparison.html#using-r2-and-adjusted-r2",
    "href": "slides/11-comparison.html#using-r2-and-adjusted-r2",
    "title": "Model comparison",
    "section": "Using \\(R^2\\) and Adjusted \\(R^2\\)",
    "text": "Using \\(R^2\\) and Adjusted \\(R^2\\)\n\n\\(R^2_{adj}\\) can be used as a quick assessment to compare the fit of multiple models; however, it should not be the only assessment!\nUse \\(R^2\\) when describing the relationship between the response and predictor variables\n\nComplete Exercises 12-13."
  },
  {
    "objectID": "slides/11-comparison.html#comparing-models-with-r2_adj",
    "href": "slides/11-comparison.html#comparing-models-with-r2_adj",
    "title": "Model comparison",
    "section": "Comparing models with \\(R^2_{adj}\\)",
    "text": "Comparing models with \\(R^2_{adj}\\)\n\n\ntip_fit_1:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.6743626\n0.6643738\n1.954983\n67.51136\n0\n5\n-350.0405\n714.0811\n735.9904\n622.9793\n163\n169\n\n\n\n\n\n\ntip_fit_2:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.6825157\n0.6624218\n1.96066\n33.96625\n0\n10\n-347.898\n719.7959\n757.3547\n607.3815\n158\n169\n\n\n\n\n\n\n\n\nWhich model would we choose based on \\(R^2\\)?\nWhich model would we choose based on Adjusted \\(R^2\\)?\nWhich statistic should we use to choose the final model - \\(R^2\\) or Adjusted \\(R^2\\)? Why?"
  },
  {
    "objectID": "slides/11-comparison.html#aic-bic",
    "href": "slides/11-comparison.html#aic-bic",
    "title": "Model comparison",
    "section": "AIC & BIC",
    "text": "AIC & BIC\nEstimators of prediction error and relative quality of models:\n\nAkaike’s Information Criterion (AIC): \\[AIC = n\\log(SS_\\text{Error}) - n \\log(n) + 2(p+1)\\] \n\n\nSchwarz’s Bayesian Information Criterion (BIC): \\[BIC = n\\log(SS_\\text{Error}) - n\\log(n) + log(n)\\times(p+1)\\]"
  },
  {
    "objectID": "slides/11-comparison.html#aic-bic-1",
    "href": "slides/11-comparison.html#aic-bic-1",
    "title": "Model comparison",
    "section": "AIC & BIC",
    "text": "AIC & BIC\n\\[\n\\begin{aligned}\n& AIC = \\color{blue}{n\\log(SS_\\text{Error})} - n \\log(n) + 2(p+1) \\\\\n& BIC = \\color{blue}{n\\log(SS_\\text{Error})} - n\\log(n) + \\log(n)\\times(p+1)\n\\end{aligned}\n\\]\n\n\nFirst Term: Decreases as p increases… why?"
  },
  {
    "objectID": "slides/11-comparison.html#aic-bic-2",
    "href": "slides/11-comparison.html#aic-bic-2",
    "title": "Model comparison",
    "section": "AIC & BIC",
    "text": "AIC & BIC\n\\[\n\\begin{aligned}\n& AIC = n\\log(SS_\\text{Error}) - \\color{blue}{n \\log(n)} + 2(p+1) \\\\\n& BIC = n\\log(SS_\\text{Error}) - \\color{blue}{n\\log(n)} + \\log(n)\\times(p+1)\n\\end{aligned}\n\\]\n\nSecond Term: Fixed for a given sample size n"
  },
  {
    "objectID": "slides/11-comparison.html#aic-bic-3",
    "href": "slides/11-comparison.html#aic-bic-3",
    "title": "Model comparison",
    "section": "AIC & BIC",
    "text": "AIC & BIC\n\\[\n\\begin{aligned} & AIC = n\\log(SS_\\text{Error}) - n\\log(n) + \\color{blue}{2(p+1)} \\\\\n& BIC = n\\log(SS_\\text{Error}) - n\\log(n) + \\color{blue}{\\log(n)\\times(p+1)}\n\\end{aligned}\n\\]\n\nThird Term: Increases as p increases"
  },
  {
    "objectID": "slides/11-comparison.html#using-aic-bic",
    "href": "slides/11-comparison.html#using-aic-bic",
    "title": "Model comparison",
    "section": "Using AIC & BIC",
    "text": "Using AIC & BIC\n\\[\n\\begin{aligned} & AIC = n\\log(SS_{Error}) - n \\log(n) + \\color{red}{2(p+1)} \\\\\n& BIC = n\\log(SS_{Error}) - n\\log(n) + \\color{red}{\\log(n)\\times(p+1)}\n\\end{aligned}\n\\]\n\nChoose model with the smaller value of AIC or BIC\nIf \\(n \\geq 8\\), the penalty for BIC is larger than that of AIC, so BIC tends to favor more parsimonious models (i.e. models with fewer terms)\n\nComplete Exercise 14."
  },
  {
    "objectID": "slides/11-comparison.html#comparing-models-with-aic-and-bic",
    "href": "slides/11-comparison.html#comparing-models-with-aic-and-bic",
    "title": "Model comparison",
    "section": "Comparing models with AIC and BIC",
    "text": "Comparing models with AIC and BIC\n\n\ntip_fit_1\n\n\n\n\n\nAIC\nBIC\n\n\n\n\n714.0811\n735.9904\n\n\n\n\n\n\ntip_fit_2\n\n\n\n\n\nAIC\nBIC\n\n\n\n\n719.7959\n757.3547\n\n\n\n\n\n\n\n\nWhich model would we choose based on AIC?\nWhich model would we choose based on BIC?"
  },
  {
    "objectID": "slides/11-comparison.html#commonalities-between-criteria",
    "href": "slides/11-comparison.html#commonalities-between-criteria",
    "title": "Model comparison",
    "section": "Commonalities between criteria",
    "text": "Commonalities between criteria\n\n\\(R^2_{adj}\\), AIC, and BIC all apply a penalty for more predictors\nThe penalty for added model complexity attempts to strike a balance between underfitting (too few predictors in the model) and overfitting (too many predictors in the model)\nGoal: Parsimony"
  },
  {
    "objectID": "slides/11-comparison.html#parsimony-and-occams-razor",
    "href": "slides/11-comparison.html#parsimony-and-occams-razor",
    "title": "Model comparison",
    "section": "Parsimony and Occam’s razor",
    "text": "Parsimony and Occam’s razor\n\nThe principle of parsimony is attributed to William of Occam (early 14th-century English nominalist philosopher), who insisted that, given a set of equally good explanations for a given phenomenon, the correct explanation is the simplest explanation1\nCalled Occam’s razor because he “shaved” his explanations down to the bare minimum\nParsimony in modeling:\n\nmodels should have as few parameters as possible\nlinear models should be preferred to non-linear models\nexperiments relying on few assumptions should be preferred to those relying on many\nmodels should be pared down until they are minimal adequate (i.e. contain the minimum number of predictors required to meet some critereon)\nsimple explanations should be preferred to complex explanations\n\n\nSource: The R Book by Michael J. Crawley."
  },
  {
    "objectID": "slides/11-comparison.html#in-pursuit-of-occams-razor",
    "href": "slides/11-comparison.html#in-pursuit-of-occams-razor",
    "title": "Model comparison",
    "section": "In pursuit of Occam’s razor",
    "text": "In pursuit of Occam’s razor\n\nOccam’s razor states that among competing hypotheses that predict equally well, the one with the fewest assumptions should be selected\nModel selection follows this principle\nWe only want to add another variable to the model if the addition of that variable brings something valuable in terms of predictive power to the model\nIn other words, we prefer the simplest best model, i.e. parsimonious model"
  },
  {
    "objectID": "slides/11-comparison.html#alternate-views",
    "href": "slides/11-comparison.html#alternate-views",
    "title": "Model comparison",
    "section": "Alternate views",
    "text": "Alternate views\n\nSometimes a simple model will outperform a more complex model . . . Nevertheless, I believe that deliberately limiting the complexity of the model is not fruitful when the problem is evidently complex. Instead, if a simple model is found that outperforms some particular complex model, the appropriate response is to define a different complex model that captures whatever aspect of the problem led to the simple model performing well.\n\nRadford Neal - Bayesian Learning for Neural Networks1\n\nSuggested blog post: Occam by Andrew Gelman"
  },
  {
    "objectID": "slides/11-comparison.html#other-concerns-with-our-approach",
    "href": "slides/11-comparison.html#other-concerns-with-our-approach",
    "title": "Model comparison",
    "section": "Other concerns with our approach",
    "text": "Other concerns with our approach\n\nAll criteria we considered for model comparison require making predictions for our data and then uses the prediction error (\\(SS_{Error}\\)) somewhere in the formula\nBut we’re making prediction for the data we used to build the model (estimate the coefficients), which can lead to overfitting"
  },
  {
    "objectID": "slides/11-comparison.html#model-selection-1",
    "href": "slides/11-comparison.html#model-selection-1",
    "title": "Model comparison",
    "section": "Model Selection",
    "text": "Model Selection\n\nSo far: We’ve come up with a variety of metrics and tests which help us compare different models\nHow do we choose the models to compare in the first place?\nToday: Best subset, forward selection, and backward selection"
  },
  {
    "objectID": "slides/11-comparison.html#aic-bic-mallows-c_p",
    "href": "slides/11-comparison.html#aic-bic-mallows-c_p",
    "title": "Model comparison",
    "section": "AIC, BIC, Mallows’ \\(C_p\\)",
    "text": "AIC, BIC, Mallows’ \\(C_p\\)\nEstimators of prediction error and relative quality of models:\nAkaike’s Information Criterion (AIC): \\[AIC = n\\log(SS_\\text{Error}) - n \\log(n) + 2(p+1)\\] \nSchwarz’s Bayesian Information Criterion (BIC): \\[BIC = n\\log(SS_\\text{Error}) - n\\log(n) + log(n)\\times(p+1)\\]\n\nMallows’ \\(C_p\\): \\[C_p = \\frac{SSE_{p}}{MSE_{full model}} - n + 2(p+1)\\]"
  },
  {
    "objectID": "slides/11-comparison.html#best-subset-selection",
    "href": "slides/11-comparison.html#best-subset-selection",
    "title": "Model comparison",
    "section": "Best Subset Selection",
    "text": "Best Subset Selection\n\nComputers are great now!\nFrequently feasible to try out EVERY combination of predictors if you total number of possible predictors is not too high."
  },
  {
    "objectID": "slides/11-comparison.html#best-subset-selection-in-r",
    "href": "slides/11-comparison.html#best-subset-selection-in-r",
    "title": "Model comparison",
    "section": "Best Subset Selection in R",
    "text": "Best Subset Selection in R\n\nlibrary(olsrr)\n\nfull_model &lt;- lm(Tip ~ ., data = tips)\n\nols_step_best_subset(full_model)\n\n                         Best Subsets Regression                          \n--------------------------------------------------------------------------\nModel Index    Predictors\n--------------------------------------------------------------------------\n     1         Bill                                                        \n     2         Bday Bill                                                   \n     3         Day Party Bill                                              \n     4         Day Party Bday Bill                                         \n     5         Day Party Age Bday Bill                                     \n     6         Day Payment Party Age Bday Bill                             \n     7         Day Payment Party Age GiftCard Bday Bill                    \n     8         Day Payment Party Age GiftCard Comps Bday Bill              \n     9         Day Payment Party Age GiftCard Comps Alcohol Bday Bill      \n    10         Day Meal Payment Party Age GiftCard Comps Alcohol Bday Bill \n--------------------------------------------------------------------------\n\n                                                    Subsets Regression Summary                                                    \n----------------------------------------------------------------------------------------------------------------------------------\n                       Adj.        Pred                                                                                            \nModel    R-Square    R-Square    R-Square     C(p)        AIC         SBIC        SBC         MSEP       FPE       HSP       APC  \n----------------------------------------------------------------------------------------------------------------------------------\n  1        0.7662      0.7648      0.7582     7.3539    650.0922    170.4124    659.4819    452.6520    2.7101    0.0161    0.2394 \n  2        0.7743      0.7716      0.7585     3.3820    646.1326    166.6253    658.6522    439.6163    2.6474    0.0158    0.2339 \n  3        0.7806      0.7710      0.7526     8.7457    651.3566    164.1101    679.5257    429.9723    2.6689    0.0159    0.2301 \n  4        0.7870      0.7763      0.7548     6.0330    648.3593    161.5085    679.6583    420.0053    2.6224    0.0156    0.2260 \n  5        0.7912      0.7780      0.7512     6.9278    648.9839    160.5385    686.5427    414.2413    2.6181    0.0156    0.2242 \n  6        0.7934      0.7775      0.7433     9.2903    651.1765    161.0873    694.9951    412.3801    2.6384    0.0157    0.2244 \n  7        0.7940      0.7767      0.7374    10.8854    652.7266    162.9029    699.6751    413.8544    2.6634    0.0159    0.2265 \n  8        0.7945      0.7758      0.7346    12.5119    654.3104    164.7602    704.3888    415.4330    2.6893    0.0161    0.2287 \n  9        0.7950      0.7749      0.7315    14.1249    655.8782    166.6146    709.0865    416.9945    2.7151    0.0162    0.2308 \n 10        0.7952      0.7721      0.7225    18.0000    659.7385    168.7314    719.2066    419.3037    2.7641    0.0165    0.2334 \n----------------------------------------------------------------------------------------------------------------------------------\nAIC: Akaike Information Criteria \n SBIC: Sawa's Bayesian Information Criteria \n SBC: Schwarz Bayesian Criteria \n MSEP: Estimated error of prediction, assuming multivariate normality \n FPE: Final Prediction Error \n HSP: Hocking's Sp \n APC: Amemiya Prediction Criteria \n\n\nShows you “best” model for every model size."
  },
  {
    "objectID": "slides/11-comparison.html#backward-elimination",
    "href": "slides/11-comparison.html#backward-elimination",
    "title": "Model comparison",
    "section": "Backward Elimination",
    "text": "Backward Elimination\nDifferent model selection technique:\n\nStart by fitting the full model (the model that includes all terms under consideration).\nIdentify the term with the largest p-value.\n\nIf p-value is large (say, greater than 5%), eliminate that term to produce a smaller model. Fit that model and return to the start of Step 2.\nIf p-value is small (less than 5%), stop since all of the predictors in the model are “significant.”\n\n\nNote: this can be altered to work with other criterea (e.g. AIC) instead of p-values. This is actually what regsubsets does."
  },
  {
    "objectID": "slides/11-comparison.html#forward-selection",
    "href": "slides/11-comparison.html#forward-selection",
    "title": "Model comparison",
    "section": "Forward Selection",
    "text": "Forward Selection\n\nStart with a model with no predictors and find the best single predictor (the largest correlation with the response gives the biggest initial).\nAdd the new predictor to the model, run the regression, and find its individual p-value:\n\nIf p-value is small (say, less than 5%), add predictor which would produce the most benefit (biggest increase in \\(R^2\\)) when added to the existing model.\nIf the p-value is large (over 5%), stop and discard this predictor. At this point, no (unused) predictor should be significant when added to the model and we are done."
  },
  {
    "objectID": "slides/11-comparison.html#stepwise-selection",
    "href": "slides/11-comparison.html#stepwise-selection",
    "title": "Model comparison",
    "section": "Stepwise Selection",
    "text": "Stepwise Selection\nForward, stepwise selection\n\nStart with a model with no predictors and find the best single predictor (the largest correlation with the response gives the biggest initial).\nAdd the new predictor to the model, run the regression, and find its individual p-value:\n\nIf p-value is small (say, less than 5%), run backward elimination, then add predictor which would produce the most benefit (biggest increase in \\(R^2\\)) when added to the existing model.\nIf the p-value is large (over 5%), stop and discard this predictor. At this point, no (unused) predictor should be significant when added to the model and we are done.\n\n\n\nWhy? Sometimes variables that were significant early on, can become insignificant after other new variables are added to the model.\n\nBackward, stepwise selection is the same, except you perform forward selection every time you delete a term from the model."
  },
  {
    "objectID": "slides/11-comparison.html#caution",
    "href": "slides/11-comparison.html#caution",
    "title": "Model comparison",
    "section": "CAUTION",
    "text": "CAUTION\n\nThese automated methods have fallen out of favor in recent years, but you can still use them and should know what they are.\nAutomated methods ARE NOT a replacement for subject matter expertise\nThink of the models that come out of these procedures as suggestions\nThe order in which variables are added to a model can help us understand which variables are more important and which are redundant.\n\nComplete Exercise 15."
  },
  {
    "objectID": "slides/11-comparison.html#recap",
    "href": "slides/11-comparison.html#recap",
    "title": "Model comparison",
    "section": "Recap",
    "text": "Recap\n\nANOVA for multiple linear regression and sum of squares\n\\(R^2\\) for multiple linear regression\nComparing models with\n\n\\(R^2\\) vs. \\(R^2_{Adj}\\)\nAIC and BIC\n\nOccam’s razor and parsimony\nChoosing models using:\n\nExhaustive search\nForward/Backward/Stepwise selection"
  },
  {
    "objectID": "slides/09-logistic-intro.html#application-exercise",
    "href": "slides/09-logistic-intro.html#application-exercise",
    "title": "Logistic regression",
    "section": "Application Exercise",
    "text": "Application Exercise\n\n📋 AE 09 - Intro to Logistic Regression\n\nComplete Exercises 0-2."
  },
  {
    "objectID": "slides/09-logistic-intro.html#topics",
    "href": "slides/09-logistic-intro.html#topics",
    "title": "Logistic regression",
    "section": "Topics",
    "text": "Topics\n\nIntroduction to modeling categorical data\nLogistic regression for binary response variable\nRelationship between odds and probabilities"
  },
  {
    "objectID": "slides/09-logistic-intro.html#computational-setup",
    "href": "slides/09-logistic-intro.html#computational-setup",
    "title": "Logistic regression",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(broom)\nlibrary(knitr)\nlibrary(ggforce)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/09-logistic-intro.html#types-of-outcome-variables",
    "href": "slides/09-logistic-intro.html#types-of-outcome-variables",
    "title": "Logistic regression",
    "section": "Types of outcome variables",
    "text": "Types of outcome variables\nQuantitative outcome variable:\n\nSales price of a house\nModel: Expected sales price given the number of bedrooms, lot size, etc.\n\n\nCategorical outcome variable:\n\nIndicator for developing coronary heart disease in the next 10 years\nModel: Probability an adult is high risk of heart disease in the next 10 years given their age, total cholesterol, etc."
  },
  {
    "objectID": "slides/09-logistic-intro.html#models-for-categorical-outcomes",
    "href": "slides/09-logistic-intro.html#models-for-categorical-outcomes",
    "title": "Logistic regression",
    "section": "Models for categorical outcomes",
    "text": "Models for categorical outcomes\n\n\nLogistic regression\n2 Outcomes\n\n1: “Success” (models probability of this category…)\n0: “Failure”\n\n\nMultinomial logistic regression\n3+ Outcomes\n\n1: Democrat\n2: Republican\n3: Independent"
  },
  {
    "objectID": "slides/09-logistic-intro.html#election-forecasts",
    "href": "slides/09-logistic-intro.html#election-forecasts",
    "title": "Logistic regression",
    "section": "2024 election forecasts",
    "text": "2024 election forecasts\nThe Economist"
  },
  {
    "objectID": "slides/09-logistic-intro.html#nba-finals-predictions",
    "href": "slides/09-logistic-intro.html#nba-finals-predictions",
    "title": "Logistic regression",
    "section": "2020 NBA finals predictions",
    "text": "2020 NBA finals predictions\n\nSource: FiveThirtyEight 2019-20 NBA Predictions"
  },
  {
    "objectID": "slides/09-logistic-intro.html#data-framingham-study",
    "href": "slides/09-logistic-intro.html#data-framingham-study",
    "title": "Logistic regression",
    "section": "Data: Framingham Study",
    "text": "Data: Framingham Study\nThis data set is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to use the patients age to predict if a randomly selected adult is high risk for heart disease in the next 10 years.\n\nheart_disease &lt;- read_csv(\"../data/framingham.csv\") |&gt;\n  select(totChol, TenYearCHD, age, BMI, cigsPerDay, heartRate) |&gt;\n  drop_na()"
  },
  {
    "objectID": "slides/09-logistic-intro.html#variables",
    "href": "slides/09-logistic-intro.html#variables",
    "title": "Logistic regression",
    "section": "Variables",
    "text": "Variables\n\nResponse:\n\nTenYearCHD:\n\n1: Patient developed heart disease within 10 years of exam\n0: Patient did not develop heart disease within 10 years of exam\n\n\nPredictor:\n\nage: age in years at time of visit"
  },
  {
    "objectID": "slides/09-logistic-intro.html#plot-the-data",
    "href": "slides/09-logistic-intro.html#plot-the-data",
    "title": "Logistic regression",
    "section": "Plot the data",
    "text": "Plot the data"
  },
  {
    "objectID": "slides/09-logistic-intro.html#lets-fit-a-linear-regression-model",
    "href": "slides/09-logistic-intro.html#lets-fit-a-linear-regression-model",
    "title": "Logistic regression",
    "section": "Let’s fit a linear regression model",
    "text": "Let’s fit a linear regression model\n\n🛑 This model produces predictions outside of 0 and 1."
  },
  {
    "objectID": "slides/09-logistic-intro.html#lets-try-another-model",
    "href": "slides/09-logistic-intro.html#lets-try-another-model",
    "title": "Logistic regression",
    "section": "Let’s try another model",
    "text": "Let’s try another model"
  },
  {
    "objectID": "slides/09-logistic-intro.html#lets-try-another-model-zooming-out",
    "href": "slides/09-logistic-intro.html#lets-try-another-model-zooming-out",
    "title": "Logistic regression",
    "section": "Let’s try another model: Zooming Out",
    "text": "Let’s try another model: Zooming Out\n\n✅ This model (called a logistic regression model) only produces predictions between 0 and 1."
  },
  {
    "objectID": "slides/09-logistic-intro.html#the-code",
    "href": "slides/09-logistic-intro.html#the-code",
    "title": "Logistic regression",
    "section": "The code",
    "text": "The code\n\nheart_disease |&gt; \n  gf_point(TenYearCHD ~ age)  |&gt;\n  gf_hline(yintercept = c(0,1), lty = 2) |&gt; \n  gf_labs(y = \"CHD Risk\", x = \"Age\") |&gt; \n  gf_refine(stat_smooth(method =\"glm\", method.args = list(family = \"binomial\"), \n              fullrange = TRUE, se = FALSE))"
  },
  {
    "objectID": "slides/09-logistic-intro.html#different-types-of-models",
    "href": "slides/09-logistic-intro.html#different-types-of-models",
    "title": "Logistic regression",
    "section": "Different types of models",
    "text": "Different types of models\n\n\n\n\n\n\n\n\nMethod\nOutcome\nModel\n\n\n\n\nLinear regression\nQuantitative\n\\(Y = \\beta_0 + \\beta_1~ X\\)\n\n\nLinear regression (transform Y)\nQuantitative\n\\(\\log(Y) = \\beta_0 + \\beta_1~ X\\)\n\n\nLogistic regression\nBinary\n\\(\\log\\big(\\frac{\\pi}{1-\\pi}\\big) = \\beta_0 + \\beta_1 ~ X\\)\n\n\n\nNote: In this class (and in most college level math classes) ((and and in R)) \\(\\log\\) means log base \\(e\\) (i.e. natural log)"
  },
  {
    "objectID": "slides/09-logistic-intro.html#linear-vs.-logistic-regression",
    "href": "slides/09-logistic-intro.html#linear-vs.-logistic-regression",
    "title": "Logistic regression",
    "section": "Linear vs. logistic regression",
    "text": "Linear vs. logistic regression\nComplete Exercise 3."
  },
  {
    "objectID": "slides/09-logistic-intro.html#linear-vs.-logistic-regression-1",
    "href": "slides/09-logistic-intro.html#linear-vs.-logistic-regression-1",
    "title": "Logistic regression",
    "section": "Linear vs. logistic regression",
    "text": "Linear vs. logistic regression\n\nState whether a linear regression model or logistic regression model is more appropriate for each scenario.\n\nUse age and education to predict if a randomly selected person will vote in the next election.\nUse budget and run time (in minutes) to predict a movie’s total revenue.\nUse age and sex to calculate the probability a randomly selected adult will visit St. Lukes in the next year."
  },
  {
    "objectID": "slides/09-logistic-intro.html#binary-response-variable",
    "href": "slides/09-logistic-intro.html#binary-response-variable",
    "title": "Logistic regression",
    "section": "Binary response variable",
    "text": "Binary response variable\n\n\\(Y\\):\n\n1: “success” (not necessarily a good thing)\n0: “failure”\n\n\\(\\pi\\): probability that \\(Y=1\\), i.e., \\(P(Y = 1)\\)\n\\(\\frac{\\pi}{1-\\pi}\\): odds that \\(Y = 1\\)\n\\(\\log\\big(\\frac{\\pi}{1-\\pi}\\big)\\): log-odds\nGo from \\(\\pi\\) to \\(\\log\\big(\\frac{\\pi}{1-\\pi}\\big)\\) using the logit transformation"
  },
  {
    "objectID": "slides/09-logistic-intro.html#odds",
    "href": "slides/09-logistic-intro.html#odds",
    "title": "Logistic regression",
    "section": "Odds",
    "text": "Odds\nSuppose there is a 70% chance it will rain tomorrow\n\nProbability it will rain is \\(\\mathbf{p = 0.7}\\)\nProbability it won’t rain is \\(\\mathbf{1 - p = 0.3}\\)\nOdds it will rain are 7 to 3, 7:3, \\(\\mathbf{\\frac{0.7}{0.3} \\approx 2.33}\\)\n\nFor every 3 times it doesn’t rain, it will rain 7 times\nFor every time it doesn’t rain, it will rain 2.33 times\n\nLog-Odds it will rain is \\(\\log\\mathbf{\\frac{0.7}{0.3} \\approx \\log(2.33) \\approx 0.847}\\)\n\nNegative \\(\\Rightarrow\\) probability of success less than 50-50 (0.5)\nPositive \\(\\Rightarrow\\) probability of success greater than 50-50 (0.5)\nWhat are the log-odds of of a probability of 0? What about 1?"
  },
  {
    "objectID": "slides/09-logistic-intro.html#from-log-odds-to-probabilities",
    "href": "slides/09-logistic-intro.html#from-log-odds-to-probabilities",
    "title": "Logistic regression",
    "section": "From log-odds to probabilities",
    "text": "From log-odds to probabilities\n\n\nlog-odds\n\\[\\omega = \\log \\frac{\\pi}{1-\\pi}\\]\nodds\n\\[e^\\omega = \\frac{\\pi}{1-\\pi}\\]\n\nprobability\n\\[\\pi = \\frac{e^\\omega}{1 + e^\\omega}\\]\n\nComplete Exercise 4-5."
  },
  {
    "objectID": "slides/09-logistic-intro.html#from-odds-to-probabilities",
    "href": "slides/09-logistic-intro.html#from-odds-to-probabilities",
    "title": "Logistic regression",
    "section": "From odds to probabilities",
    "text": "From odds to probabilities\n\nLogistic model: log-odds = \\(\\log\\big(\\frac{\\pi}{1-\\pi}\\big) = \\beta_0 + \\beta_1~X\\)\nOdds = \\(\\exp\\big\\{\\log\\big(\\frac{\\pi}{1-\\pi}\\big)\\big\\} = \\frac{\\pi}{1-\\pi}\\)\nCombining (1) and (2) with what we saw earlier\n\n\n\\[\\text{probability} = \\pi = \\frac{\\exp\\{\\beta_0 + \\beta_1~X\\}}{1 + \\exp\\{\\beta_0 + \\beta_1~X\\}}\\]"
  },
  {
    "objectID": "slides/09-logistic-intro.html#logistic-regression-model",
    "href": "slides/09-logistic-intro.html#logistic-regression-model",
    "title": "Logistic regression",
    "section": "Logistic regression model",
    "text": "Logistic regression model\nLogit form: \\[\\log\\big(\\frac{\\pi}{1-\\pi}\\big) = \\beta_0 + \\beta_1~X\\]\n\nProbability form:\n\\[\n\\pi = \\frac{\\exp\\{\\beta_0 + \\beta_1~X\\}}{1 + \\exp\\{\\beta_0 + \\beta_1~X\\}}\n\\]"
  },
  {
    "objectID": "slides/09-logistic-intro.html#variables-1",
    "href": "slides/09-logistic-intro.html#variables-1",
    "title": "Logistic regression",
    "section": "Variables",
    "text": "Variables\n\nResponse:\n\nTenYearCHD:\n\n1: Patient developed heart disease within 10 years of exam\n0: Patient did not develop heart disease within 10 years of exam\n\n\nPredictors:\n\nage: age in years"
  },
  {
    "objectID": "slides/09-logistic-intro.html#logistic-regression-model-1",
    "href": "slides/09-logistic-intro.html#logistic-regression-model-1",
    "title": "Logistic regression",
    "section": "Logistic regression model",
    "text": "Logistic regression model\nLogit form: \\[\\log\\big(\\frac{\\pi}{1-\\pi}\\big) = \\beta_0 + \\beta_1~X\\]\n\nProbability form:\n\\[\n\\pi = \\frac{\\exp\\{\\beta_0 + \\beta_1~X\\}}{1 + \\exp\\{\\beta_0 + \\beta_1~X\\}}\n\\]\n\n\nToday: Using R to fit this model."
  },
  {
    "objectID": "slides/09-logistic-intro.html#tenyearchd-vs.-age",
    "href": "slides/09-logistic-intro.html#tenyearchd-vs.-age",
    "title": "Logistic regression",
    "section": "TenYearCHD vs. age",
    "text": "TenYearCHD vs. age\n\nheart_disease |&gt; \ngf_sina(age ~ factor(TenYearCHD)) |&gt; \n  gf_labs(x = \"TenYearCHD - 1: yes, 0: no\",\n       y = \"Age\", \n       title = \"Age vs. Ten YearCHD\")"
  },
  {
    "objectID": "slides/09-logistic-intro.html#tenyearchd-vs.-age-1",
    "href": "slides/09-logistic-intro.html#tenyearchd-vs.-age-1",
    "title": "Logistic regression",
    "section": "TenYearCHD vs. age",
    "text": "TenYearCHD vs. age\n\nheart_disease |&gt; \ngf_violin(age ~ factor(TenYearCHD), fill = \"steelblue\") |&gt; \n  gf_labs(x = \"TenYearCHD - 1: yes, 0: no\",\n       y = \"Age\", \n       title = \"Age vs. TenYearCHD\")"
  },
  {
    "objectID": "slides/09-logistic-intro.html#tenyearchd-vs.-age-2",
    "href": "slides/09-logistic-intro.html#tenyearchd-vs.-age-2",
    "title": "Logistic regression",
    "section": "TenYearCHD vs. age",
    "text": "TenYearCHD vs. age\n\nheart_disease |&gt; \ngf_boxplot(age ~ factor(TenYearCHD), fill = \"steelblue\") |&gt; \n  gf_sina(size = 0.75, alpha=0.25) |&gt; \n  gf_labs(x = \"TenYearCHD - 1: yes, 0: no\",\n       y = \"Age\", \n       title = \"Age vs. TenYearCHD\")"
  },
  {
    "objectID": "slides/09-logistic-intro.html#lets-fit-a-model",
    "href": "slides/09-logistic-intro.html#lets-fit-a-model",
    "title": "Logistic regression",
    "section": "Let’s fit a model",
    "text": "Let’s fit a model\n\nheart_disease_fit &lt;- glm(TenYearCHD ~ age, data = heart_disease, family = \"binomial\")\n\ntidy(heart_disease_fit) |&gt; kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.6614125\n0.2899446\n-19.52584\n0\n\n\nage\n0.0763254\n0.0053760\n14.19754\n0"
  },
  {
    "objectID": "slides/09-logistic-intro.html#the-model",
    "href": "slides/09-logistic-intro.html#the-model",
    "title": "Logistic regression",
    "section": "The model",
    "text": "The model\n\ntidy(heart_disease_fit) |&gt; kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.661\n0.290\n-19.526\n0\n\n\nage\n0.076\n0.005\n14.198\n0\n\n\n\n\n\n\\[\\textbf{Logit form:}\\qquad\\log\\Big(\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}\\Big) = -5.561 + 0.076 \\times \\text{age}\\]\n\\[\\textbf{Probability form:}\\qquad\\hat{\\pi} = \\frac{\\exp(-5.561 + 0.076 \\times \\text{age})}{1+\\exp(-5.561 + 0.075 \\times \\text{age})}\\]\nwhere \\(\\hat{\\pi}\\) is the predicted probability of developing heart disease in the next 10 years."
  },
  {
    "objectID": "slides/09-logistic-intro.html#interpreting-hatbetas",
    "href": "slides/09-logistic-intro.html#interpreting-hatbetas",
    "title": "Logistic regression",
    "section": "Interpreting \\(\\hat{\\beta}\\)’s",
    "text": "Interpreting \\(\\hat{\\beta}\\)’s\n\ntidy(heart_disease_fit) |&gt; kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.661\n0.290\n-19.526\n0\n\n\nage\n0.076\n0.005\n14.198\n0\n\n\n\n\n\nFor every addition year of age, the log-odds of developing heart disease in the next 10 years, increases by 0.076.\n\nComplete Exercises 6-8."
  },
  {
    "objectID": "slides/09-logistic-intro.html#interpretability-of-beta-for-predicted-probabilities",
    "href": "slides/09-logistic-intro.html#interpretability-of-beta-for-predicted-probabilities",
    "title": "Logistic regression",
    "section": "Interpretability of \\(\\beta\\) for predicted probabilities",
    "text": "Interpretability of \\(\\beta\\) for predicted probabilities\n\n\nSLOPE IS CHANGING!\nIncrease in \\(\\hat{\\pi}\\) due to increase of 1 year of Age depends on what starting age is"
  },
  {
    "objectID": "slides/09-logistic-intro.html#glm-and-augment",
    "href": "slides/09-logistic-intro.html#glm-and-augment",
    "title": "Logistic regression",
    "section": "glm and augment",
    "text": "glm and augment\nThe .fitted values in augment correspond to predictions from the logistic form of the model (i.e. the log-odds):\n\naugment(heart_disease_fit)  |&gt; head()\n\n# A tibble: 6 × 8\n  TenYearCHD   age .fitted .resid     .hat .sigma   .cooksd .std.resid\n       &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1          0    39   -2.68 -0.363 0.000472  0.891 0.0000161     -0.363\n2          0    46   -2.15 -0.469 0.000330  0.891 0.0000192     -0.469\n3          0    48   -2.00 -0.504 0.000295  0.891 0.0000200     -0.504\n4          1    61   -1.01  1.62  0.000730  0.891 0.000999       1.62 \n5          0    46   -2.15 -0.469 0.000330  0.891 0.0000192     -0.469\n6          0    43   -2.38 -0.421 0.000393  0.891 0.0000182     -0.421\n\n\n\nNote: The residuals do not make sense here!\nFor observation 1\n\\[\\text{predicted probability} = \\hat{\\pi} = \\frac{\\exp\\{-2.680\\}}{1 + \\exp\\{-2.680\\}} = 0.0733\\]"
  },
  {
    "objectID": "slides/09-logistic-intro.html#using-predict-with-glm",
    "href": "slides/09-logistic-intro.html#using-predict-with-glm",
    "title": "Logistic regression",
    "section": "Using predict with glm",
    "text": "Using predict with glm\nDefault output is log-odds:\n\npredict(heart_disease_fit, new_data = heart_disease) |&gt; head() |&gt; kable(digits = 3)\n\n\n\n\nx\n\n\n\n\n-2.685\n\n\n-2.150\n\n\n-1.998\n\n\n-1.006\n\n\n-2.150\n\n\n-2.379"
  },
  {
    "objectID": "slides/09-logistic-intro.html#using-predict-with-glm-1",
    "href": "slides/09-logistic-intro.html#using-predict-with-glm-1",
    "title": "Logistic regression",
    "section": "Using predict with glm",
    "text": "Using predict with glm\nMore commonly you want the predicted probability:\n\npredict(heart_disease_fit, newdata = heart_disease, type = \"response\") |&gt; head() |&gt; kable(digits = 3)\n\n\n\n\nx\n\n\n\n\n0.064\n\n\n0.104\n\n\n0.119\n\n\n0.268\n\n\n0.104\n\n\n0.085\n\n\n\n\n\n\nComplete Exercise 9"
  },
  {
    "objectID": "slides/09-logistic-intro.html#recap",
    "href": "slides/09-logistic-intro.html#recap",
    "title": "Logistic regression",
    "section": "Recap",
    "text": "Recap\n\nIntroduced logistic regression for binary response variable\nDescribed relationship between odds and probabilities\nFit logistic regression models using glm\nInterpreted coefficients in logistic regression models\nUsed logistic regression model to calculate predicted odds and probabilities\nUse predict to make predictions using glm"
  },
  {
    "objectID": "slides/07-mlr.html#application-exercise",
    "href": "slides/07-mlr.html#application-exercise",
    "title": "Multiple linear regression (MLR)",
    "section": "Application Exercise",
    "text": "Application Exercise\n\n📋 AE 07 - Multiple Linear Regression and Categorical Predictors\n\nComplete Exercise 0-3."
  },
  {
    "objectID": "slides/07-mlr.html#computational-setup",
    "href": "slides/07-mlr.html#computational-setup",
    "title": "Multiple linear regression (MLR)",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(mosaic)\nlibrary(ISLR2)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(coursekata)\nlibrary(kableExtra)\nlibrary(scales)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))\n\n# Create new variable\n\nCredit &lt;- Credit |&gt; \n  mutate(Has_Balance = factor(ifelse(Balance == 0, \"No\", \"Yes\")))"
  },
  {
    "objectID": "slides/07-mlr.html#data-credit-cards",
    "href": "slides/07-mlr.html#data-credit-cards",
    "title": "Multiple linear regression (MLR)",
    "section": "Data: Credit Cards",
    "text": "Data: Credit Cards\nThe data is from the Credit data set in the ISLR2 R package. It is a simulated data set of 400 credit card customers.\n\n\nRows: 400\nColumns: 12\n$ Income      &lt;dbl&gt; 14.891, 106.025, 104.593, 148.924, 55.882, 80.180, 20.996,…\n$ Limit       &lt;dbl&gt; 3606, 6645, 7075, 9504, 4897, 8047, 3388, 7114, 3300, 6819…\n$ Rating      &lt;dbl&gt; 283, 483, 514, 681, 357, 569, 259, 512, 266, 491, 589, 138…\n$ Cards       &lt;dbl&gt; 2, 3, 4, 3, 2, 4, 2, 2, 5, 3, 4, 3, 1, 1, 2, 3, 3, 3, 1, 2…\n$ Age         &lt;dbl&gt; 34, 82, 71, 36, 68, 77, 37, 87, 66, 41, 30, 64, 57, 49, 75…\n$ Education   &lt;dbl&gt; 11, 15, 11, 11, 16, 10, 12, 9, 13, 19, 14, 16, 7, 9, 13, 1…\n$ Own         &lt;fct&gt; No, Yes, No, Yes, No, No, Yes, No, Yes, Yes, No, No, Yes, …\n$ Student     &lt;fct&gt; No, Yes, No, No, No, No, No, No, No, Yes, No, No, No, No, …\n$ Married     &lt;fct&gt; Yes, Yes, No, No, Yes, No, No, No, No, Yes, Yes, No, Yes, …\n$ Region      &lt;fct&gt; South, West, West, West, South, South, East, West, South, …\n$ Balance     &lt;dbl&gt; 333, 903, 580, 964, 331, 1151, 203, 872, 279, 1350, 1407, …\n$ Has_Balance &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, No,…"
  },
  {
    "objectID": "slides/07-mlr.html#variables",
    "href": "slides/07-mlr.html#variables",
    "title": "Multiple linear regression (MLR)",
    "section": "Variables",
    "text": "Variables\nFeatures (another name for predictors)\n\nCards: Number of credit cards\nRating: Credit Rating\nHas_Balance: Whether they have a credit card balance\n\nOutcome\n\nLimit: Credit limit"
  },
  {
    "objectID": "slides/07-mlr.html#outcome-limit",
    "href": "slides/07-mlr.html#outcome-limit",
    "title": "Multiple linear regression (MLR)",
    "section": "Outcome: Limit",
    "text": "Outcome: Limit\n\n\nCode\nCredit |&gt; \n  gf_density(~Limit, fill = \"steelblue\") |&gt; \n  gf_labs(title = \"Distribution of credit limit\",\n          x = \"Credit Limit\")|&gt; \n  gf_refine(scale_x_continuous(labels = dollar_format()))\n\n\n\n\n\n\n\n\n\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n\n\n855\n3088\n4622.5\n5872.75\n13913\n4735.6\n2308.199\n400\n0"
  },
  {
    "objectID": "slides/07-mlr.html#predictors",
    "href": "slides/07-mlr.html#predictors",
    "title": "Multiple linear regression (MLR)",
    "section": "Predictors",
    "text": "Predictors\n\n\nCode\np1 &lt;- Credit |&gt; \n  gf_density(~Limit, fill = \"steelblue\") |&gt; \n  gf_labs(title = \"Distribution of credit limit\",\n          x = \"Credit Limit\")|&gt; \n    gf_refine(scale_x_continuous(labels = dollar_format()))\n\np2 &lt;- Credit |&gt; \n  gf_histogram(~Rating, binwidth = 50) |&gt; \n  gf_labs(title = \"\",\n       x = \"Credit Rating\")\n\np3 &lt;- Credit |&gt; \n  gf_histogram(~Cards, binwidth = 1) |&gt; \n  gf_labs(title = \"\",\n       x = \"Number of Credit Cards\")\n\np4 &lt;- Credit |&gt; \n  gf_bar(~Has_Balance)|&gt; \n  gf_labs(title = \"\",\n       x = \"Has a Credit Card Balance\")\n\n(p1 + p2) / (p3 + p4)"
  },
  {
    "objectID": "slides/07-mlr.html#outcome-vs.-predictors",
    "href": "slides/07-mlr.html#outcome-vs.-predictors",
    "title": "Multiple linear regression (MLR)",
    "section": "Outcome vs. predictors",
    "text": "Outcome vs. predictors\n\n\nCode\nlibrary(GGally)\n\nCredit |&gt; \n  select(Limit, Rating, Cards, Has_Balance) |&gt; \n  ggpairs()"
  },
  {
    "objectID": "slides/07-mlr.html#single-vs.-multiple-predictors",
    "href": "slides/07-mlr.html#single-vs.-multiple-predictors",
    "title": "Multiple linear regression (MLR)",
    "section": "Single vs. multiple predictors",
    "text": "Single vs. multiple predictors\nSo far we’ve used a single predictor variable to understand variation in a quantitative response variable\n\nNow we want to use multiple predictor variables to understand variation in a quantitative response variable"
  },
  {
    "objectID": "slides/07-mlr.html#multiple-linear-regression-mlr",
    "href": "slides/07-mlr.html#multiple-linear-regression-mlr",
    "title": "Multiple linear regression (MLR)",
    "section": "Multiple linear regression (MLR)",
    "text": "Multiple linear regression (MLR)\nBased on the analysis goals, we will use a multiple linear regression model of the following form\n\\[\n\\begin{aligned}\\hat{\\text{Limit}} ~ =\n\\hat{\\beta}_0 & + \\hat{\\beta}_1 \\text{Rating} + \\hat{\\beta}_2 \\text{Cards}\n\\end{aligned}\n\\]\nSimilar to simple linear regression, this model assumes that at each combination of the predictor variables, the values of Limit follow a Normal distribution."
  },
  {
    "objectID": "slides/07-mlr.html#multiple-linear-regression-1",
    "href": "slides/07-mlr.html#multiple-linear-regression-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nRecall: The simple linear regression model assumes\n\\[\nY|X\\sim N(\\beta_0 + \\beta_1 X, \\sigma_{\\epsilon}^2)\n\\]\n\nSimilarly: The multiple linear regression model assumes\n\\[\nY|X_1, X_2, \\ldots, X_p \\sim N(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p, \\sigma_{\\epsilon}^2)\n\\]"
  },
  {
    "objectID": "slides/07-mlr.html#multiple-linear-regression-2",
    "href": "slides/07-mlr.html#multiple-linear-regression-2",
    "title": "Multiple linear regression (MLR)",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nAt any combination of the predictors, the mean value of the response \\(Y\\), is\n\\[\n\\mu_{Y|X_1, \\ldots, X_p} = \\beta_0 + \\beta_1 X_{1} + \\beta_2 X_2 + \\dots + \\beta_p X_p\n\\]\n\nUsing multiple linear regression, we can estimate the mean response for any combination of predictors\n\\[\n\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_{1} + \\hat{\\beta}_2 X_2 + \\dots + \\hat{\\beta}_p X_{p}\n\\]"
  },
  {
    "objectID": "slides/07-mlr.html#model-fit",
    "href": "slides/07-mlr.html#model-fit",
    "title": "Multiple linear regression (MLR)",
    "section": "Model fit",
    "text": "Model fit\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-341.15903\n24.7670758\n-13.77470\n0\n\n\nRating\n14.90573\n0.0496795\n300.03798\n0\n\n\nCards\n-72.31808\n5.6054513\n-12.90138\n0"
  },
  {
    "objectID": "slides/07-mlr.html#model-equation",
    "href": "slides/07-mlr.html#model-equation",
    "title": "Multiple linear regression (MLR)",
    "section": "Model equation",
    "text": "Model equation\n\\[\n\\begin{align}\\hat{\\text{Limit}} =  -341.159 &+14.906 \\times \\text{Rating}\\\\\n& -72.318 \\times \\text{Cards}\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/07-mlr.html#interpreting-hatbeta_j",
    "href": "slides/07-mlr.html#interpreting-hatbeta_j",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting \\(\\hat{\\beta}_j\\)",
    "text": "Interpreting \\(\\hat{\\beta}_j\\)\n\nThe estimated coefficient \\(\\hat{\\beta}_j\\) is the expected change in the mean of \\(y\\) when \\(x_j\\) increases by one unit, holding the values of all other predictor variables constant.\n\n\n\nComplete Exercises 4-6."
  },
  {
    "objectID": "slides/07-mlr.html#prediction",
    "href": "slides/07-mlr.html#prediction",
    "title": "Multiple linear regression (MLR)",
    "section": "Prediction",
    "text": "Prediction\n\nWhat is the predicted credit limit for a borrower with these characteristics?\n\n\nhead(Credit, n = 1) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIncome\nLimit\nRating\nCards\nAge\nEducation\nOwn\nStudent\nMarried\nRegion\nBalance\nHas_Balance\n\n\n\n\n14.891\n3606\n283\n2\n34\n11\nNo\nNo\nYes\nSouth\n333\nYes\n\n\n\n\n\n\n\n-341.159 +14.906 * 283 -72.318 * 2\n\n[1] 3732.603\n\n\n\nThe predicted credit limit for an borrower with an credit rating of 700 and who has 2 credit cards is $3733."
  },
  {
    "objectID": "slides/07-mlr.html#prediction-revisited",
    "href": "slides/07-mlr.html#prediction-revisited",
    "title": "Multiple linear regression (MLR)",
    "section": "Prediction, revisited",
    "text": "Prediction, revisited\nJust like with simple linear regression, we can use the predict function in R to calculate the appropriate intervals for our predicted values:\n\n\n       fit      lwr      upr\n1 3732.526 3430.472 4034.581\n\n\n\n\n\n\n\n\nNote\n\n\nDifference in predicted value due to rounding the coefficients on the previous slide.\n\n\n\nComplete Exercise 7."
  },
  {
    "objectID": "slides/07-mlr.html#prediction-interval-for-haty",
    "href": "slides/07-mlr.html#prediction-interval-for-haty",
    "title": "Multiple linear regression (MLR)",
    "section": "Prediction interval for \\(\\hat{y}\\)",
    "text": "Prediction interval for \\(\\hat{y}\\)\n\nCalculate a 90% confidence interval for the predicted credit limit for an individual borrower an credit rating of 700, and who has 2 credit cards.\n\n\n\npredict(lim_fit, new_borrower, interval = \"prediction\", level = 0.90)\n\n       fit      lwr      upr\n1 3732.526 3479.216 3985.837\n\n\n\nWhen would you use \"confidence\"? Would the interval be wider or narrower?"
  },
  {
    "objectID": "slides/07-mlr.html#cautions",
    "href": "slides/07-mlr.html#cautions",
    "title": "Multiple linear regression (MLR)",
    "section": "Cautions",
    "text": "Cautions\n\nDo not extrapolate! Because there are multiple predictor variables, there is the potential to extrapolate in many directions\nThe multiple regression model only shows association, not causality\n\nTo show causality, you must have a carefully designed experiment or carefully account for confounding variables in an observational study"
  },
  {
    "objectID": "slides/07-mlr.html#indicator-variables",
    "href": "slides/07-mlr.html#indicator-variables",
    "title": "Multiple linear regression (MLR)",
    "section": "Indicator variables",
    "text": "Indicator variables\n\nSuppose there is a categorical variable with \\(K\\) categories (levels)\nWe can make \\(K\\) indicator variables - one indicator for each category\nAn indicator variable takes values 1 or 0\n\n1 if the observation belongs to that category\n0 if the observation does not belong to that category"
  },
  {
    "objectID": "slides/07-mlr.html#data-manipulation-create-indicator-variables-for-has_balance",
    "href": "slides/07-mlr.html#data-manipulation-create-indicator-variables-for-has_balance",
    "title": "Multiple linear regression (MLR)",
    "section": "Data manipulation: Create indicator variables for Has_Balance",
    "text": "Data manipulation: Create indicator variables for Has_Balance\n\nCredit_dummy &lt;- Credit |&gt;\n  mutate(\n    No_Balance = if_else(Has_Balance == \"No\", 1, 0),\n    Yes_Balance = if_else(Has_Balance == \"Yes\", 1, 0)\n  )\n\nCredit_dummy |&gt;\n  select(Has_Balance, No_Balance, Yes_Balance) |&gt;\n  slice(1, 12)\n\n  Has_Balance No_Balance Yes_Balance\n1         Yes          0           1\n2          No          1           0"
  },
  {
    "objectID": "slides/07-mlr.html#indicators-in-the-model",
    "href": "slides/07-mlr.html#indicators-in-the-model",
    "title": "Multiple linear regression (MLR)",
    "section": "Indicators in the model",
    "text": "Indicators in the model\n\nWe will use \\(K-1\\) of the indicator variables in the model.\nThe reference level or baseline is the category that doesn’t have a term in the model.\nThe coefficients of the indicator variables in the model are interpreted as the expected change in the response compared to the baseline, holding all other variables constant.\nThis approach is also called dummy coding and R will do this for you\n\n\nCredit_dummy |&gt;\n  select(Has_Balance, No_Balance, Yes_Balance) |&gt;\n  slice(1, 12)\n\n  Has_Balance No_Balance Yes_Balance\n1         Yes          0           1\n2          No          1           0"
  },
  {
    "objectID": "slides/07-mlr.html#interpreting-categorical-predictors",
    "href": "slides/07-mlr.html#interpreting-categorical-predictors",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting Categorical Predictors",
    "text": "Interpreting Categorical Predictors\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n2152.722\n194.211\n11.084\n0\n\n\nHas_BalanceYes\n3332.746\n220.609\n15.107\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhere do we see each of the estimates in the plot?\nWhere do we see the values we’d predict in the plot?\nAre Has_Balance and Limit correlated?\n\n\n\n\n\n−+\n03:00\n\n\n\n\nComplete Exercises 8-10."
  },
  {
    "objectID": "slides/07-mlr.html#model-equation-1",
    "href": "slides/07-mlr.html#model-equation-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Model equation",
    "text": "Model equation\n\\[\n\\begin{align}\\hat{\\text{Limit}} =  2152.722 &+ 3332.746 \\times \\text{Yes_Balance}\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/07-mlr.html#credit-limit-vs.-cards-parallel-slopes",
    "href": "slides/07-mlr.html#credit-limit-vs.-cards-parallel-slopes",
    "title": "Multiple linear regression (MLR)",
    "section": "Credit Limit vs. Cards: parallel slopes",
    "text": "Credit Limit vs. Cards: parallel slopes\n\n\nCode\ncards_model &lt;- lm(Limit ~ Cards, data = Credit)\n\ncards_hasbal_model &lt;- lm(Limit ~ Cards + Has_Balance, data = Credit)\n\np1 &lt;- plotModel(cards_model) |&gt;\n  gf_labs(title = \"SLR\")\np2 &lt;- plotModel(cards_hasbal_model) |&gt;\n  gf_labs(title = \"Balance Indicator\")\n\np1 + p2"
  },
  {
    "objectID": "slides/07-mlr.html#parallel-slopes-interpretation",
    "href": "slides/07-mlr.html#parallel-slopes-interpretation",
    "title": "Multiple linear regression (MLR)",
    "section": "Parallel slopes interpretation",
    "text": "Parallel slopes interpretation\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n2257.044\n272.007\n8.298\n0.000\n\n\nCards\n-36.964\n67.419\n-0.548\n0.584\n\n\nHas_BalanceYes\n3339.198\n221.117\n15.102\n0.000\n\n\n\n\n\n\nSlope of Cards is -36.964 regardless of Has_Balance level\nChange in Has_Balance corresponds to a shift in the intercept\n\nIntercept for No is 2257.044\nFor Yes shift intercept up 3339.198\n\n(i.e. intercept \\(= 2257.044 + 3339.198 = 5596.242\\))\n\n\nComplete Exercise 12"
  },
  {
    "objectID": "slides/07-mlr.html#interaction-terms-1",
    "href": "slides/07-mlr.html#interaction-terms-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Interaction terms",
    "text": "Interaction terms\n\nSometimes the relationship between a predictor variable and the response depends on the value of another predictor variable.\nThis is an interaction effect.\nTo account for this, we can include interaction terms in the model.\nWe want a model of the form:\n\n\\[\n\\begin{aligned}\\hat{Y} ~ =\n\\hat{\\beta}_0 & + \\hat{\\beta}_1 X_1 + \\hat{\\beta}_2 X_2 +\n\\hat{\\beta}_3X_1\\times X_2\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/07-mlr.html#interaction-terms-quantitative-and-categorical",
    "href": "slides/07-mlr.html#interaction-terms-quantitative-and-categorical",
    "title": "Multiple linear regression (MLR)",
    "section": "Interaction terms: Quantitative and Categorical",
    "text": "Interaction terms: Quantitative and Categorical\n\\[\n\\begin{aligned}\\hat{\\text{Limit}} ~ =\n\\hat{\\beta}_0 & + \\hat{\\beta}_1 \\text{Cards} + \\hat{\\beta}_2 \\text{Has_Balance} +\n\\hat{\\beta}_3\\text{Cards}\\times\\text{Has_Balance}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/07-mlr.html#interest-rate-vs.-cards-interaction-term",
    "href": "slides/07-mlr.html#interest-rate-vs.-cards-interaction-term",
    "title": "Multiple linear regression (MLR)",
    "section": "Interest rate vs. cards: interaction term",
    "text": "Interest rate vs. cards: interaction term"
  },
  {
    "objectID": "slides/07-mlr.html#bike-rentals-vs.-temperature-interaction-term",
    "href": "slides/07-mlr.html#bike-rentals-vs.-temperature-interaction-term",
    "title": "Multiple linear regression (MLR)",
    "section": "Bike Rentals vs. Temperature: interaction term",
    "text": "Bike Rentals vs. Temperature: interaction term"
  },
  {
    "objectID": "slides/07-mlr.html#interpreting-interaction-terms",
    "href": "slides/07-mlr.html#interpreting-interaction-terms",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting interaction terms",
    "text": "Interpreting interaction terms\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n2324.664\n515.510\n4.509\n0.000\n\n\nCards\n-60.924\n169.143\n-0.360\n0.719\n\n\nHas_BalanceYes\n3257.976\n570.455\n5.711\n0.000\n\n\nCards:Has_BalanceYes\n28.499\n184.470\n0.154\n0.877\n\n\n\n\n\n\nSlope of Rating depends on Has_Balance level\nEquivalent: fit two separate linear models on the data corresponding to each level of Has_Balance"
  },
  {
    "objectID": "slides/07-mlr.html#understanding-the-model",
    "href": "slides/07-mlr.html#understanding-the-model",
    "title": "Multiple linear regression (MLR)",
    "section": "Understanding the model",
    "text": "Understanding the model\n\\[\n\\begin{aligned}\n\\hat{Limit} &= 2324.664 - 60.924 \\times Cards \\\\\n&\\qquad+ 3257.976 \\times Has\\_Balance\\\\\n&\\qquad+ 28.499 \\times Cards \\times Has\\_Balance\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/07-mlr.html#interpreting-the-interaction-term",
    "href": "slides/07-mlr.html#interpreting-the-interaction-term",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting the interaction term",
    "text": "Interpreting the interaction term\n\nFor a borrower in no balance, the slope of Cards is \\(-60.924\\)\nFor a borrower with a balance, the slope of Cards is \\((-60.924 + 28.499=-32.425)\\)\n\n\nComplete Exercise 13 and 14."
  },
  {
    "objectID": "slides/07-mlr.html#interaction-terms-two-quantitative",
    "href": "slides/07-mlr.html#interaction-terms-two-quantitative",
    "title": "Multiple linear regression (MLR)",
    "section": "Interaction terms: Two Quantitative",
    "text": "Interaction terms: Two Quantitative\n\\[\n\\begin{aligned}\\hat{\\text{Limit}} ~ =\n\\hat{\\beta}_0 & + \\hat{\\beta}_1 \\text{Cards} + \\hat{\\beta}_2 \\text{Rating} +\n\\hat{\\beta}_3\\text{Cards}\\times\\text{Rating}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/07-mlr.html#interpreting-interaction-terms-1",
    "href": "slides/07-mlr.html#interpreting-interaction-terms-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting interaction terms",
    "text": "Interpreting interaction terms\n\nWhat the interaction means: The effect of the number of open credit cards on the credit limit depends on the borrowers credit rating and vice versa"
  },
  {
    "objectID": "slides/07-mlr.html#visualizing-model-no-interaction",
    "href": "slides/07-mlr.html#visualizing-model-no-interaction",
    "title": "Multiple linear regression (MLR)",
    "section": "Visualizing Model: No Interaction",
    "text": "Visualizing Model: No Interaction"
  },
  {
    "objectID": "slides/07-mlr.html#visualizing-interaction-model-exaggerated",
    "href": "slides/07-mlr.html#visualizing-interaction-model-exaggerated",
    "title": "Multiple linear regression (MLR)",
    "section": "Visualizing Interaction Model: Exaggerated",
    "text": "Visualizing Interaction Model: Exaggerated"
  },
  {
    "objectID": "slides/07-mlr.html#visualizing-interaction-model-real",
    "href": "slides/07-mlr.html#visualizing-interaction-model-real",
    "title": "Multiple linear regression (MLR)",
    "section": "Visualizing Interaction Model: Real",
    "text": "Visualizing Interaction Model: Real"
  },
  {
    "objectID": "slides/07-mlr.html#model-fit-1",
    "href": "slides/07-mlr.html#model-fit-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Model Fit",
    "text": "Model Fit\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-315.00522\n46.72441\n-6.74177\n0.00000\n\n\nCards\n-81.11569\n14.45641\n-5.61105\n0.00000\n\n\nRating\n14.83433\n0.11902\n124.64042\n0.00000\n\n\nCards:Rating\n0.02376\n0.03598\n0.66030\n0.50945\n\n\n\n\n\n\\[\n\\begin{aligned}\\hat{\\text{Limit}} ~ =\n&  -315.005 + 14.834~\\text{Rating} -81.116~\\text{Cards}\\\\\n& \\qquad+ 0.024~\\text{Rating}\\times\\text{Cards}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/07-mlr.html#interpreting-the-interaction-term-1",
    "href": "slides/07-mlr.html#interpreting-the-interaction-term-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting the interaction term",
    "text": "Interpreting the interaction term\n\nFor a fixed Rating the slope of Cards is \\((-81.116 + 0.024\\times\\text{Rating})\\)\nFor a fixed Cards the slope of Rating is \\((14.834 + 0.024\\times\\text{Cards})\\)"
  },
  {
    "objectID": "slides/07-mlr.html#whats-actually-happening",
    "href": "slides/07-mlr.html#whats-actually-happening",
    "title": "Multiple linear regression (MLR)",
    "section": "What’s actually happening:",
    "text": "What’s actually happening:\n\nCredit_int &lt;- Credit |&gt;\n  mutate(Interaction = Cards * Rating)\n\nCredit_int |&gt;\n  select(Limit, Cards, Rating, Interaction) |&gt;\n  head() |&gt;\n  kable()\n\n\n\n\nLimit\nCards\nRating\nInteraction\n\n\n\n\n3606\n2\n283\n566\n\n\n6645\n3\n483\n1449\n\n\n7075\n4\n514\n2056\n\n\n9504\n3\n681\n2043\n\n\n4897\n2\n357\n714\n\n\n8047\n4\n569\n2276"
  },
  {
    "objectID": "slides/07-mlr.html#whats-actually-happening-1",
    "href": "slides/07-mlr.html#whats-actually-happening-1",
    "title": "Multiple linear regression (MLR)",
    "section": "What’s actually happening:",
    "text": "What’s actually happening:\n\nlm(Limit ~ Cards + Rating + Interaction, data = Credit_int) |&gt;\n  tidy() |&gt;\n  kable(digits = 5)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-315.00522\n46.72441\n-6.74177\n0.00000\n\n\nCards\n-81.11569\n14.45641\n-5.61105\n0.00000\n\n\nRating\n14.83433\n0.11902\n124.64042\n0.00000\n\n\nInteraction\n0.02376\n0.03598\n0.66030\n0.50945"
  },
  {
    "objectID": "slides/07-mlr.html#recap",
    "href": "slides/07-mlr.html#recap",
    "title": "Multiple linear regression (MLR)",
    "section": "Recap",
    "text": "Recap\n\nIntroduced multiple linear regression\nInterpreted coefficients in the multiple linear regression model\nCalculated predictions and associated intervals for multiple linear regression models\nIntroduced categorical variables\nUsed interaction terms"
  },
  {
    "objectID": "slides/05-slr-conditions.html#computational-set-up",
    "href": "slides/05-slr-conditions.html#computational-set-up",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "Computational set up",
    "text": "Computational set up\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(ggformula)   # for plotting using formulas\nlibrary(broom)       # for formatting model output\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(kableExtra)  # also for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# Spotify Dataset\nspotify &lt;- read_csv(\"../data/spotify-popular.csv\")\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/05-slr-conditions.html#quick-data-cleaning",
    "href": "slides/05-slr-conditions.html#quick-data-cleaning",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "Quick Data Cleaning",
    "text": "Quick Data Cleaning\n\nspotify &lt;- spotify |&gt; \n  mutate(duration_min = duration_ms / 60000)\n\n\n\nWhat is this code doing?\nWhy might I be doing it?"
  },
  {
    "objectID": "slides/05-slr-conditions.html#the-regression-model-revisited",
    "href": "slides/05-slr-conditions.html#the-regression-model-revisited",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "The regression model, revisited",
    "text": "The regression model, revisited\n\nspotify_fit &lt;- lm(danceability ~ duration_min, data = spotify)\n\ntidy(spotify_fit, conf.int = TRUE, conf.level = 0.95) |&gt;\n   kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n0.781\n0.028\n28.351\n0.000\n0.727\n0.835\n\n\nduration_min\n-0.024\n0.008\n-3.151\n0.002\n-0.039\n-0.009\n\n\n\n\n\n\nThere is strong statistical evidence that there is a linear relationship between the duration of a song and it’s danceability.\nWe are 95% confidence that as the length of a song increases by one minute the danceability will decrease by between 0.009 and 0.039 units."
  },
  {
    "objectID": "slides/05-slr-conditions.html#mathematical-representation-visualized",
    "href": "slides/05-slr-conditions.html#mathematical-representation-visualized",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "Mathematical representation, visualized",
    "text": "Mathematical representation, visualized\n\\[\nY|X \\sim N(\\beta_0 + \\beta_1 X, \\sigma_\\epsilon^2)\n\\]\n\nImage source: Introduction to the Practice of Statistics (5th ed)"
  },
  {
    "objectID": "slides/05-slr-conditions.html#model-conditions-1",
    "href": "slides/05-slr-conditions.html#model-conditions-1",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "Model conditions",
    "text": "Model conditions\n\nLinearity: There is a linear relationship between the outcome and predictor variables\nConstant variance: The variability of the errors is equal for all values of the predictor variable\nNormality: The errors follow a normal distribution\nIndependence: The errors are independent from each other"
  },
  {
    "objectID": "slides/05-slr-conditions.html#warning",
    "href": "slides/05-slr-conditions.html#warning",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "WARNING",
    "text": "WARNING\n\nMany of these assumptions are for the population\nWe want to determine whether they are met from your data\nIn real life, these conditions are almost always violated in one way or another\nQuestions you should ask yourself:\n\nAre my conditions close enough to being satisfied that I can trust the results of my inference\nDo I have reason to believe that my conditions are GROSSLY violated?\nBased on what I see, how trustworthy do I think the results of my inference are."
  },
  {
    "objectID": "slides/05-slr-conditions.html#engage-soap-box",
    "href": "slides/05-slr-conditions.html#engage-soap-box",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "ENGAGE: SOAP BOX",
    "text": "ENGAGE: SOAP BOX\nStatistics and numbers are often used to make arguments seem more “rigorous” or infallible. I’m sure you’ve heard the phrase “the numbers are the numbers” or “you can’t argue with the numbers”. More often than not, this is BULLSHIT. Most data analyses involve making decision which are subjective. The interpretability of any form of statistical inference is heavily influenced by whether the assumptions and conditions of that inference is met, which they almost never are. It is up to the practitioner to determine whether those conditions are met and what impact those conditions have on the results of those analyses. In my work, I rarely encounter practitioners who even know what the conditions are, let alone understand why they are important. FURTHERMORE!!! the quality of your analysis is only as good as the quality of your data. Remember a crap study design will yield crap data which will yield crappy analysis. Statistical analyses yield one important form of evidence which should be combined with other forms of evidence when making an argument."
  },
  {
    "objectID": "slides/05-slr-conditions.html#linearity",
    "href": "slides/05-slr-conditions.html#linearity",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "Linearity",
    "text": "Linearity\n\nIf the linear model, \\(\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1X\\), adequately describes the relationship between \\(X\\) and \\(Y\\), then the residuals should reflect random (chance) error\nTo assess this, we can look at a plot of the residuals (\\(e_i\\)’s) vs. the fitted values (\\(\\hat{y}_i\\)’s) or predictors(or \\(x_i\\)’s)\nThere should be no distinguishable pattern in the residuals plot, i.e. the residuals should be randomly scattered\nA non-random pattern (e.g. a parabola) suggests a linear model that does not adequately describe the relationship between \\(X\\) and \\(Y\\)"
  },
  {
    "objectID": "slides/05-slr-conditions.html#linearity-1",
    "href": "slides/05-slr-conditions.html#linearity-1",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "Linearity",
    "text": "Linearity\n✅ The residuals vs. fitted values plot should show a random scatter of residuals (no distinguishable pattern or structure)"
  },
  {
    "objectID": "slides/05-slr-conditions.html#the-augment-function",
    "href": "slides/05-slr-conditions.html#the-augment-function",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "The augment function",
    "text": "The augment function\naugment is from the broom package:\n\nspotify_aug &lt;- augment(spotify_fit)\n\nhead(spotify_aug) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndanceability\nduration_min\n.fitted\n.resid\n.hat\n.sigma\n.cooksd\n.std.resid\n\n\n\n\n0.733\n2.464000\n0.7212126\n0.0117874\n0.0057228\n0.1304132\n0.0000237\n0.0907334\n\n\n0.630\n3.258650\n0.7019569\n-0.0719569\n0.0021749\n0.1303749\n0.0003332\n-0.5529036\n\n\n0.877\n3.864133\n0.6872849\n0.1897151\n0.0024253\n0.1301401\n0.0025838\n1.4579193\n\n\n0.831\n3.674783\n0.6918732\n0.1391268\n0.0020725\n0.1302669\n0.0011866\n1.0689702\n\n\n0.668\n2.650183\n0.7167011\n-0.0487011\n0.0044968\n0.1303962\n0.0003170\n-0.3746465\n\n\n0.626\n3.377017\n0.6990886\n-0.0730886\n0.0020230\n0.1303736\n0.0003196\n-0.5615572"
  },
  {
    "objectID": "slides/05-slr-conditions.html#residuals-vs.-fitted-values-code",
    "href": "slides/05-slr-conditions.html#residuals-vs.-fitted-values-code",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "Residuals vs. fitted values (code)",
    "text": "Residuals vs. fitted values (code)\n\nspotify_aug &lt;- augment(spotify_fit)\n\ngf_point(.resid ~ .fitted, data = spotify_aug) |&gt;\n  gf_hline(yintercept = 0, linetype = \"dashed\") |&gt;\n  gf_labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Residuals vs. fitted values\"\n  )"
  },
  {
    "objectID": "slides/05-slr-conditions.html#non-linear-relationships",
    "href": "slides/05-slr-conditions.html#non-linear-relationships",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "Non-linear relationships",
    "text": "Non-linear relationships"
  },
  {
    "objectID": "slides/05-slr-conditions.html#violations-of-linearity",
    "href": "slides/05-slr-conditions.html#violations-of-linearity",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "Violations of Linearity",
    "text": "Violations of Linearity\n\nImpact: inference relies on estimates of \\(\\sigma_\\epsilon\\) computed from residuals:\n\nResiduals will be larger in certain places so estimates will be inaccurate\nTherefore, inference (i.e. CIs and p-values) will be inaccurate\nMost importantly… your predictions will be wrong most of the time\n\nRemedy: transform your data (to come)\n\n\nComplete Exercises 1-3"
  },
  {
    "objectID": "slides/05-slr-conditions.html#constant-variance",
    "href": "slides/05-slr-conditions.html#constant-variance",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "Constant variance",
    "text": "Constant variance\n\nIf the spread of the distribution of \\(Y\\) is equal for all values of \\(X\\) then the spread of the residuals should be approximately equal for each value of \\(X\\)\nTo assess this, we can look at a plot of the residuals vs. the fitted values\nThe vertical spread of the residuals should be approximately equal as you move from left to right\nCAREFUL: Inconsistent distribution of \\(X\\)s can make it seem as if there is non-constant variance"
  },
  {
    "objectID": "slides/05-slr-conditions.html#constant-variance-1",
    "href": "slides/05-slr-conditions.html#constant-variance-1",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "Constant variance",
    "text": "Constant variance\n✅ The vertical spread of the residuals is relatively constant across the plot"
  },
  {
    "objectID": "slides/05-slr-conditions.html#non-constant-variance-fan-pattern",
    "href": "slides/05-slr-conditions.html#non-constant-variance-fan-pattern",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "Non-constant variance: Fan-Pattern",
    "text": "Non-constant variance: Fan-Pattern\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstant variance is frequently violated when the error/variance is proportional to the response variable\nWhose wealth fluctuates more per day… Dr. Friedlander’s or Elon Musk’s?"
  },
  {
    "objectID": "slides/05-slr-conditions.html#violations-of-constant-variance",
    "href": "slides/05-slr-conditions.html#violations-of-constant-variance",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "Violations of Constant Variance",
    "text": "Violations of Constant Variance\n\nImpact: inference relies on estimates of \\(\\sigma_\\epsilon\\) computed from residuals:\n\nResiduals will be larger in certain places so estimates will be inaccurate\nTherefore, inference (i.e. CIs and p-values) will be inaccurate\n\nRemedy: transform your data (to come)\n\n\nComplete Exercises 4"
  },
  {
    "objectID": "slides/05-slr-conditions.html#normality",
    "href": "slides/05-slr-conditions.html#normality",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "Normality",
    "text": "Normality\n\nThe linear model assumes that the distribution of \\(Y\\) is Normal for every value of \\(X\\)\nThis is impossible to check in practice, so we will look at the overall distribution of the residuals to assess if the normality assumption is satisfied\nA histogram of the residuals should look approximately normal, symmetric, without any huge outliers\nA normal QQ-plot falls along a diagonal line\nMost inferential methods for regression are robust to some departures from normality, so we can proceed with inference if the sample size is sufficiently large, roughly \\(n &gt; 30\\) depending on how non-normal your residuals look\n\nNotable exception: predictions intervals!"
  },
  {
    "objectID": "slides/05-slr-conditions.html#check-normality-using-a-histogram",
    "href": "slides/05-slr-conditions.html#check-normality-using-a-histogram",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "Check normality using a histogram",
    "text": "Check normality using a histogram"
  },
  {
    "objectID": "slides/05-slr-conditions.html#check-normality-using-a-qq-plot",
    "href": "slides/05-slr-conditions.html#check-normality-using-a-qq-plot",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "Check normality using a QQ-plot",
    "text": "Check normality using a QQ-plot\n\n\nCode\ngf_qq(~.resid, data = spotify_aug) |&gt;\n  gf_qqline() |&gt;\n  gf_labs(x = \"Theoretical quantile\",\n       y = \"Observed quantile\",\n       title = \"Normal QQ-plot of residuals\")\n\n\n\n\n\\(x\\)-axis: quantile we would expect from a true normal distribution\n\\(y\\)-axis: quantile we observe in the data\nBell-shaped does not necessarily equal normal… QQ-plot can detect distributions with heavier (i.e. more spread out) tails than a normal distribution"
  },
  {
    "objectID": "slides/05-slr-conditions.html#check-normality-using-a-qq-plot-1",
    "href": "slides/05-slr-conditions.html#check-normality-using-a-qq-plot-1",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "Check normality using a QQ-plot",
    "text": "Check normality using a QQ-plot\n\n\n\n\nCode\ngf_histogram(~.resid, data = spotify_aug,\n             bins=7, color = \"white\") |&gt;\n  gf_labs(\n    x = \"Residual\",\n    y = \"Count\",\n    title = \"Histogram of residuals\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngf_qq(~.resid, data = spotify_aug) |&gt;\n  gf_qqline() |&gt;\n  gf_labs(x = \"Theoretical quantile\",\n       y = \"Observed quantile\",\n       title = \"Normal QQ-plot of residuals\")\n\n\n\n\n\n\n\n\n\n\n\nAssess whether residuals lie along the diagonal line of the Quantile-quantile plot (QQ-plot).\nIf so, the residuals are normally distributed.\nNote: QQ-Plots are pretty sensitive so it doesn’t take too much departure to conclude non-normality"
  },
  {
    "objectID": "slides/05-slr-conditions.html#normality-1",
    "href": "slides/05-slr-conditions.html#normality-1",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "Normality",
    "text": "Normality\n❌ The residuals do not appear to follow a normal distribution, because the points do not lie on the diagonal line, so normality is not satisfied.\n✅ The sample size \\(n =  508&gt;  30\\), so the sample size is large enough to relax this condition and proceed with inference (mostly)."
  },
  {
    "objectID": "slides/05-slr-conditions.html#violations-of-normality",
    "href": "slides/05-slr-conditions.html#violations-of-normality",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "Violations of Normality",
    "text": "Violations of Normality\n\nImpact: depends what you want to do and how large your sample size is\n\nYour predictions intervals will be wrong… they will be symmetric when they shouldn’t be…\nIf you have a large sample size not a big deal for anything else\n\nRemedy… depends on what you want to do\n\nIf sample size is large enough and don’t care about prediction intervals… do nothing\nOtherwise, transform data… (hard for small sample sizes)\n\n\n\nComplete Exercises 5 and 6."
  },
  {
    "objectID": "slides/05-slr-conditions.html#independence",
    "href": "slides/05-slr-conditions.html#independence",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "Independence",
    "text": "Independence\n\nWe can often check the independence assumption based on the context of the data and how the observations were collected\nTwo common violations of the independence assumption:\n\nTemporal Correlation: If the data were collected over time, plot the residuals in time order to see if there is a pattern (serial correlation)\nCluster Effect: If there are subgroups represented in the data that are not accounted for in the model, you can color the points in the residual plots by group to see if the model systematically over or under predicts for a particular subgroup\nSpatial Correlation: If observations that were close to one another are more correlated than ones which are far apart then independent is violated"
  },
  {
    "objectID": "slides/05-slr-conditions.html#independence-1",
    "href": "slides/05-slr-conditions.html#independence-1",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "Independence",
    "text": "Independence\nComplete Exercise 7\n\n❌ Based on the information we have, it’s unlikely the data are independent."
  },
  {
    "objectID": "slides/05-slr-conditions.html#violations-of-independence",
    "href": "slides/05-slr-conditions.html#violations-of-independence",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "Violations of Independence",
    "text": "Violations of Independence\n\nImpact: depends on how it’s violated\n\nIn some cases you’ll underestimate p-values (too many false positives) and make CIs which are too narrow\nIn other cases you’ll do the opposite\n\nRemedy… depends on the source and type of dependence\n\nAdd variable accounting for dependence to model\nOtherwise, beyond the scope of this class\n\nTime-Series Analysis\nMixed-Effects Models\nSpacial Statistics"
  },
  {
    "objectID": "slides/05-slr-conditions.html#partitioning-variability",
    "href": "slides/05-slr-conditions.html#partitioning-variability",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "Partitioning Variability",
    "text": "Partitioning Variability\nLet’s think about variation:\n\nDATA = MODEL + ERROR\n\\(\\substack{\\text{Variation} \\\\ \\text{in Y}} = \\substack{\\text{Variation explained} \\\\ \\text{by model}} + \\substack{\\text{Variation not explained} \\\\ \\text{by model}}\\)"
  },
  {
    "objectID": "slides/05-slr-conditions.html#partitioning-variability-anova",
    "href": "slides/05-slr-conditions.html#partitioning-variability-anova",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "Partitioning Variability (ANOVA)",
    "text": "Partitioning Variability (ANOVA)\n\n\\(y_i - \\bar{y} = (\\hat{y}_i - \\bar{y}) + (y_i-\\hat{y}_i)\\)\nSquare and sum: \\(\\sum(y_i-\\bar{y})^2 = \\sum(\\hat{y} - \\bar{y})^2 + \\sum(y-\\hat{y})^2\\)\n\\(\\substack{\\text{Sum of squares} \\\\ \\text{Total}} = \\substack{\\text{Sum of squares} \\\\ \\text{model}} + \\substack{\\text{Sum of squares} \\\\ \\text{error}}\\)\n\\(SSTotal = SSModel + SSE\\)\n\\(SST = SSM + SSE\\)"
  },
  {
    "objectID": "slides/05-slr-conditions.html#anova-in-r",
    "href": "slides/05-slr-conditions.html#anova-in-r",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "ANOVA in R",
    "text": "ANOVA in R\n\nspotify_fit |&gt;\n  anova() |&gt;\n  tidy() |&gt;\n  kable()\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nduration_min\n1\n0.1685294\n0.1685294\n9.928516\n0.0017237\n\n\nResiduals\n506\n8.5889829\n0.0169743\nNA\nNA\n\n\n\n\n\n\n\nMore on this later in the semester\nComplete Exercise 8."
  },
  {
    "objectID": "slides/05-slr-conditions.html#recall-correlation-coefficient",
    "href": "slides/05-slr-conditions.html#recall-correlation-coefficient",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "Recall: Correlation Coefficient",
    "text": "Recall: Correlation Coefficient\n\nThe correlation coefficient, \\(r\\), is a number between -1 and +1 that measures how strong the linear relationship between two variables \\(x\\) and \\(y\\) is.\n\n\\[\nr = \\frac{\\sum(x_i - \\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum(x_i-\\bar{x})^2\\sum(y_i-\\bar{y})^2}}\n= \\frac{\\sum(x_i - \\bar{x})(y_i-\\bar{y})}{s_xs_y}\n\\]"
  },
  {
    "objectID": "slides/05-slr-conditions.html#two-statistics-r2",
    "href": "slides/05-slr-conditions.html#two-statistics-r2",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "Two statistics: \\(R^2\\)",
    "text": "Two statistics: \\(R^2\\)\n\nR-squared, \\(R^2\\), Coefficient of Determination : Percentage of variability in the outcome explained by the regression model (in the context of SLR, the predictor) \\[\nR^2 = Cor(y, \\hat{y})^2\n\\]\n\nAlso called PRE (Percent Reduction in Error) because: \\[\nR^2 = \\frac{SSModel}{SSTotal}\n\\]"
  },
  {
    "objectID": "slides/05-slr-conditions.html#two-statistics-rmse",
    "href": "slides/05-slr-conditions.html#two-statistics-rmse",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "Two statistics: RMSE",
    "text": "Two statistics: RMSE\n\nRoot mean square error, RMSE: A measure of the average error (average difference between observed and predicted values of the outcome) \\[\nRMSE = \\sqrt{\\frac{\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}{n}}\n\\]\n\nSometimes people just case about numerator (SSE) or version without the square-root (MSE)\nSometimes the denominator may have \\(n-1\\) instead\n\n\n\n\nWhat indicates a good model fit? Higher or lower \\(R^2\\)? Higher or lower RMSE?"
  },
  {
    "objectID": "slides/05-slr-conditions.html#r2",
    "href": "slides/05-slr-conditions.html#r2",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\n\nRanges between 0 (terrible predictor) and 1 (perfect predictor)\nHas no units\nCalculate with rsq() from yardstick package using the augmented data:\n\n\nlibrary(yardstick)\nspotify_aug &lt;- augment(spotify_fit)\n\nrsq(spotify_aug, truth = danceability, estimate = .fitted) |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrsq\nstandard\n0.019244"
  },
  {
    "objectID": "slides/05-slr-conditions.html#interpreting-r2",
    "href": "slides/05-slr-conditions.html#interpreting-r2",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "Interpreting \\(R^2\\)",
    "text": "Interpreting \\(R^2\\)\n\n🗳️ Discussion\n\n\nThe \\(R^2\\) of the model for danceability from Average_Income_K is 1.9%. Which of the following is the correct interpretation of this value?\n\n\nduration_min correctly predicts 1.9% of danceability.\n1.9% of the variability in danceability can be explained by duration_min.\n1.9% of the variability in duration_min can be explained by danceability.\n1.9% of the time danceability can be predicted by duration_min.\n\n\n\nComplete Exercise 9."
  },
  {
    "objectID": "slides/05-slr-conditions.html#activity",
    "href": "slides/05-slr-conditions.html#activity",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "Activity",
    "text": "Activity\n\nIn groups, at the board, design a simulation-based procedure for producing a p-value for the following hypothesis test.\n\n\\(H_0: R^2 = 0\\)\n\\(H_A: R^2 \\neq 0\\)"
  },
  {
    "objectID": "slides/05-slr-conditions.html#rmse",
    "href": "slides/05-slr-conditions.html#rmse",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "RMSE",
    "text": "RMSE\n\nRanges between 0 (perfect predictor) and infinity (terrible predictor)\nSame units as the response variable\nInterpretation (kind of): how much does my model miss by, on average.\nCalculate with rmse() from yardstick package using the augmented data:\n\n\nrmse(spotify_aug, truth = danceability, estimate = .fitted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.130\n\n\n\nComplete Exercise 10."
  },
  {
    "objectID": "slides/05-slr-conditions.html#using-the-word-good",
    "href": "slides/05-slr-conditions.html#using-the-word-good",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "Using the word “Good”",
    "text": "Using the word “Good”\n\nThere is no such thing as a “Good” \\(R^2\\) or, especially, RMSE without context\nWhether your model is a “Good” model depends on many things:\n\nWhat are you using your model for?\nHow good are other models?"
  },
  {
    "objectID": "slides/05-slr-conditions.html#recap",
    "href": "slides/05-slr-conditions.html#recap",
    "title": "SLR: Conditions + Model Evaluation",
    "section": "Recap",
    "text": "Recap\nUsed residual plots to check conditions for SLR:\n\n\n\nLinearity (residuals vs fitted vals)\nConstant variance (residuals vs fitted vals)\n\n\n\nNormality (histogram/QQ-plot of residuals)\nIndependence (knowledge of data collection)\n\n\n\nNote: Predictions are still valid as long as linearity is met but p-values and CIs are not without other three\n\n\n\nCan decompose total variation (SST) into variation explained by the model (SSM) and leftover variation (SSE)\nTwo metrics for evaluating and comparing models:\n\n\\(R^2\\): What proportion of the variation in the response variable is explained by the model?\n\\(RMSE\\): How far is does my model miss by on average?"
  },
  {
    "objectID": "slides/03-slr-sbi.html#application-exercise",
    "href": "slides/03-slr-sbi.html#application-exercise",
    "title": "SLR: Simulation-based inference",
    "section": "Application exercise",
    "text": "Application exercise\n\n📋 AE 03 - Simulation Based Inference\n\nComplete Exercises 0-2."
  },
  {
    "objectID": "slides/03-slr-sbi.html#data-san-antonio-income-organic-food-access",
    "href": "slides/03-slr-sbi.html#data-san-antonio-income-organic-food-access",
    "title": "SLR: Simulation-based inference",
    "section": "Data: San Antonio Income & Organic Food Access",
    "text": "Data: San Antonio Income & Organic Food Access\n\n\n\nAverage household income (per zip code) and number of organic vegetable offerings in San Antonio, TX\nData from HEB website, compiles by high school student Linda Saucedo, Fall 2019\nSource: Skew The Script\n\n\n\n\nGoal: Use the average household income to understand variation in access to organic foods."
  },
  {
    "objectID": "slides/03-slr-sbi.html#computational-setup",
    "href": "slides/03-slr-sbi.html#computational-setup",
    "title": "SLR: Simulation-based inference",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(ggformula)  # for modeling\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for neatly formatted tables\nlibrary(kableExtra)  # also for neatly formatted tables\n\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 16))"
  },
  {
    "objectID": "slides/03-slr-sbi.html#exploratory-data-analysis",
    "href": "slides/03-slr-sbi.html#exploratory-data-analysis",
    "title": "SLR: Simulation-based inference",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\n\nCode\nheb &lt;- read_csv(\"../data/HEBIncome.csv\") |&gt; \n  mutate(Avg_Income_K = Avg_Household_Income/1000)\n\ngf_point(Number_Organic ~ Avg_Income_K, data = heb, alpha = 0.7) |&gt; \n  gf_labs(\n    x = \"Average Household Income (in thousands)\",\n    y = \"Number of Organic Vegetables\",\n  ) |&gt; \n  gf_refine(scale_x_continuous(labels = label_dollar()))"
  },
  {
    "objectID": "slides/03-slr-sbi.html#modeling",
    "href": "slides/03-slr-sbi.html#modeling",
    "title": "SLR: Simulation-based inference",
    "section": "Modeling",
    "text": "Modeling\n\nheb_fit &lt;- lm(Number_Organic ~ Avg_Income_K, data = heb)\n\ntidy(heb_fit) |&gt;\n  kable(digits=2) #neatly format table to 2 digits\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-14.72\n9.30\n-1.58\n0.12\n\n\nAvg_Income_K\n0.96\n0.13\n7.50\n0.00\n\n\n\n\n\n\n\nIntercept: HEBs in Zip Codes with an average household income of $0 are expected to have -14.72 organic vegetable options, on average.\n\nIs this interpretation useful?\n\nSlope: For each additional $1,000 in average household income, we expect the number of organic options available at nearby HEBs to increase by 0.96, on average."
  },
  {
    "objectID": "slides/03-slr-sbi.html#from-sample-to-population",
    "href": "slides/03-slr-sbi.html#from-sample-to-population",
    "title": "SLR: Simulation-based inference",
    "section": "From sample to population",
    "text": "From sample to population\n\nFor each additional $1,000 in average household income, we expect the number of organic options available at nearby HEBs to increase by 0.96, on average.\n\n\n\nEstimate is valid for the single sample of 37 HEBs\nWhat if we’re not interested quantifying the relationship between the household income and access to organic vegetables in this single sample?\nWhat if we want to say something about the relationship between these variables for all supermarkets in America?"
  },
  {
    "objectID": "slides/03-slr-sbi.html#statistical-inference",
    "href": "slides/03-slr-sbi.html#statistical-inference",
    "title": "SLR: Simulation-based inference",
    "section": "Statistical inference",
    "text": "Statistical inference\n\nStatistical inference refers to ideas, methods, and tools for to generalizing the single observed sample to make statements (inferences) about the population it comes from\nFor our inferences to be valid, the sample should be random and representative of the population we’re interested in"
  },
  {
    "objectID": "slides/03-slr-sbi.html#sampling-is-natural",
    "href": "slides/03-slr-sbi.html#sampling-is-natural",
    "title": "SLR: Simulation-based inference",
    "section": "Sampling is natural",
    "text": "Sampling is natural\n\n\nWhen you taste a spoonful of soup and decide the spoonful you tasted isn’t salty enough, that’s exploratory analysis\nIf you generalize and conclude that your entire soup needs salt, that’s an inference\nFor your inference to be valid, the spoonful you tasted (the sample) needs to be representative of the entire pot (the population)"
  },
  {
    "objectID": "slides/03-slr-sbi.html#confidence-interval-via-bootstrapping",
    "href": "slides/03-slr-sbi.html#confidence-interval-via-bootstrapping",
    "title": "SLR: Simulation-based inference",
    "section": "Confidence interval via bootstrapping",
    "text": "Confidence interval via bootstrapping\n\nBootstrap new samples from the original sample\nFit models to each of the samples and estimate the slope\nUse features of the distribution of the bootstrapped slopes to construct a confidence interval"
  },
  {
    "objectID": "slides/03-slr-sbi.html#inference-for-simple-linear-regression",
    "href": "slides/03-slr-sbi.html#inference-for-simple-linear-regression",
    "title": "SLR: Simulation-based inference",
    "section": "Inference for simple linear regression",
    "text": "Inference for simple linear regression\n\nCalculate a confidence interval for the slope, \\(\\beta_1\\)\nConduct a hypothesis test for the slope, \\(\\beta_1\\)\nWhy not \\(\\beta_0\\)?\nWe can but it isn’t super interesting typically\n\n\n\n\nWhat is a confidence interval?\nWhat is a hypothesis test?"
  },
  {
    "objectID": "slides/03-slr-sbi.html#confidence-interval",
    "href": "slides/03-slr-sbi.html#confidence-interval",
    "title": "SLR: Simulation-based inference",
    "section": "Confidence interval",
    "text": "Confidence interval\n\nConfidence interval: plausible range of values for a population parameter\nsingle point estimate \\(\\implies\\) fishing in a murky lake with a spear\nconfidence interval \\(\\implies\\) fishing with a net\n\nWe can throw a spear where we saw a fish but we will probably miss, if we toss a net in that area, we have a good chance of catching the fish\nIf we report a point estimate, we probably will not hit the exact population parameter, but if we report a range of plausible values we have a good shot at capturing the parameter\nHigh confidence \\(\\implies\\) wider interval (larger net)\n\nRemember: single CI \\(\\implies\\) either you hit parameter or you don’t\n\nFavorite Visualization"
  },
  {
    "objectID": "slides/03-slr-sbi.html#confidence-interval-for-the-slope-1",
    "href": "slides/03-slr-sbi.html#confidence-interval-for-the-slope-1",
    "title": "SLR: Simulation-based inference",
    "section": "Confidence interval for the slope",
    "text": "Confidence interval for the slope\nA confidence interval will allow us to make a statement like “For each $1K in average income, the model predicts the number of organic vegetables available at local supermarkets to be higher, on average, by 0.96, plus or minus X options.”\n\n\nShould X be 1? 2? 3?\nIf we were to take another sample of 37 would we expect the slope calculated based on that sample to be exactly 0.96? Off by 1? 2? 3?\nThe answer depends on how variable (from one sample to another sample) the sample statistic (the slope) is\nWe need a way to quantify the variability of the sample statistic"
  },
  {
    "objectID": "slides/03-slr-sbi.html#quantify-the-variability-of-the-slope",
    "href": "slides/03-slr-sbi.html#quantify-the-variability-of-the-slope",
    "title": "SLR: Simulation-based inference",
    "section": "Quantify the variability of the slope",
    "text": "Quantify the variability of the slope\nfor estimation\n\nTwo approaches:\n\nVia simulation (what we’ll do today)\nVia mathematical models (what we’ll do soon)\n\nBootstrapping to quantify the variability of the slope for the purpose of estimation:\n\nGenerate new samples by sampling with replacement from the original sample\nFit models to each of the new samples and estimate the slope\nUse features of the distribution of the bootstrapped slopes to construct a confidence interval"
  },
  {
    "objectID": "slides/03-slr-sbi.html#original-sample",
    "href": "slides/03-slr-sbi.html#original-sample",
    "title": "SLR: Simulation-based inference",
    "section": "Original Sample",
    "text": "Original Sample"
  },
  {
    "objectID": "slides/03-slr-sbi.html#bootstrap-sample-1",
    "href": "slides/03-slr-sbi.html#bootstrap-sample-1",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrap sample 1",
    "text": "Bootstrap sample 1"
  },
  {
    "objectID": "slides/03-slr-sbi.html#bootstrap-sample-2",
    "href": "slides/03-slr-sbi.html#bootstrap-sample-2",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrap sample 2",
    "text": "Bootstrap sample 2"
  },
  {
    "objectID": "slides/03-slr-sbi.html#bootstrap-sample-3",
    "href": "slides/03-slr-sbi.html#bootstrap-sample-3",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrap sample 3",
    "text": "Bootstrap sample 3"
  },
  {
    "objectID": "slides/03-slr-sbi.html#bootstrap-sample-4",
    "href": "slides/03-slr-sbi.html#bootstrap-sample-4",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrap sample 4",
    "text": "Bootstrap sample 4"
  },
  {
    "objectID": "slides/03-slr-sbi.html#bootstrap-sample-5",
    "href": "slides/03-slr-sbi.html#bootstrap-sample-5",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrap sample 5",
    "text": "Bootstrap sample 5"
  },
  {
    "objectID": "slides/03-slr-sbi.html#bootstrap-samples-1---5",
    "href": "slides/03-slr-sbi.html#bootstrap-samples-1---5",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrap samples 1 - 5",
    "text": "Bootstrap samples 1 - 5"
  },
  {
    "objectID": "slides/03-slr-sbi.html#bootstrap-samples-1---100",
    "href": "slides/03-slr-sbi.html#bootstrap-samples-1---100",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrap samples 1 - 100",
    "text": "Bootstrap samples 1 - 100"
  },
  {
    "objectID": "slides/03-slr-sbi.html#slopes-of-bootstrap-samples",
    "href": "slides/03-slr-sbi.html#slopes-of-bootstrap-samples",
    "title": "SLR: Simulation-based inference",
    "section": "Slopes of bootstrap samples",
    "text": "Slopes of bootstrap samples\n\nFill in the blank: For each additional $1k in average household income, the model predicts the number of organic vegetables available to be higher, on average, by 0.96, plus or minus ___."
  },
  {
    "objectID": "slides/03-slr-sbi.html#slopes-of-bootstrap-samples-1",
    "href": "slides/03-slr-sbi.html#slopes-of-bootstrap-samples-1",
    "title": "SLR: Simulation-based inference",
    "section": "Slopes of bootstrap samples",
    "text": "Slopes of bootstrap samples\n\nFill in the blank: For each additional $1k in average household income, the model predicts the number of organic vegetables available to be higher, on average, by 0.96, plus or minus ___."
  },
  {
    "objectID": "slides/03-slr-sbi.html#confidence-level",
    "href": "slides/03-slr-sbi.html#confidence-level",
    "title": "SLR: Simulation-based inference",
    "section": "Confidence level",
    "text": "Confidence level\n\nHow confident are you that the true slope is between 0.8 and 1.2? How about 0.9 and 1.0? How about 1.0 and 1.4?"
  },
  {
    "objectID": "slides/03-slr-sbi.html#confidence-interval-1",
    "href": "slides/03-slr-sbi.html#confidence-interval-1",
    "title": "SLR: Simulation-based inference",
    "section": "95% confidence interval",
    "text": "95% confidence interval\n\n\n95% bootstrapped confidence interval: bounded by the middle 95% of the bootstrap distribution\nWe are 95% confident that for each additional $1K in average household income, the model predicts the number of organic vegetables options at local supermarkets to be higher, on average, by 0.81 to 1.31."
  },
  {
    "objectID": "slides/03-slr-sbi.html#computing-the-ci-for-the-slope-i",
    "href": "slides/03-slr-sbi.html#computing-the-ci-for-the-slope-i",
    "title": "SLR: Simulation-based inference",
    "section": "Computing the CI for the slope I",
    "text": "Computing the CI for the slope I\nCalculate the observed slope:\n\nlibrary(infer) # package that does Simulation-Based Inference\n\nobserved_fit &lt;- heb |&gt;\n  specify(Number_Organic ~ Avg_Income_K) |&gt;\n  fit()\n\nobserved_fit\n\n# A tibble: 2 × 2\n  term         estimate\n  &lt;chr&gt;           &lt;dbl&gt;\n1 intercept     -14.7  \n2 Avg_Income_K    0.959"
  },
  {
    "objectID": "slides/03-slr-sbi.html#computing-the-ci-for-the-slope-ii",
    "href": "slides/03-slr-sbi.html#computing-the-ci-for-the-slope-ii",
    "title": "SLR: Simulation-based inference",
    "section": "Computing the CI for the slope II",
    "text": "Computing the CI for the slope II\nTake 100 bootstrap samples and fit models to each one:\n\nset.seed(1120)\n\nboot_fits &lt;- heb |&gt;\n  specify(Number_Organic ~ Avg_Income_K) |&gt;\n  generate(reps = 100, type = \"bootstrap\") |&gt;\n  fit()\n\nboot_fits\n\n# A tibble: 200 × 3\n# Groups:   replicate [100]\n   replicate term         estimate\n       &lt;int&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1         1 intercept     -40.9  \n 2         1 Avg_Income_K    1.25 \n 3         2 intercept     -23.9  \n 4         2 Avg_Income_K    1.09 \n 5         3 intercept     -18.6  \n 6         3 Avg_Income_K    1.02 \n 7         4 intercept      -1.96 \n 8         4 Avg_Income_K    0.828\n 9         5 intercept     -15.1  \n10         5 Avg_Income_K    0.951\n# ℹ 190 more rows"
  },
  {
    "objectID": "slides/03-slr-sbi.html#computing-the-ci-for-the-slope-iii",
    "href": "slides/03-slr-sbi.html#computing-the-ci-for-the-slope-iii",
    "title": "SLR: Simulation-based inference",
    "section": "Computing the CI for the slope III",
    "text": "Computing the CI for the slope III\nPercentile method: Compute the 95% CI as the middle 95% of the bootstrap distribution:\n\nget_confidence_interval(\n  boot_fits, \n  point_estimate = observed_fit, \n  level = 0.95,\n  type = \"percentile\" #default method\n)\n\n# A tibble: 2 × 3\n  term         lower_ci upper_ci\n  &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;\n1 Avg_Income_K    0.822     1.27\n2 intercept     -34.5      -2.56\n\n\nComplete Exercises 3-6"
  },
  {
    "objectID": "slides/03-slr-sbi.html#precision-vs.-accuracy",
    "href": "slides/03-slr-sbi.html#precision-vs.-accuracy",
    "title": "SLR: Simulation-based inference",
    "section": "Precision vs. accuracy",
    "text": "Precision vs. accuracy\n\nIf we want to be very certain that we capture the population parameter, should we use a wider or a narrower interval? What drawbacks are associated with using a wider interval?"
  },
  {
    "objectID": "slides/03-slr-sbi.html#precision-vs.-accuracy-1",
    "href": "slides/03-slr-sbi.html#precision-vs.-accuracy-1",
    "title": "SLR: Simulation-based inference",
    "section": "Precision vs. accuracy",
    "text": "Precision vs. accuracy\n\nHow can we get best of both worlds – high precision and high accuracy?"
  },
  {
    "objectID": "slides/03-slr-sbi.html#changing-confidence-level",
    "href": "slides/03-slr-sbi.html#changing-confidence-level",
    "title": "SLR: Simulation-based inference",
    "section": "Changing confidence level",
    "text": "Changing confidence level\n\nHow would you modify the following code to calculate a 90% confidence interval? How would you modify it for a 99% confidence interval?\n\n\nget_confidence_interval(\n  boot_fits, \n  point_estimate = observed_fit, \n  level = 0.95,\n  type = \"percentile\"\n)\n\n# A tibble: 2 × 3\n  term         lower_ci upper_ci\n  &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;\n1 Avg_Income_K    0.822     1.27\n2 intercept     -34.5      -2.56"
  },
  {
    "objectID": "slides/03-slr-sbi.html#changing-confidence-level-1",
    "href": "slides/03-slr-sbi.html#changing-confidence-level-1",
    "title": "SLR: Simulation-based inference",
    "section": "Changing confidence level",
    "text": "Changing confidence level\n\n## confidence level: 90%\nget_confidence_interval(\n  boot_fits, point_estimate = observed_fit, \n  level = 0.90, type = \"percentile\"\n)\n\n# A tibble: 2 × 3\n  term         lower_ci upper_ci\n  &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;\n1 Avg_Income_K    0.829     1.23\n2 intercept     -31.7      -3.76\n\n## confidence level: 99%\nget_confidence_interval(\n  boot_fits, point_estimate = observed_fit, \n  level = 0.99, type = \"percentile\"\n)\n\n# A tibble: 2 × 3\n  term         lower_ci upper_ci\n  &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;\n1 Avg_Income_K    0.795    1.36 \n2 intercept     -43.3     -0.535\n\n\nComplete Exercises 7-11."
  },
  {
    "objectID": "slides/03-slr-sbi.html#the-regression-model",
    "href": "slides/03-slr-sbi.html#the-regression-model",
    "title": "SLR: Simulation-based inference",
    "section": "The regression model",
    "text": "The regression model\n\nheb_fit &lt;- lm(Number_Organic ~ Avg_Income_K, data = heb)\n\ntidy(heb_fit) |&gt;\n  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-14.72\n9.30\n-1.58\n0.12\n\n\nAvg_Income_K\n0.96\n0.13\n7.50\n0.00\n\n\n\n\n\n\nSlope: For each additional $1,000 in average household income, we expect the number of organic options available at nearby HEBs to increase by 0.96, on average."
  },
  {
    "objectID": "slides/03-slr-sbi.html#research-question-and-hypotheses",
    "href": "slides/03-slr-sbi.html#research-question-and-hypotheses",
    "title": "SLR: Simulation-based inference",
    "section": "Research question and hypotheses",
    "text": "Research question and hypotheses\n“Do the data provide sufficient evidence that \\(\\beta_1\\) (the true slope for the population) is different from 0?”\n\nNull hypothesis: there is no linear relationship between Number_Organic and Avg_Income_K\n\\[\nH_0: \\beta_1 = 0\n\\]\n\n\nAlternative hypothesis: there is a linear relationship between Number_Organic and Avg_Income_K\n\\[\nH_A: \\beta_1 \\ne 0\n\\]"
  },
  {
    "objectID": "slides/03-slr-sbi.html#hypothesis-testing-as-a-court-trial",
    "href": "slides/03-slr-sbi.html#hypothesis-testing-as-a-court-trial",
    "title": "SLR: Simulation-based inference",
    "section": "Hypothesis testing as a court trial",
    "text": "Hypothesis testing as a court trial\n\nNull hypothesis, \\(H_0\\): Defendant is innocent\nAlternative hypothesis, \\(H_A\\): Defendant is guilty\nPresent the evidence: Collect data\nJudge the evidence: “Could these data plausibly have happened by chance if the null hypothesis were true?”\n\nYes: Fail to reject \\(H_0\\)\nNo: Reject \\(H_0\\)\n\nNot guilty \\(\\neq\\) innocent \\(\\implies\\) why we say “fail to reject the null” rather than “accept the null”"
  },
  {
    "objectID": "slides/03-slr-sbi.html#hypothesis-testing-framework",
    "href": "slides/03-slr-sbi.html#hypothesis-testing-framework",
    "title": "SLR: Simulation-based inference",
    "section": "Hypothesis testing framework",
    "text": "Hypothesis testing framework\n\nStart with a null hypothesis, \\(H_0\\) that represents the status quo\nSet an alternative hypothesis, \\(H_A\\) that represents the research question, i.e. claim we’re testing\nUnder the assumption that the null hypothesis is true, calculate a p-value (probability of getting outcome or outcome even more favorable to the alternative)\n\nif the test results suggest that the data do not provide convincing evidence for the alternative hypothesis, stick with the null hypothesis\nif they do, then reject the null hypothesis in favor of the alternative\n\n\nComplete Exercise 12"
  },
  {
    "objectID": "slides/03-slr-sbi.html#quantify-the-variability-of-the-slope-1",
    "href": "slides/03-slr-sbi.html#quantify-the-variability-of-the-slope-1",
    "title": "SLR: Simulation-based inference",
    "section": "Quantify the variability of the slope",
    "text": "Quantify the variability of the slope\nfor testing\n\nTwo approaches:\n\nVia simulation\nVia mathematical models\n\nUse Randomization to quantify the variability of the slope for the purpose of testing, under the assumption that the null hypothesis is true:\n\nSimulate new samples from the original sample via permutation\nFit models to each of the samples and estimate the slope\nUse features of the distribution of the permuted slopes to conduct a hypothesis test"
  },
  {
    "objectID": "slides/03-slr-sbi.html#permutation-described",
    "href": "slides/03-slr-sbi.html#permutation-described",
    "title": "SLR: Simulation-based inference",
    "section": "Permutation, described",
    "text": "Permutation, described\n\n\n\nUse permuting to simulate data under the assumption the null hypothesis is true and measure the natural variability in the data due to sampling, not due to variables being correlated\n\nPermute/shuffle response variable to eliminate any existing relationship with explanatory variable\n\nEach Number_Organic value is randomly assigned to the Avg_Household_K, i.e. Number_Organic and Avg_Household_K are no longer matched for a given store\n\n\n\n\n# A tibble: 37 × 3\n   Number_Organic_Original Number_Organic_Permuted Avg_Income_K\n                     &lt;dbl&gt;                   &lt;dbl&gt;        &lt;dbl&gt;\n 1                      36                      73         71.2\n 2                       4                      29         34.2\n 3                      28                      35         71.2\n 4                      31                      38         48.8\n 5                      78                      78         78.1\n 6                      14                      14         40.5\n 7                      12                      82         38.2\n 8                      18                      31         50.4\n 9                      38                       4         49.4\n10                      84                      12         66.1\n# ℹ 27 more rows"
  },
  {
    "objectID": "slides/03-slr-sbi.html#permutation-visualized",
    "href": "slides/03-slr-sbi.html#permutation-visualized",
    "title": "SLR: Simulation-based inference",
    "section": "Permutation, visualized",
    "text": "Permutation, visualized\n\n\n\nEach of the observed values for area (and for price) exist in both the observed data plot as well as the permuted price plot\nPermuting removes the relationship between area and price"
  },
  {
    "objectID": "slides/03-slr-sbi.html#permutation-repeated",
    "href": "slides/03-slr-sbi.html#permutation-repeated",
    "title": "SLR: Simulation-based inference",
    "section": "Permutation, repeated",
    "text": "Permutation, repeated\nRepeated permutations allow for quantifying the variability in the slope under the condition that there is no linear relationship (i.e., that the null hypothesis is true)"
  },
  {
    "objectID": "slides/03-slr-sbi.html#concluding-the-hypothesis-test",
    "href": "slides/03-slr-sbi.html#concluding-the-hypothesis-test",
    "title": "SLR: Simulation-based inference",
    "section": "Concluding the hypothesis test",
    "text": "Concluding the hypothesis test\n\nIs the observed slope of \\(\\hat{\\beta_1} = 0.96\\) (or an even more extreme slope) a likely outcome under the null hypothesis that \\(\\beta = 0\\)? What does this mean for our original question: “Do the data provide sufficient evidence that \\(\\beta_1\\) (the true slope for the population) is different from 0?”"
  },
  {
    "objectID": "slides/03-slr-sbi.html#permutation-pipeline-i",
    "href": "slides/03-slr-sbi.html#permutation-pipeline-i",
    "title": "SLR: Simulation-based inference",
    "section": "Permutation pipeline I",
    "text": "Permutation pipeline I\n\nset.seed(1218)\n\nheb |&gt;\n  specify(Number_Organic ~ Avg_Income_K)\n\nResponse: Number_Organic (numeric)\nExplanatory: Avg_Income_K (numeric)\n# A tibble: 37 × 2\n   Number_Organic Avg_Income_K\n            &lt;dbl&gt;        &lt;dbl&gt;\n 1             36         71.2\n 2              4         34.2\n 3             28         71.2\n 4             31         48.8\n 5             78         78.1\n 6             14         40.5\n 7             12         38.2\n 8             18         50.4\n 9             38         49.4\n10             84         66.1\n# ℹ 27 more rows"
  },
  {
    "objectID": "slides/03-slr-sbi.html#permutation-pipeline-ii",
    "href": "slides/03-slr-sbi.html#permutation-pipeline-ii",
    "title": "SLR: Simulation-based inference",
    "section": "Permutation pipeline II",
    "text": "Permutation pipeline II\n\nset.seed(1218)\n\nheb |&gt;\n  specify(Number_Organic ~ Avg_Income_K) |&gt;\n  hypothesize(null = \"independence\")\n\nResponse: Number_Organic (numeric)\nExplanatory: Avg_Income_K (numeric)\nNull Hypothesis: independence\n# A tibble: 37 × 2\n   Number_Organic Avg_Income_K\n            &lt;dbl&gt;        &lt;dbl&gt;\n 1             36         71.2\n 2              4         34.2\n 3             28         71.2\n 4             31         48.8\n 5             78         78.1\n 6             14         40.5\n 7             12         38.2\n 8             18         50.4\n 9             38         49.4\n10             84         66.1\n# ℹ 27 more rows"
  },
  {
    "objectID": "slides/03-slr-sbi.html#permutation-pipeline-iii",
    "href": "slides/03-slr-sbi.html#permutation-pipeline-iii",
    "title": "SLR: Simulation-based inference",
    "section": "Permutation pipeline III",
    "text": "Permutation pipeline III\n\nset.seed(1218)\n\nheb |&gt;\n  specify(Number_Organic ~ Avg_Income_K) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 1000, type = \"permute\")\n\nResponse: Number_Organic (numeric)\nExplanatory: Avg_Income_K (numeric)\nNull Hypothesis: independence\n# A tibble: 37,000 × 3\n# Groups:   replicate [1,000]\n   Number_Organic Avg_Income_K replicate\n            &lt;dbl&gt;        &lt;dbl&gt;     &lt;int&gt;\n 1             38         71.2         1\n 2             56         34.2         1\n 3             28         71.2         1\n 4             14         48.8         1\n 5             29         78.1         1\n 6             36         40.5         1\n 7             84         38.2         1\n 8             18         50.4         1\n 9             96         49.4         1\n10             26         66.1         1\n# ℹ 36,990 more rows"
  },
  {
    "objectID": "slides/03-slr-sbi.html#permutation-pipeline-iv",
    "href": "slides/03-slr-sbi.html#permutation-pipeline-iv",
    "title": "SLR: Simulation-based inference",
    "section": "Permutation pipeline IV",
    "text": "Permutation pipeline IV\n\nset.seed(1218)\n\nheb |&gt;\n  specify(Number_Organic ~ Avg_Income_K) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 1000, type = \"permute\") |&gt;\n  fit()\n\n# A tibble: 2,000 × 3\n# Groups:   replicate [1,000]\n   replicate term         estimate\n       &lt;int&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1         1 intercept     47.8   \n 2         1 Avg_Income_K   0.0555\n 3         2 intercept     58.0   \n 4         2 Avg_Income_K  -0.0914\n 5         3 intercept     57.3   \n 6         3 Avg_Income_K  -0.0817\n 7         4 intercept     78.9   \n 8         4 Avg_Income_K  -0.394 \n 9         5 intercept     34.8   \n10         5 Avg_Income_K   0.244 \n# ℹ 1,990 more rows"
  },
  {
    "objectID": "slides/03-slr-sbi.html#permutation-pipeline-v",
    "href": "slides/03-slr-sbi.html#permutation-pipeline-v",
    "title": "SLR: Simulation-based inference",
    "section": "Permutation pipeline V",
    "text": "Permutation pipeline V\n\nset.seed(1218)\n\nnull_dist &lt;- heb |&gt;\n  specify(Number_Organic ~ Avg_Income_K) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 1000, type = \"permute\") |&gt;\n  fit()"
  },
  {
    "objectID": "slides/03-slr-sbi.html#visualize-the-null-distribution",
    "href": "slides/03-slr-sbi.html#visualize-the-null-distribution",
    "title": "SLR: Simulation-based inference",
    "section": "Visualize the null distribution",
    "text": "Visualize the null distribution\n\nnull_dist |&gt;\n  filter(term == \"Avg_Income_K\") |&gt;\n  gf_histogram(~estimate, color = \"white\")\n\n\nComplete Exercises 13 and 14."
  },
  {
    "objectID": "slides/03-slr-sbi.html#reason-around-the-p-value",
    "href": "slides/03-slr-sbi.html#reason-around-the-p-value",
    "title": "SLR: Simulation-based inference",
    "section": "Reason around the p-value",
    "text": "Reason around the p-value\n\nIn a world where the there is no relationship between the the number of organic food options and the nearby average household income (\\(\\beta_1 = 0\\)), what is the probability that we observe a sample of 37 stores where the slope fo the model predicting the number of organic options from average household income is 0.96 or even more extreme?"
  },
  {
    "objectID": "slides/03-slr-sbi.html#compute-the-p-value",
    "href": "slides/03-slr-sbi.html#compute-the-p-value",
    "title": "SLR: Simulation-based inference",
    "section": "Compute the p-value",
    "text": "Compute the p-value\n\nWhat does this warning mean?\n\n\nget_p_value(\n  null_dist,\n  obs_stat = observed_fit, # Same as from confidence intervals\n  direction = \"two-sided\"\n)\n\nWarning: Please be cautious in reporting a p-value of 0. This result is an approximation\nbased on the number of `reps` chosen in the `generate()` step.\nℹ See `get_p_value()` (`?infer::get_p_value()`) for more information.\nPlease be cautious in reporting a p-value of 0. This result is an approximation\nbased on the number of `reps` chosen in the `generate()` step.\nℹ See `get_p_value()` (`?infer::get_p_value()`) for more information.\n\n\n# A tibble: 2 × 2\n  term         p_value\n  &lt;chr&gt;          &lt;dbl&gt;\n1 Avg_Income_K       0\n2 intercept          0\n\n\nComplete Exercises 15 and 16."
  },
  {
    "objectID": "slides/03-slr-sbi.html#recap",
    "href": "slides/03-slr-sbi.html#recap",
    "title": "SLR: Simulation-based inference",
    "section": "Recap",
    "text": "Recap\n\nPopulation: Complete set of observations of whatever we are studying, e.g., people, tweets, photographs, etc. (population size = \\(N\\))\nSample: Subset of the population, ideally random and representative (sample size = \\(n\\))\nSample statistic \\(\\ne\\) population parameter, but if the sample is good, it can be a good estimate\nStatistical inference: Discipline that concerns itself with the development of procedures, methods, and theorems that allow us to extract meaning and information from data that has been generated by stochastic (random) process"
  },
  {
    "objectID": "slides/03-slr-sbi.html#recap-continued",
    "href": "slides/03-slr-sbi.html#recap-continued",
    "title": "SLR: Simulation-based inference",
    "section": "Recap Continued",
    "text": "Recap Continued\n\nEstimation: Use data to compute point estimate\n\nReport the estimate with a confidence interval, and the width of this interval depends on the variability of sample statistics from different samples from the population\n\nTesting: Conduct a hypothesis test\n\nAssume research question isn’t true (Null hypothesis)\nAsk what distribution of test statistic is if null is true\nAsk if your data would be unusual if under this null distribution\nP-value: Probability your data (or even stronger evidence) was obstained from null distribution"
  },
  {
    "objectID": "slides/01-welcome.html#meet-prof.-friedlander",
    "href": "slides/01-welcome.html#meet-prof.-friedlander",
    "title": "Welcome to MAT 212!",
    "section": "Meet Prof. Friedlander!",
    "text": "Meet Prof. Friedlander!\n\nEducation and career journey\n\nGrew up outside New York City\nBS in Math & Statistics from Rice University (Houston, TX)\nBusiness Analyst at Capital One (Plano, TX)\nMS and PhD in Statistics & Operations Research from UNC-Chapel Hill\nPostdoc in Population Genetics at University of Chicago\nAssistant Professor of Math at St. Norbert College (Green Bay, WI)\n\nWork focuses on statistics education, queuing theory, and population genetics\nBig sports fan: NY Knicks, Giants, Rangers, Yankees, UNC Tarheels\nDad of three cute dogs: Allie, Miriam, Tony"
  },
  {
    "objectID": "slides/01-welcome.html#meet-prof.-friedlander-1",
    "href": "slides/01-welcome.html#meet-prof.-friedlander-1",
    "title": "Welcome to MAT 212!",
    "section": "Meet Prof. Friedlander!",
    "text": "Meet Prof. Friedlander!"
  },
  {
    "objectID": "slides/01-welcome.html#tell-me-about-yourself",
    "href": "slides/01-welcome.html#tell-me-about-yourself",
    "title": "Welcome to MAT 212!",
    "section": "Tell me about yourself",
    "text": "Tell me about yourself\nNavigate to Teams.\nIn a private chat to me answer the following questions:\n\nWhat would you like me to call you?\nWhy are you taking this class?\nHow are you feeling about taking this class? Be honest… you won’t hurt my feelings.\nHow much experience with R, if any, do you have?\nIs there anything else you would like me to know about you? E.g. athlete, preferred pronouns, accommodations, etc…\n\nIn the class discussion forum, please recommend at least one song for the class playlist… Feel free to suggest as many songs as you like.\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/01-welcome.html#what-is-a-model",
    "href": "slides/01-welcome.html#what-is-a-model",
    "title": "Welcome to MAT 212!",
    "section": "What is a model?",
    "text": "What is a model?\n\nDATA = MODEL + ERROR\nDATA = PATTERN + DEPARTURES FROM PATTERN\n\nHow do we identify the actual pattern?\n\nGOAL: Find a model for a relationship between a response/outcome/target variable \\(Y\\) and one (or more) explanatory/predictor variables (\\(X_1,\\ldots,X_k\\))\nModels are a simplified but tractable version of reality\n\n\nWhat are response and explanatory variables?"
  },
  {
    "objectID": "slides/01-welcome.html#geoge-e.-p.-box",
    "href": "slides/01-welcome.html#geoge-e.-p.-box",
    "title": "Welcome to MAT 212!",
    "section": "Geoge E. P. Box",
    "text": "Geoge E. P. Box\n\n\n\nFrom Wikipedia: British statistician, who worked in the areas of quality control, time-series analysis, design of experiments, and Bayesian inference. He has been called “one of the great statistical minds of the 20th century”.\n“all models are wrong, but some are useful”"
  },
  {
    "objectID": "slides/01-welcome.html#why-build-a-model",
    "href": "slides/01-welcome.html#why-build-a-model",
    "title": "Welcome to MAT 212!",
    "section": "Why build a model?",
    "text": "Why build a model?\n\nMaking predictions\nUnderstanding relationships\nAssessing differences"
  },
  {
    "objectID": "slides/01-welcome.html#what-is-regression-analysis",
    "href": "slides/01-welcome.html#what-is-regression-analysis",
    "title": "Welcome to MAT 212!",
    "section": "What is regression analysis?",
    "text": "What is regression analysis?\n\n\n“In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships among variables. It includes many techniques for modeling and analyzing several variables, when the focus is on the relationship between a dependent variable and one or more independent variables (or ‘predictors’). More specifically, regression analysis helps one understand how the typical value of the dependent variable (or ‘criterion variable’) changes when any one of the independent variables is varied, while the other independent variables are held fixed.”\n\nSource: Wikipedia (previous definition)\n\nNote: I don’t really like the terms “independent” and “dependent” variables"
  },
  {
    "objectID": "slides/01-welcome.html#examples-of-regression-in-practice",
    "href": "slides/01-welcome.html#examples-of-regression-in-practice",
    "title": "Welcome to MAT 212!",
    "section": "Examples of regression in practice",
    "text": "Examples of regression in practice\n\nNew Yorkers Will Pay $56 A Month To Trim A Minute Off Their Commute\nHow FiveThirtyEight’s 2020 Presidential Forecast Works — And What’s Different Because Of COVID-19\nEffect of Forensic Evidence on Criminal Justice Case Processing\nWhy it’s so freaking hard to make a good COVID-19 model (from March 2020)"
  },
  {
    "objectID": "slides/01-welcome.html#course-faq",
    "href": "slides/01-welcome.html#course-faq",
    "title": "Welcome to MAT 212!",
    "section": "Course FAQ",
    "text": "Course FAQ\nQ - What background is assumed for the course?\nA - Introductory statistics or previous experience with mathematics at a level that would allow you to learn intro stats concepts relatively easily\n\nQ - Will we be doing computing?\nA - Yes. We will use the computing language R for analysis and Quarto for writing up results.\n\n\nQ - Am I expected to have experience using any of these tools?\nA - No. I do not expect you to have any exposure to R and certainly not Quarto.\n\n\nQ - Will we learn the mathematical theory of regression?\nA - Yes and No. The course is primarily focused on application; however, we will discuss some of the mathematics of simple linear regression.\n\n\nQ - How much time should I be spending on this class?\nA - This is a 3-credit class taught over 15 days which meets for 2.5 hours per day. That means that you should be spending approximately 9 hours per day working on this course (i.e. 6.5 hours outside of class)"
  },
  {
    "objectID": "slides/01-welcome.html#course-learning-objectives",
    "href": "slides/01-welcome.html#course-learning-objectives",
    "title": "Welcome to MAT 212!",
    "section": "Course learning objectives",
    "text": "Course learning objectives\nBy the end of the semester, you will be able to…\n\nanalyze real-world data to answer questions about multivariable relationships.\nuse R to fit and evaluate linear and logistic regression models.\nassess whether a proposed model is appropriate and describe its limitations.\nuse Quarto to write reproducible reports.\neffectively communicate statistical results through writing and oral presentations."
  },
  {
    "objectID": "slides/01-welcome.html#course-topics",
    "href": "slides/01-welcome.html#course-topics",
    "title": "Welcome to MAT 212!",
    "section": "Course Topics",
    "text": "Course Topics\n\nWhat is a quantitative and what is a categorical variable?\n\n\n\n\n\nChapter\nResponse\nPredictor/Explanatory\n\n\n\n\n1-2\nQuantitative\nSingle Quantitative\n\n\n3-4\nQuantitative\nMultiple Quantitative\n\n\n5\nQuantitative\nSingle Categorical\n\n\n6-8\nQuantitative\nMultiple Categorical\n\n\n9\nCategorical\nSingle Quant/Cat\n\n\n10\nCategorical\nMultiple Quant/Cat\n\n\n11\nBoth\nBoth"
  },
  {
    "objectID": "slides/01-welcome.html#course-toolkit",
    "href": "slides/01-welcome.html#course-toolkit",
    "title": "Welcome to MAT 212!",
    "section": "Course toolkit",
    "text": "Course toolkit\n\nCourse website: mat212wi25.netlify.app\n\nCentral hub for the course!\nTour of the website\n\nCanvas: cofi.instructure.com\n\nGradebook\nAssignment submissions\nAnnouncements\n\nTeams\n\nActivities\nDiscussions\nHomework Help"
  },
  {
    "objectID": "slides/01-welcome.html#computing-toolkit",
    "href": "slides/01-welcome.html#computing-toolkit",
    "title": "Welcome to MAT 212!",
    "section": "Computing toolkit",
    "text": "Computing toolkit\n\n\n\n\n\n\n\n\n\nAll analyses using R, a statistical programming language\nWrite reproducible reports in Quarto\nAccess RStudio through College of Idaho posit Workbench\n\nUse your College of Idaho email and password\n\n\n\n\n\nSign into RStudio!\n\n\n\n\n−+\n03:00"
  },
  {
    "objectID": "slides/01-welcome.html#prepare-participate-practice-perform",
    "href": "slides/01-welcome.html#prepare-participate-practice-perform",
    "title": "Welcome to MAT 212!",
    "section": "Prepare, Participate, Practice, Perform",
    "text": "Prepare, Participate, Practice, Perform\n\n\nPrepare: Introduce new content and prepare for lectures by completing the readings (and sometimes watching videos)\nParticipate: Attend and actively participate in lectures, office hours, team meetings\nPractice: Practice applying statistical concepts and computing with application exercises during lecture, graded for completion\nPerform: Put together what you’ve learned to analyze real-world data\n\nHomework assignments (individual)\nTwo oral exams\nFinal group projects"
  },
  {
    "objectID": "slides/01-welcome.html#grading",
    "href": "slides/01-welcome.html#grading",
    "title": "Welcome to MAT 212!",
    "section": "Grading",
    "text": "Grading\n\n\n\n\nCategory\nPercentage\n\n\n\n\nHomework\n25%\n\n\nFinal Project\n25%\n\n\nExam 01\n20%\n\n\nExam 02\n20%\n\n\nApplication Exercises\n10%\n\n\n\nNote: You must receive at least a 60% on your two exams to pass the course.\nSee the syllabus for details on how the final letter grade will be calculated."
  },
  {
    "objectID": "slides/01-welcome.html#support",
    "href": "slides/01-welcome.html#support",
    "title": "Welcome to MAT 212!",
    "section": "Support",
    "text": "Support\n\nAttend office hours to meet with Prof. Friedlander (Boone 126B)\n\nMTWRF : 11:00am-12:00pm\n\nAsk and answer questions on course discussion forum\nUse email for questions regarding personal matters and/or grades\nSee the Course Support page for more details"
  },
  {
    "objectID": "slides/01-welcome.html#late-homework",
    "href": "slides/01-welcome.html#late-homework",
    "title": "Welcome to MAT 212!",
    "section": "Late Homework",
    "text": "Late Homework\n\nThere will be a 5% deduction for each 24-hour period the assignment is late for the first two days. After 2 days, students will receive a 30% reduction. No homework will be accepted after it is returned to the class."
  },
  {
    "objectID": "slides/01-welcome.html#late-application-exercises",
    "href": "slides/01-welcome.html#late-application-exercises",
    "title": "Welcome to MAT 212!",
    "section": "Late Application Exercises",
    "text": "Late Application Exercises\nAEs are due the day after the class they are assigned. No late work is accepted for application exercises, since these are designed as in-class activities to help you prepare for homework."
  },
  {
    "objectID": "slides/01-welcome.html#school-sponsored-events",
    "href": "slides/01-welcome.html#school-sponsored-events",
    "title": "Welcome to MAT 212!",
    "section": "School-Sponsored Events",
    "text": "School-Sponsored Events\nIf an application exercise or project must be missed due to a school-sponsored event, you must let me know at least a week ahead of time so that we can schedule a time for you to make up the work before you leave. If you must miss a exam or a project presentation due to illness, you must let me know before class that day so that we can schedule a time for you to make it up. Failure to adhere to this policy will result in a 35% penalty on the corresponding assignment."
  },
  {
    "objectID": "slides/01-welcome.html#academic-integrity",
    "href": "slides/01-welcome.html#academic-integrity",
    "title": "Welcome to MAT 212!",
    "section": "Academic integrity",
    "text": "Academic integrity\n\nThe College of Idaho maintains that academic honesty and integrity are essential values in the educational process. Operating under an Honor Code philosophy, the College expects conduct rooted in honesty, integrity, and understanding, allowing members of a diverse student body to live together and interact and learn from one another in ways that protect both personal freedom and community standards. Violations of academic honesty are addressed primarily by the instructor and may be referred to the Student Judicial Board.\n\nBy participating in this course, you are agreeing that all your work and conduct will be in accordance with the College of Idaho Honor Code."
  },
  {
    "objectID": "slides/01-welcome.html#collaboration-sharing-code",
    "href": "slides/01-welcome.html#collaboration-sharing-code",
    "title": "Welcome to MAT 212!",
    "section": "Collaboration & sharing code",
    "text": "Collaboration & sharing code\n\nI have policies!\nLet’s read about them in the Academic honesty section of the syllabus"
  },
  {
    "objectID": "slides/01-welcome.html#use-of-artificial-intelligence-ai",
    "href": "slides/01-welcome.html#use-of-artificial-intelligence-ai",
    "title": "Welcome to MAT 212!",
    "section": "Use of artificial intelligence (AI)",
    "text": "Use of artificial intelligence (AI)\n\nYou should treat AI tools, such as ChatGPT, the same as other online resources.\nThere are two guiding principles that govern how you can use AI in this course:1\n\n(1) Cognitive dimension: Working with AI should not reduce your ability to think clearly. We will practice using AI to facilitate—rather than hinder—learning.\n(2) Ethical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity.\n\n\nThese guiding principles are based on Course Policies related to ChatGPT and other AI Tools developed by Joel Gladd, Ph.D.↩︎"
  },
  {
    "objectID": "slides/01-welcome.html#use-of-artificial-intelligence-ai-1",
    "href": "slides/01-welcome.html#use-of-artificial-intelligence-ai-1",
    "title": "Welcome to MAT 212!",
    "section": "Use of artificial intelligence (AI)",
    "text": "Use of artificial intelligence (AI)\n✅ AI tools for code: You may make use of the technology for coding examples on assignments or to fix bugs in your code. However, if you do so, you must explicitly cite where you obtained the code and AI should serve as a learning aid, not a replacement for thinking.\n❌ No AI tools for narrative: Unless instructed otherwise, AI is not permitted for writing narrative on assignments.\n\n\n\n\n\n\nImportant\n\n\nIn general, you may use AI as a resource as you complete assignments but not to answer the exercises for you. You are ultimately responsible for the work you turn in; it should reflect your understanding of the course content. Any code or content from your homework which was not covered in class or cited, is eligible to be on your exams."
  },
  {
    "objectID": "slides/01-welcome.html#five-tips-for-success",
    "href": "slides/01-welcome.html#five-tips-for-success",
    "title": "Welcome to MAT 212!",
    "section": "Five tips for success",
    "text": "Five tips for success\n\nComplete all the preparation work (readings and videos) before class.\nAsk questions.\nStart your work (homework and projects) early!\nDon’t procrastinate and don’t let a day pass by with lingering questions.\nStay up-to-date on announcements on Canvas and sent via email."
  },
  {
    "objectID": "slides/01-welcome.html#what-should-i-know-about-this-class",
    "href": "slides/01-welcome.html#what-should-i-know-about-this-class",
    "title": "Welcome to MAT 212!",
    "section": "What should I know about this class",
    "text": "What should I know about this class\n\nThis class is a lot of work\nSteep learning curve in the beginning… stick with it! I promise you can do it!\nMore writing than you probably expected… it is not enough for Dr. F to know what you mean to say… you must say that! Dr F. always asks: “If this student said this in a job interview, would they get hired?”\nIn statistics, there is rarely one RIGHT answer… it’s all about extracting information from data to make arguments"
  },
  {
    "objectID": "slides/01-welcome.html#dr.-fs-pet-peeves",
    "href": "slides/01-welcome.html#dr.-fs-pet-peeves",
    "title": "Welcome to MAT 212!",
    "section": "Dr. F’s Pet Peeves",
    "text": "Dr. F’s Pet Peeves\n\nShowing up late to class\nUsing Generative AI to do your thinking for you"
  },
  {
    "objectID": "slides/01-welcome.html#bug-bounty",
    "href": "slides/01-welcome.html#bug-bounty",
    "title": "Welcome to MAT 212!",
    "section": "Bug Bounty",
    "text": "Bug Bounty\n\nIf you find a typo on the website, slides, homework, activities, etc (e.g. broken link, typo, etc…) and you are the first person to point it out, you will receive a bonus point toward your HW grade. However, you may not exceed 100%.\nIf you message me about this during class, you will not receive your extra credit."
  },
  {
    "objectID": "slides/01-welcome.html#topics",
    "href": "slides/01-welcome.html#topics",
    "title": "Welcome to MAT 212!",
    "section": "Topics",
    "text": "Topics\n\nData analysis life cycle\nReproducible data analysis"
  },
  {
    "objectID": "slides/01-welcome.html#section",
    "href": "slides/01-welcome.html#section",
    "title": "Welcome to MAT 212!",
    "section": "",
    "text": "Source: R for Data Science with additions from The Art of Statistics: How to Learn from Data."
  },
  {
    "objectID": "slides/01-welcome.html#section-1",
    "href": "slides/01-welcome.html#section-1",
    "title": "Welcome to MAT 212!",
    "section": "",
    "text": "Source: R for Data Science"
  },
  {
    "objectID": "slides/01-welcome.html#reproducibility-checklist",
    "href": "slides/01-welcome.html#reproducibility-checklist",
    "title": "Welcome to MAT 212!",
    "section": "Reproducibility checklist",
    "text": "Reproducibility checklist\n\nWhat does it mean for an analysis to be reproducible?\n\n\nNear term goals:\n✔️ Can the tables and figures be exactly reproduced from the code and data?\n✔️ Does the code actually do what you think it does?\n✔️ In addition to what was done, is it clear why it was done?\n\n\nLong term goals:\n✔️ Can the code be used for other data?\n✔️ Can you extend the code to do other things?"
  },
  {
    "objectID": "slides/01-welcome.html#why-is-reproducibility-important",
    "href": "slides/01-welcome.html#why-is-reproducibility-important",
    "title": "Welcome to MAT 212!",
    "section": "Why is reproducibility important?",
    "text": "Why is reproducibility important?\n\nResults produced are more reliable and trustworthy (Ostblom and Timbers 2022)\nFacilitates more effective collaboration (Ostblom and Timbers 2022)\nContributing to science, which builds and organizes knowledge in terms of testable hypotheses (Alexander 2023)\nPossible to identify and correct errors or biases in the analysis process (Alexander 2023)"
  },
  {
    "objectID": "slides/01-welcome.html#when-things-go-wrong",
    "href": "slides/01-welcome.html#when-things-go-wrong",
    "title": "Welcome to MAT 212!",
    "section": "When things go wrong",
    "text": "When things go wrong\n\n\n\nReproducibility error\nConsequence\nSource(s)\n\n\n\n\nLimitations in Excel data formats\nLoss of 16,000 COVID case records in the UK\n(Kelion 2020)\n\n\nAutomatic formatting in Excel\nImportant genes disregarded in scientific studies\n(Ziemann, Eren, and El-Osta 2016)\n\n\nDeletion of a cell caused rows to shift\nMix-up of which patient group received the treatment\n(Wallensteen et al. 2018)\n\n\nUsing binary instead of explanatory labels\nMix-up of the intervention with the control group\n(Aboumatar and Wise 2019)\n\n\nUsing the same notation for missing data and zero values\nPaper retraction\n(Whitehouse et al. 2021)\n\n\nIncorrectly copying data in a spreadsheet\nDelay in the opening of a hospital\n(Picken 2020)\n\n\n\nSource: Ostblom and Timbers (2022)"
  },
  {
    "objectID": "slides/01-welcome.html#toolkit",
    "href": "slides/01-welcome.html#toolkit",
    "title": "Welcome to MAT 212!",
    "section": "Toolkit",
    "text": "Toolkit\n\nScriptability \\(\\rightarrow\\) R\nLiterate programming (code, narrative, output in one place) \\(\\rightarrow\\) Quarto\nVersion control \\(\\rightarrow\\) Git / GitHub (Beyond the scope of this course)"
  },
  {
    "objectID": "slides/01-welcome.html#r-and-rstudio",
    "href": "slides/01-welcome.html#r-and-rstudio",
    "title": "Welcome to MAT 212!",
    "section": "R and RStudio",
    "text": "R and RStudio\n\nR is a statistical programming language\nRStudio is a convenient interface for R (an integrated development environment, IDE)\n\n\nSource: Statistical Inference via Data Science"
  },
  {
    "objectID": "slides/01-welcome.html#rstudio-ide",
    "href": "slides/01-welcome.html#rstudio-ide",
    "title": "Welcome to MAT 212!",
    "section": "RStudio IDE",
    "text": "RStudio IDE"
  },
  {
    "objectID": "slides/01-welcome.html#quarto",
    "href": "slides/01-welcome.html#quarto",
    "title": "Welcome to MAT 212!",
    "section": "Quarto",
    "text": "Quarto\n\nFully reproducible reports – the analysis is run from the beginning each time you render\nCode goes in chunks and narrative goes outside of chunks\nVisual editor to make document editing experience similar to a word processor (Google docs, Word, Pages, etc.)"
  },
  {
    "objectID": "slides/01-welcome.html#quarto-1",
    "href": "slides/01-welcome.html#quarto-1",
    "title": "Welcome to MAT 212!",
    "section": "Quarto",
    "text": "Quarto"
  },
  {
    "objectID": "slides/01-welcome.html#how-will-we-use-quarto",
    "href": "slides/01-welcome.html#how-will-we-use-quarto",
    "title": "Welcome to MAT 212!",
    "section": "How will we use Quarto?",
    "text": "How will we use Quarto?\n\nEvery application exercise and assignment is written in a Quarto document\nYou’ll have a template Quarto document to start with\nThe amount of scaffolding in the template will decrease over the semester"
  },
  {
    "objectID": "slides/01-welcome.html#group-work-roles",
    "href": "slides/01-welcome.html#group-work-roles",
    "title": "Welcome to MAT 212!",
    "section": "Group Work Roles",
    "text": "Group Work Roles\nAny time we are working on AEs, I will randomly assign you to groups of two/three. Each person will have a role:\n\nDriver:\n\nWrites code and fills out Quarto template\nConnects to TV\nResponsible for submitting final work\n\nReporter:\n\nShares thoughts from group during class discussions\nShould have computer closed\n\nFacilitator:\n\nResponsible for looking stuff up (e.g. code, cheatsheets, etc.)"
  },
  {
    "objectID": "slides/01-welcome.html#carbohydrates-in-starbucks-food",
    "href": "slides/01-welcome.html#carbohydrates-in-starbucks-food",
    "title": "Welcome to MAT 212!",
    "section": "Carbohydrates in Starbucks food",
    "text": "Carbohydrates in Starbucks food\n\nStarbucks often displays the total calories in their food items but not the other nutritional information.\nCarbohydrates are a body’s main fuel source. The Dietary Guidelines for America recommend that carbohydrates make up 45% to 65% of total daily calories.1\nOur goal is to understand the relationship between the amount of carbohydrates and calories in Starbucks food items. We’d also like to assess if the relationship differs based on the type of food item (bakery, salad, sandwich, etc.)\n\nSource: Mayo Clinic"
  },
  {
    "objectID": "slides/01-welcome.html#starbucks-data",
    "href": "slides/01-welcome.html#starbucks-data",
    "title": "Welcome to MAT 212!",
    "section": "Starbucks data",
    "text": "Starbucks data\n\nlibrary(openintro)\n\nstarbucks &lt;- starbucks |&gt; \n  mutate(bakery = factor(if_else(type == \"bakery\", \"bakery\", \"non-bakery\")))\n\nglimpse(starbucks)\n\nRows: 77\nColumns: 8\n$ item     &lt;chr&gt; \"8-Grain Roll\", \"Apple Bran Muffin\", \"Apple Fritter\", \"Banana…\n$ calories &lt;int&gt; 350, 350, 420, 490, 130, 370, 460, 370, 310, 420, 380, 320, 3…\n$ fat      &lt;dbl&gt; 8, 9, 20, 19, 6, 14, 22, 14, 18, 25, 17, 12, 17, 21, 5, 18, 1…\n$ carb     &lt;int&gt; 67, 64, 59, 75, 17, 47, 61, 55, 32, 39, 51, 53, 34, 57, 52, 7…\n$ fiber    &lt;int&gt; 5, 7, 0, 4, 0, 5, 2, 0, 0, 0, 2, 3, 2, 2, 3, 3, 2, 3, 0, 2, 0…\n$ protein  &lt;int&gt; 10, 6, 5, 7, 0, 6, 7, 6, 5, 7, 4, 6, 5, 5, 12, 7, 8, 6, 0, 10…\n$ type     &lt;fct&gt; bakery, bakery, bakery, bakery, bakery, bakery, bakery, baker…\n$ bakery   &lt;fct&gt; bakery, bakery, bakery, bakery, bakery, bakery, bakery, baker…\n\n\n\nObservations: 77 Starbucks food items\nVariables:\n\ncarb: Total carbohydrates (in grams)\ncalories: Total calories\nbakery: bakery: bakery food item, non-bakery: other food type"
  },
  {
    "objectID": "slides/01-welcome.html#terminology",
    "href": "slides/01-welcome.html#terminology",
    "title": "Welcome to MAT 212!",
    "section": "Terminology",
    "text": "Terminology\n\ncarb is the response variable\n\nvariable whose variation we want to understand / variable we wish to predict\nalso known as outcome or target variable\n\n\n\n\ncalories, bakery are the explanatory variables\n\nvariables used to account for variation in the response\nalso known as predictor, or input variables"
  },
  {
    "objectID": "slides/01-welcome.html#univariate-exploratory-data-analysis",
    "href": "slides/01-welcome.html#univariate-exploratory-data-analysis",
    "title": "Welcome to MAT 212!",
    "section": "Univariate exploratory data analysis",
    "text": "Univariate exploratory data analysis"
  },
  {
    "objectID": "slides/01-welcome.html#bivariate-exploratory-data-analysis",
    "href": "slides/01-welcome.html#bivariate-exploratory-data-analysis",
    "title": "Welcome to MAT 212!",
    "section": "Bivariate exploratory data analysis",
    "text": "Bivariate exploratory data analysis"
  },
  {
    "objectID": "slides/01-welcome.html#function-between-response-and-predictors",
    "href": "slides/01-welcome.html#function-between-response-and-predictors",
    "title": "Welcome to MAT 212!",
    "section": "Function between response and predictors",
    "text": "Function between response and predictors\n\n\\[\\text{carb} = f(\\text{calories}, \\text{bakery}) + \\epsilon\\]\n\n\nGoal: Determine \\(f\\)\nHow do we determine \\(f\\)?\n\nMake an assumption about the functional form \\(f\\) (parametric model)\nUse the data to fit a model based on that form"
  },
  {
    "objectID": "slides/01-welcome.html#determine-f",
    "href": "slides/01-welcome.html#determine-f",
    "title": "Welcome to MAT 212!",
    "section": "Determine \\(f\\)",
    "text": "Determine \\(f\\)\n\nChoose the functional form of \\(f\\), i.e., choose the appropriate model given the response variable\n\n\nSuppose \\(f\\) takes the form of a linear model\n\\[y = f(\\mathbf{X}) = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p + \\epsilon\\]\n\n\n\nUse the data to fit (or train) the model, i.e, estimate the model parameters, \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\)"
  },
  {
    "objectID": "slides/01-welcome.html#carb-vs.-calories",
    "href": "slides/01-welcome.html#carb-vs.-calories",
    "title": "Welcome to MAT 212!",
    "section": "Carb vs. Calories",
    "text": "Carb vs. Calories\n\n\\[\\text{carb} = \\beta_0 + \\beta_1 ~\\text{calories} + \\epsilon\\]"
  },
  {
    "objectID": "slides/01-welcome.html#carb-vs.-calories-bakery",
    "href": "slides/01-welcome.html#carb-vs.-calories-bakery",
    "title": "Welcome to MAT 212!",
    "section": "Carb vs. Calories + Bakery",
    "text": "Carb vs. Calories + Bakery\n\n\\[\\text{carb} = \\beta_0 + \\beta_1 ~\\text{calories} + \\beta_2 ~\\text{bakery} + \\epsilon\\]"
  },
  {
    "objectID": "slides/01-welcome.html#carb-vs.-calories-bakery-with-interaction",
    "href": "slides/01-welcome.html#carb-vs.-calories-bakery-with-interaction",
    "title": "Welcome to MAT 212!",
    "section": "Carb vs. Calories + Bakery (with interaction)",
    "text": "Carb vs. Calories + Bakery (with interaction)\n\n\\[{\\small \\text{carb} = \\beta_0 + \\beta_1 ~\\text{calories} + \\beta_2 ~\\text{bakery} + \\beta_3 ~ \\text{calories} \\times \\text{bakery}  + \\epsilon}\\]"
  },
  {
    "objectID": "slides/01-welcome.html#example-palmers-penguins",
    "href": "slides/01-welcome.html#example-palmers-penguins",
    "title": "Welcome to MAT 212!",
    "section": "Example: Palmer’s Penguins",
    "text": "Example: Palmer’s Penguins\nSize measurements, clutch observations, and blood isotope ratios for adult foraging Adélie, Chinstrap, and Gentoo penguins observed on islands in the Palmer Archipelago near Palmer Station, Antarctica."
  },
  {
    "objectID": "slides/01-welcome.html#example-palmers-penguins-1",
    "href": "slides/01-welcome.html#example-palmers-penguins-1",
    "title": "Welcome to MAT 212!",
    "section": "Example: Palmer’s Penguins",
    "text": "Example: Palmer’s Penguins\n\n\nWhat does the relationship between Length and Depth look like?"
  },
  {
    "objectID": "slides/01-welcome.html#example-palmers-penguins-2",
    "href": "slides/01-welcome.html#example-palmers-penguins-2",
    "title": "Welcome to MAT 212!",
    "section": "Example: Palmer’s Penguins",
    "text": "Example: Palmer’s Penguins\n\n\nWhat does the relationship between Length and Depth look like now?"
  },
  {
    "objectID": "slides/01-welcome.html#simpsons-paradox-1",
    "href": "slides/01-welcome.html#simpsons-paradox-1",
    "title": "Welcome to MAT 212!",
    "section": "Simpson’s Paradox",
    "text": "Simpson’s Paradox\nSimpson’s Paradox is when there is a clear relationship between two variables but when you introduce a third variable that relationship disappears of reverses."
  },
  {
    "objectID": "slides/01-welcome.html#statistical-model-vs.-regression-equation",
    "href": "slides/01-welcome.html#statistical-model-vs.-regression-equation",
    "title": "Welcome to MAT 212!",
    "section": "Statistical model vs. regression equation",
    "text": "Statistical model vs. regression equation\nStatistical model (also known as data-generating model)\n\\[{\\small \\text{carb} = \\beta_0 + \\beta_1 ~\\text{calories} + \\beta_2 ~\\text{bakery} + \\beta_3 ~ \\text{calories} \\times \\text{bakery}  + \\epsilon}\\]\nModels the process for generating values of the response in the population (function + error)\n\n\nRegression equation\nEstimate of the function using the sample data\n\\[{\\small \\hat{\\text{carb}} = \\hat{\\beta}_0 + \\hat{\\beta}_1 ~\\text{calories} + \\hat{\\beta}_2 ~\\text{bakery} + \\hat{\\beta}_3 ~ \\text{calories} \\times \\text{bakery}}\\]"
  },
  {
    "objectID": "slides/01-welcome.html#why-fit-a-model",
    "href": "slides/01-welcome.html#why-fit-a-model",
    "title": "Welcome to MAT 212!",
    "section": "Why fit a model?",
    "text": "Why fit a model?\n\nPrediction: Expected value of the response variable for given values of the predictor variables\nInference: Conclusion about the relationship between the response and predictor variables\n\n\n\nWhat is an example of a prediction question that can be answered using the model of carb vs. calories and bakery?\nWhat is an example of an inference question that can be answered using the model of carb vs. calories and bakery?"
  },
  {
    "objectID": "slides/01-welcome.html#recap",
    "href": "slides/01-welcome.html#recap",
    "title": "Welcome to MAT 212!",
    "section": "Recap",
    "text": "Recap\n\nWe can use exploratory data analysis to describe the relationship between two variables\nWe make an assumption about the relationship between variables when doing linear regression\nThe two main objectives for fitting a linear regression model are (1) prediction and (2) inference"
  },
  {
    "objectID": "slides/01-welcome.html#for-monday",
    "href": "slides/01-welcome.html#for-monday",
    "title": "Welcome to MAT 212!",
    "section": "For Monday",
    "text": "For Monday\n\nRead the syllabus\nComplete and submit today’s AE\nFind you partner for the project"
  },
  {
    "objectID": "slides/01-welcome.html#references",
    "href": "slides/01-welcome.html#references",
    "title": "Welcome to MAT 212!",
    "section": "References",
    "text": "References\n\n\n\n\nAlexander, Rohan. 2023. “Telling Stories with Data,” June. https://doi.org/10.1201/9781003229407.\n\n\nOstblom, Joel, and Tiffany Timbers. 2022. “Opinionated Practices for Teaching Reproducibility: Motivation, Guided Instruction and Practice.” Journal of Statistics and Data Science Education 30 (3): 241–50. https://doi.org/10.1080/26939169.2022.2074922."
  },
  {
    "objectID": "project/project-tips.html",
    "href": "project/project-tips.html",
    "title": "Final project tips + resources",
    "section": "",
    "text": "Ask questions if any of the expectations are unclear.\nMake sure each team member is contributing, both in terms of quality and quantity of contribution (we will be reviewing commits from different team members).\nAll team members are expected to contribute equally to the completion of this assignment and group assessments will be given at its completion - anyone judged to not have sufficient contributed to the final product will have their grade penalized. While different teams members may have different backgrounds and abilities, it is the responsibility of every team member to understand how and why all code and approaches in the assignment works.\n\n\n\n\nInformative title and axis labels\nFlipped coordinates to make names readable\nArranged bars based on count\nCapitalized manufacturer names\nOptional: Added color - Use a coordinated color scheme throughout paper / presentation\nOptional: Applied a theme - Use same theme throughout paper / presentation\n\n\nmpg |&gt;\n  count(manufacturer) |&gt;\n  mutate(manufacturer = str_to_title(manufacturer)) |&gt;\n  gf_col(n ~ fct_reorder(manufacturer, n) , stat = \"identity\", fill = \"steelblue\") |&gt; \n  gf_refine(coord_flip()) |&gt; \n  gf_labs(x = \"Manufacturer\", \n       y = \"Count\", \n       title = \"The most common manufacturer is Dodge\") |&gt; \n  gf_refine(theme_bw() )\n\n\n\n\n\n\n\n\n\n\n\n\nUse the kable function from the knitr package to neatly output all tables and model output. This will also ensure all model coefficients are displayed.\n\nUse the digits argument to display only 3 or 4 significant digits.\nUse the caption argument to add captions to your table.\n\n\n\nmodel &lt;- lm(mpg ~ hp, data = mtcars)\ntidy(model) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n30.099\n1.634\n18.421\n0\n\n\nhp\n-0.068\n0.010\n-6.742\n0\n\n\n\n\n\n\n\n\n\nDon’t use variable names in your narrative! Use descriptive terms, so the reader understands your narrative without relying on the data dictionary.\n\n❌ There is a negative linear relationship between mpg and hp.\n✅ There is a negative linear relationship between a car’s fuel economy (in miles per gallon) and its horsepower.\n\nKnow your audience: Your report should be written for a general audience who has an understanding of statistics at the level of MAT 212.\nAvoid subject matter jargon: Don’t assume the audience knows all of the specific terminology related to your subject area. If you must use jargon, include a brief definition the first time you introduce a term.\nTell the “so what”: Your report and presentation should be more than a list of interpretations and technical definitions. Focus on what the results mean, i.e. what you want the audience to know about your topic after reading your report or viewing your presentation.\n\n❌ For every one unit increase in horsepower, we expect the miles per gallon to decrease by 0.068 units, on average.\n✅ If the priority is to have good fuel economy, then one should choose a car with lower horsepower. Based on our model, the fuel economy is expected to decrease, on average, by 0.68 miles per gallon for every 10 additional horsepower.\n\nTell a story: All visualizations, tables, model output, and narrative should tell a cohesive story!\nUse one voice: Though multiple people are writing the report, it should read as if it’s from a single author. At least one team member should read through the report before submission to ensure it reads like a cohesive document.\n\n\n\n\n\nExploring RStudio’s Visual Markdown Editor\nR for Data Science\nData Visualization: A Practical Introduction"
  },
  {
    "objectID": "project/project-tips.html#tips",
    "href": "project/project-tips.html#tips",
    "title": "Final project tips + resources",
    "section": "",
    "text": "Ask questions if any of the expectations are unclear.\nMake sure each team member is contributing, both in terms of quality and quantity of contribution (we will be reviewing commits from different team members).\nAll team members are expected to contribute equally to the completion of this assignment and group assessments will be given at its completion - anyone judged to not have sufficient contributed to the final product will have their grade penalized. While different teams members may have different backgrounds and abilities, it is the responsibility of every team member to understand how and why all code and approaches in the assignment works.\n\n\n\n\nInformative title and axis labels\nFlipped coordinates to make names readable\nArranged bars based on count\nCapitalized manufacturer names\nOptional: Added color - Use a coordinated color scheme throughout paper / presentation\nOptional: Applied a theme - Use same theme throughout paper / presentation\n\n\nmpg |&gt;\n  count(manufacturer) |&gt;\n  mutate(manufacturer = str_to_title(manufacturer)) |&gt;\n  gf_col(n ~ fct_reorder(manufacturer, n) , stat = \"identity\", fill = \"steelblue\") |&gt; \n  gf_refine(coord_flip()) |&gt; \n  gf_labs(x = \"Manufacturer\", \n       y = \"Count\", \n       title = \"The most common manufacturer is Dodge\") |&gt; \n  gf_refine(theme_bw() )\n\n\n\n\n\n\n\n\n\n\n\n\nUse the kable function from the knitr package to neatly output all tables and model output. This will also ensure all model coefficients are displayed.\n\nUse the digits argument to display only 3 or 4 significant digits.\nUse the caption argument to add captions to your table.\n\n\n\nmodel &lt;- lm(mpg ~ hp, data = mtcars)\ntidy(model) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n30.099\n1.634\n18.421\n0\n\n\nhp\n-0.068\n0.010\n-6.742\n0\n\n\n\n\n\n\n\n\n\nDon’t use variable names in your narrative! Use descriptive terms, so the reader understands your narrative without relying on the data dictionary.\n\n❌ There is a negative linear relationship between mpg and hp.\n✅ There is a negative linear relationship between a car’s fuel economy (in miles per gallon) and its horsepower.\n\nKnow your audience: Your report should be written for a general audience who has an understanding of statistics at the level of MAT 212.\nAvoid subject matter jargon: Don’t assume the audience knows all of the specific terminology related to your subject area. If you must use jargon, include a brief definition the first time you introduce a term.\nTell the “so what”: Your report and presentation should be more than a list of interpretations and technical definitions. Focus on what the results mean, i.e. what you want the audience to know about your topic after reading your report or viewing your presentation.\n\n❌ For every one unit increase in horsepower, we expect the miles per gallon to decrease by 0.068 units, on average.\n✅ If the priority is to have good fuel economy, then one should choose a car with lower horsepower. Based on our model, the fuel economy is expected to decrease, on average, by 0.68 miles per gallon for every 10 additional horsepower.\n\nTell a story: All visualizations, tables, model output, and narrative should tell a cohesive story!\nUse one voice: Though multiple people are writing the report, it should read as if it’s from a single author. At least one team member should read through the report before submission to ensure it reads like a cohesive document.\n\n\n\n\n\nExploring RStudio’s Visual Markdown Editor\nR for Data Science\nData Visualization: A Practical Introduction"
  },
  {
    "objectID": "project/project-review-guidelines.html",
    "href": "project/project-review-guidelines.html",
    "title": "Self and Peer Review",
    "section": "",
    "text": "Writing without revising is the literary equivalent of waltzing gaily out of the house in your underwear. ― Patricia Fuller"
  },
  {
    "objectID": "project/project-review-guidelines.html#anecdote-you-can-skip-if-you-want",
    "href": "project/project-review-guidelines.html#anecdote-you-can-skip-if-you-want",
    "title": "Self and Peer Review",
    "section": "Anecdote you can skip if you want",
    "text": "Anecdote you can skip if you want\nI’d like to take a minute and explain why I place so much emphasis on writing and communication. When I was a graduate student and I turned in my first draft of my very first paper to my advisor, it was bad… Like WAAAAAY BAD. My advisor was a wonderful and kind gentleman and he didn’t want to be mean so instead of just putting a big red X through everything that I wrote, he crossed out all of the word individually except for words like “the”, “and”, “but”, etc. as if he was just making small changes to the wording of what I had wrote. However, I got the message and started putting a lot more effort into my writing. It was hard. My undergraduate was at an engineering school where all of my homeworks had “right” or “wrong” answers. I think I only wrote three essays during my entire college career and it showed. This is why I place so much emphasis on you being able to write and communicate your thoughts in a way that (a) makes it easy for other people to understand and (b) makes it clear that you know what you’re talking about."
  },
  {
    "objectID": "project/project-review-guidelines.html#introduction",
    "href": "project/project-review-guidelines.html#introduction",
    "title": "Self and Peer Review",
    "section": "Introduction",
    "text": "Introduction\nThis document addresses Stages IV (Peer Review) and V (Project Revision) of your MAT-212 final project. The purpose of this exercise is to teach you how to improve your writing. The best way I have found to improve my writing is to just stop, reread, and mercilessly delete and rewrite. Entire paragraphs may move or disappear. The second purpose of this exercise is to show you (a little bit) how academic peer review works. In a professional publication, reviewers critique your writing/explanation and your method. Here, peer reviewers will focus just on critiquing of writing/explanation; your professor will comment on method. The process of writing and responding to comments in a separate document mimics how the review process works for professional publications."
  },
  {
    "objectID": "project/project-review-guidelines.html#stage-iv-peer-review",
    "href": "project/project-review-guidelines.html#stage-iv-peer-review",
    "title": "Self and Peer Review",
    "section": "Stage IV: Peer review",
    "text": "Stage IV: Peer review\n\nThe first draft of anything is shit. ― Ernest Hemingway\n\nEach person will have one manuscript to read. The purpose of the peer-review is to identify areas of the paper in which communication of statistical ideas needs to be improved. You may also offer suggestions for additional statistical analysis or ideas for future research, but it is not your goal to critique or criticize the analyses that have already been completed (the professor will provide guidance in this aspect). As a reviewer, it is NOT your job to correct spelling or grammatical mistakes. It is also NOT your job to offer precise rewording of sentences or phrases, just to point out where such rephrasing could occur. Your feedback will be provided to the author in a separate typed document. For your assigned manuscript, follow this procedure:\n\nYou will be assigned a manuscript to peer review through the Stage III assignment on Canvas.\nRead the manuscript, noting confusing things (underline, circle, write notes in the margins, etc.). On the first read-through, focus on getting the MAIN IDEA of the paper.\n\nRe-read the manuscript. Now specifically focus on the confusing parts, and see if you can figure them out. Maybe the author needs to explain things in a better order? or use less jargon? Is the writing style professional, concise, precise, and understandable? Write down these comments.\nRe-read, focusing on each section of the paper (the “Revising Guide” will probably help here). Does it include the necessary items listed in the Project Report Section of the project instruction? Are all things in the correct sections? Is all relevant information provided?\nAre figures/graphs/tables well-labeled and understandable?\nSummarize your comments in a typed document using the “Revising Guide” as a… well… guide. The most helpful feedback will consider the paper from two perspectives:\n\nGeneral comments about structure and style.\nSpecific comments about confusing sentences, missing info, etc. For specific comments, please include page number and line or paragraph number. Your comments may be phrased as questions (“What is the implication of XXX being statistically significant?”). Alternatively, you may offer suggestions (“Page 3, paragraph 4: It might clarify what you mean if you explain XXX before this point, rather than waiting until page 5. I found I had to flip between pages to understand your point.”) or general comments (“I don’t understand the first sentence of paragraph 5 on page 6.”).\n\nTurn in the typed document (“Revising Guide” with comments) to the Stage IV Canvas assignment and email it to the author. If you’ve written comments on the physical draft, give that back to the author as well."
  },
  {
    "objectID": "project/project-review-guidelines.html#stage-iv.v-self-review",
    "href": "project/project-review-guidelines.html#stage-iv.v-self-review",
    "title": "Self and Peer Review",
    "section": "Stage IV.V: Self-review",
    "text": "Stage IV.V: Self-review\n\nWhen your story is ready for rewrite, cut it to the bone. Get rid of every ounce of excess fat. This is going to hurt; revising a story down to the bare essentials is always a little like murdering children, but it must be done. —Stephen King\n\nIt will be helpful, after having a few days away from your paper, to review it using the “Revising Guide”. Try to look at it with fresh eyes. Read it aloud. Convince a friend/significant other/family member to listen to you read it. Encourage them to interrupt you when they don’t understand something or the writing is unclear. Interrupt yourself when you don’t understand something or the writing is unclear. Make notes to yourself about changes you should make: things you should add or delete from the paper, ways to make it more clear or more concise, etc. (You may not want to start making those changes until you read the professor and peer feedback.)"
  },
  {
    "objectID": "project/project-review-guidelines.html#stage-v-revise",
    "href": "project/project-review-guidelines.html#stage-v-revise",
    "title": "Self and Peer Review",
    "section": "Stage V: Revise",
    "text": "Stage V: Revise\n\nThe time to begin writing an article is when you have finished it to your satisfaction. By that time you begin to clearly and logically perceive what it is you really want to say. —Mark Twain\n\n\nRead carefully through the comment documents you received from peers and professor review. Consider also your self-review.\nMake thoughtful, liberal (and possibly painful) changes to your paper.\nMarvel at how much better (and probably shorter) your paper is.\nProvide responses to each comment document. For this, I recommend just adding your response under each point-by-point comment in each document. Make sure your responses are in a different color font than the reviewer’s, so I can tell the different between the reviewer’s comments and your responses. In some cases, the response may be as simple as “change was made.” For others, (particularly comments you choose to ignore), you may provide context or reasoning. (e.g. “I chose not to include a graph of blah versus blah because of space issues. However, I now comment on the relationship between blah and blah in paragraph 2 of page 4.”)\nSubmit the following on Canvas by the due date:\n\nthe final paper\nthe Annotated Appendix, as described in the Project Guidelines. (Even if you have not made any changes to the Appendix, upload it along with the paper.)\nthe peer feedback you received, with your response to comments included\nthe professor feedback you received, with your response to comments included"
  },
  {
    "objectID": "project/poster-rubric.html",
    "href": "project/poster-rubric.html",
    "title": "Final poster rubric",
    "section": "",
    "text": "Your poster will be evaluated as follows. Each student will be required to fill out the following rubric for every other poster. In addition, Dr. Friedlander will fill out the following rubric. 50% of your final poster grade will be based on Dr. Friedlander’s assessment and 50% will be based on your peer’s assessment. If your project was completed in pairs, Dr. Friedlander will assess each member separately.\nInstructions to reviewer: Use these criteria to rate the poster presentation on a scale of 1-5 (1=strongly disagree; 3=neutral; 5=strongly agree).\n\nPoster Organization\n\nPoster presentation is easy to follow. 1 2 3 4 5\nGraphics or other visuals on the poster enhance the oral description. 1 2 3 4 5\n\n\n\nPresentation Content\n\nQuestion or hypothesis being addressed is stated clearly, 1 2 3 4 5\nA model answering the research question is presented and analyzed used content from class. 1 2 3 4 5\nThe model selected and the inferential techniques applied are approrpiate for the problem being addressed. 1 2 3 4 5\nContent is easy to understand. 1 2 3 4 5\nThere is enough detail about methods for me to understand the results. 1 2 3 4 5\nResults are stated clearly. 1 2 3 4 5\nResults are interpreted based on previous literature and reasonable next steps/implications are proposed. 1 2 3 4 5\n\n\n\nQuestion and Answer\n\nPresenter’s response to questions demonstrated knowledge of subject matter. 1 2 3 4 5\nPresenter’s response to questions demonstrated knowledge of the statistical approach taken with the project. 1 2 3 4 5",
    "crumbs": [
      "Project",
      "Poster Rubric"
    ]
  },
  {
    "objectID": "prepare/prep-13.html",
    "href": "prepare/prep-13.html",
    "title": "Preparation for Model Comparison",
    "section": "",
    "text": "Stat2 Textbook: Chapter 3.2"
  },
  {
    "objectID": "prepare/prep-13.html#assigned-reading",
    "href": "prepare/prep-13.html#assigned-reading",
    "title": "Preparation for Model Comparison",
    "section": "",
    "text": "Stat2 Textbook: Chapter 3.2"
  },
  {
    "objectID": "prepare/prep-11.html",
    "href": "prepare/prep-11.html",
    "title": "Preparation for Multicollinearity",
    "section": "",
    "text": "Stat2 Textbook: Chapter 3.5"
  },
  {
    "objectID": "prepare/prep-11.html#assigned-reading",
    "href": "prepare/prep-11.html#assigned-reading",
    "title": "Preparation for Multicollinearity",
    "section": "",
    "text": "Stat2 Textbook: Chapter 3.5"
  },
  {
    "objectID": "prepare/prep-09.html",
    "href": "prepare/prep-09.html",
    "title": "Preparation for Logistic Regression",
    "section": "",
    "text": "Stat2 Textbook: Chapters 9 (Intro), 9.1, 9.2"
  },
  {
    "objectID": "prepare/prep-09.html#assigned-reading",
    "href": "prepare/prep-09.html#assigned-reading",
    "title": "Preparation for Logistic Regression",
    "section": "",
    "text": "Stat2 Textbook: Chapters 9 (Intro), 9.1, 9.2"
  },
  {
    "objectID": "prepare/prep-07.html",
    "href": "prepare/prep-07.html",
    "title": "Preparation for Multiple Linear Regression",
    "section": "",
    "text": "Stat2 Textbook: Chapter 3.1, 3.3, 3.4"
  },
  {
    "objectID": "prepare/prep-07.html#assigned-reading",
    "href": "prepare/prep-07.html#assigned-reading",
    "title": "Preparation for Multiple Linear Regression",
    "section": "",
    "text": "Stat2 Textbook: Chapter 3.1, 3.3, 3.4"
  },
  {
    "objectID": "prepare/prep-05.html",
    "href": "prepare/prep-05.html",
    "title": "Preparation for Conditions for Inference and Model Evaluation",
    "section": "",
    "text": "Stat2 Textbook: Chapter 1.3, 2.2, 2.3"
  },
  {
    "objectID": "prepare/prep-05.html#assigned-reading",
    "href": "prepare/prep-05.html#assigned-reading",
    "title": "Preparation for Conditions for Inference and Model Evaluation",
    "section": "",
    "text": "Stat2 Textbook: Chapter 1.3, 2.2, 2.3"
  },
  {
    "objectID": "prepare/prep-03.html",
    "href": "prepare/prep-03.html",
    "title": "Preparation for Simulation Based Inference",
    "section": "",
    "text": "Stat2: Topic 4.6 Randomization Test for Relationship\nStat2: Topic 4.7 Bootstrap for Regression"
  },
  {
    "objectID": "prepare/prep-03.html#assigned-reading",
    "href": "prepare/prep-03.html#assigned-reading",
    "title": "Preparation for Simulation Based Inference",
    "section": "",
    "text": "Stat2: Topic 4.6 Randomization Test for Relationship\nStat2: Topic 4.7 Bootstrap for Regression"
  },
  {
    "objectID": "prepare/prep-03.html#additional-resources-optional",
    "href": "prepare/prep-03.html#additional-resources-optional",
    "title": "Preparation for Simulation Based Inference",
    "section": "Additional resources (optional)",
    "text": "Additional resources (optional)\n📖 IMS, Sec 24.2: Randomization test for the slope\n📖 IMS, Sec 24.3: Bootstrap confidence interval for the slope"
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Useful links",
    "section": "",
    "text": "RStudio\n🔗 on CofI Posit Workbench\n\n\nDiscussion forum\n🔗 on Teams\n\n\nCanvas Course\n🔗 on Canvas",
    "crumbs": [
      "Useful links"
    ]
  },
  {
    "objectID": "hw/hw-09.html",
    "href": "hw/hw-09.html",
    "title": "HW 09: Multiple Logistic regression",
    "section": "",
    "text": "In this homework, you’ll continue analyzing data from an online Ipsos survey that was conducted for the FiveThirtyEight article “Why Many Americans Don’t Vote” using logistic regression for interpretation and prediction. You can read more about the polling design and respondents in the README of the GitHub repo for the data.\n\n\nBy the end of the assignment you will be able to…\n\nUse logistic regression to explore the relationship between a binary response variable and multiple predictor variables\nInterpret coefficients of logistic regression models\nUse statistics to help choose the best fit model\nConduct inference controlling the familywise error rate\nIncorporate survey weights into your analysis\n\n\n\n\n\nGo to RStudio and login with your College of Idaho Email and Password.\nMake a subfolder in your hw directory to store this homework.\nLog into Canvas, navigate to Homework 10, and upload the hw-10.qmd files into the folder your just made.\n\n\n\n\nThe following packages will be used for this assignment.\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(ggformula)\nlibrary(Stat2Data)\nlibrary(knitr)\n\n# add other packages as needed"
  },
  {
    "objectID": "hw/hw-09.html#learning-goals",
    "href": "hw/hw-09.html#learning-goals",
    "title": "HW 09: Multiple Logistic regression",
    "section": "",
    "text": "By the end of the assignment you will be able to…\n\nUse logistic regression to explore the relationship between a binary response variable and multiple predictor variables\nInterpret coefficients of logistic regression models\nUse statistics to help choose the best fit model\nConduct inference controlling the familywise error rate\nIncorporate survey weights into your analysis"
  },
  {
    "objectID": "hw/hw-09.html#getting-started",
    "href": "hw/hw-09.html#getting-started",
    "title": "HW 09: Multiple Logistic regression",
    "section": "",
    "text": "Go to RStudio and login with your College of Idaho Email and Password.\nMake a subfolder in your hw directory to store this homework.\nLog into Canvas, navigate to Homework 10, and upload the hw-10.qmd files into the folder your just made."
  },
  {
    "objectID": "hw/hw-09.html#packages",
    "href": "hw/hw-09.html#packages",
    "title": "HW 09: Multiple Logistic regression",
    "section": "",
    "text": "The following packages will be used for this assignment.\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(ggformula)\nlibrary(Stat2Data)\nlibrary(knitr)\n\n# add other packages as needed"
  },
  {
    "objectID": "hw/hw-09.html#exercise-1",
    "href": "hw/hw-09.html#exercise-1",
    "title": "HW 09: Multiple Logistic regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nLet’s prepare the data for analysis and modeling. Most of this can be copied from homework 7.\n\nCreate a new variable called frequent_voter that takes the value 1 if the voter_category is “always” and 0 otherwise.\nThe variable Q30 contains the respondent’s political party identification. Make a new variable, party_id, that simplifies Q30 into three categories: “Democrat”, “Republican”, “Independent/Neither”, The category “Independent/Neither” will also include respondents who did not answer the question.\nMake sure that educ, race, gender, income_catand party_id are all factors."
  },
  {
    "objectID": "hw/hw-09.html#exercise-2",
    "href": "hw/hw-09.html#exercise-2",
    "title": "HW 09: Multiple Logistic regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nLet’s start by fitting a model using the demographic factors - ppage, educ, race, gender, income_cat - to predict the odds a person is a frequent voter. Fit the model and display the model using 3 digits.\n\nConsider the relationship between ppage and one’s voting behavior. Interpret the coefficient of ppage in the context of the data in terms of the odds a person is a frequent voter."
  },
  {
    "objectID": "hw/hw-09.html#exercise-3",
    "href": "hw/hw-09.html#exercise-3",
    "title": "HW 09: Multiple Logistic regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nGenerate a 95% confidence interval for the coefficient of ppage. Interpret this interval in the context of the data in terms of the odds a person is a frequent voter."
  },
  {
    "objectID": "hw/hw-09.html#exercise-4",
    "href": "hw/hw-09.html#exercise-4",
    "title": "HW 09: Multiple Logistic regression",
    "section": "Exercise 4",
    "text": "Exercise 4\nShould party identification be added to the model? Use a drop-in-deviance test to determine if party identification should be added to the model fit in the previous exercise. Include the hypotheses in mathematical notation, the output from the test, and the conclusion in the context of the data."
  },
  {
    "objectID": "hw/hw-09.html#exercise-5",
    "href": "hw/hw-09.html#exercise-5",
    "title": "HW 09: Multiple Logistic regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nDisplay the model chosen from the previous exercise using 3 digits.\nThen use the model selected to write a short paragraph (2 - 5 sentences) describing the effect (or lack of effect) of political party on the odds a person is a frequent voter. The paragraph should include an indication of which levels (if any) are statistically significant along with specifics about the differences in the odds between the political parties, as appropriate. Discuss whether the choice of reference level is important to your findings."
  },
  {
    "objectID": "hw/hw-09.html#exercise-6",
    "href": "hw/hw-09.html#exercise-6",
    "title": "HW 09: Multiple Logistic regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nRead Topic 5.7 Multiple Comparisons and Fisher’s Least Significant Difference in the course textbook.\nIn the code below:\n\nInput your glm object (your model) in the first blank.\nInput the name of the variable for party identification in the second.\nChange eval: FALSE to eval:TRUE and run the code.\n\n\nlibrary(multcomp)\n\nglht(__________, linfct = mcp(_________ = \"Tukey\")) |&gt; \n  tidy(conf.int = TRUE, conf.level = 0.95)\n\nThe code you just executed generates estimates of the pairwise differences between all of the levels of the categorical variable you put in the second blank and adjusts your p-values and confidence intervals to control the familywise error rate. Note that despite that fact that it says “Tukey”, it is NOT using Tukey HSD to do this, but the interpretation of the p-values and confidence intervals is the same.\nDescribe what familywise error rate means in this context? If you are controlling for your familywise error rate, how would you expect your p-values and confidence intervals to change?"
  },
  {
    "objectID": "hw/hw-09.html#exercise-7",
    "href": "hw/hw-09.html#exercise-7",
    "title": "HW 09: Multiple Logistic regression",
    "section": "Exercise 7",
    "text": "Exercise 7\nRewrite the paragraph you wrote in Exercise 5, incorporating information from Exercise 6."
  },
  {
    "objectID": "hw/hw-09.html#exercise-8",
    "href": "hw/hw-09.html#exercise-8",
    "title": "HW 09: Multiple Logistic regression",
    "section": "Exercise 8",
    "text": "Exercise 8\nIn the article, the authors write\n\n“Nonvoters were more likely to have lower incomes; to be young; to have lower levels of education; and to say they don’t belong to either political party, which are all traits that square with what we know about people less likely to engage with the political system.”\n\nConsider the model you selected in Exercise 4. Is it consistent with this statement? Briefly explain why or why not."
  },
  {
    "objectID": "hw/hw-09.html#exercise-9",
    "href": "hw/hw-09.html#exercise-9",
    "title": "HW 09: Multiple Logistic regression",
    "section": "Exercise 9",
    "text": "Exercise 9\nYou have been tasked by a local political organization to identify adults in the community who are frequent voters. These adults will receive targeted political mailings that will be different from the mailings sent to adults who are not frequent voters. You will use the model selected in Exercise 4 to identify the frequent voters."
  },
  {
    "objectID": "hw/hw-09.html#exercise-10",
    "href": "hw/hw-09.html#exercise-10",
    "title": "HW 09: Multiple Logistic regression",
    "section": "Exercise 10",
    "text": "Exercise 10\nThis survey actually uses something called weighting to produce more accurate estimates. If we want to be able to generalize our results to the population of interest we HAVE to use these weights.\n\nWatch this video on weighting survey data.\nRead the README on the github repo for the data.\nFill in the blanks in the code below to refit our glm model while accounted for the weights in the data.\nChange eval: FALSE to eval: TRUE and add any code you want to look at the model. tidy and kable should work just fine.\n\n\nlibrary(survey)\n\n# create survey object\nsurvey_538 &lt;- svydesign(data = [data_set], ids = ~[column with IDs], weights = ~[column describing weights])\n\n# fit logistic regression model\nweighted_model &lt;- svyglm([insert formula], design= [survey object], family = \"binomial\")\n\nWrite a paragraph including the following:\n\nWhat groups of people were oversampled and does that mean they get larger or smaller weights in the data?\nWhy weighting was used?\nTo what extent did the results from your analysis considering the weights differ from your results earlier in this homework."
  },
  {
    "objectID": "hw/hw-09.html#footnotes",
    "href": "hw/hw-09.html#footnotes",
    "title": "HW 09: Multiple Logistic regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Grammar & Writing” grade is decided based on your grammar and writing. This is typically decided by choosing one of the questions and assessing the writing.↩︎\nThe “Workflow & formatting” grade is to assess the reproducible workflow and document format. This includes having a neatly organized document with readable code and your name and the date in the YAML.↩︎"
  },
  {
    "objectID": "hw/hw-07.html",
    "href": "hw/hw-07.html",
    "title": "HW 07: Intro to Logistic regression",
    "section": "",
    "text": "In this homework, you’ll analyze data from an online Ipsos survey that was conducted for the FiveThirtyEight article “Why Many Americans Don’t Vote” using logistic regression for interpretation and prediction. You can read more about the polling design and respondents in the README of the GitHub repo for the data.\n\n\nBy the end of the assignment you will be able to…\n\nUse logistic regression to explore the relationship between a binary response variable and multiple predictor variables\nConduct exploratory data analysis for logistic regression\nInterpret coefficients of logistic regression model\nUse the logistic regression model for prediction\n\n\n\n\n\nGo to RStudio and login with your College of Idaho Email and Password.\nMake a subfolder in your hw directory to store this homework.\nLog into Canvas, navigate to Homework 7, and upload the hw-07.qmd files into the folder your just made.\n\n\n\n\nThe following packages will be used for this assignment.\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(ggformula)\nlibrary(Stat2Data)\nlibrary(knitr)\n\n# add other packages as needed"
  },
  {
    "objectID": "hw/hw-07.html#learning-goals",
    "href": "hw/hw-07.html#learning-goals",
    "title": "HW 07: Intro to Logistic regression",
    "section": "",
    "text": "By the end of the assignment you will be able to…\n\nUse logistic regression to explore the relationship between a binary response variable and multiple predictor variables\nConduct exploratory data analysis for logistic regression\nInterpret coefficients of logistic regression model\nUse the logistic regression model for prediction"
  },
  {
    "objectID": "hw/hw-07.html#getting-started",
    "href": "hw/hw-07.html#getting-started",
    "title": "HW 07: Intro to Logistic regression",
    "section": "",
    "text": "Go to RStudio and login with your College of Idaho Email and Password.\nMake a subfolder in your hw directory to store this homework.\nLog into Canvas, navigate to Homework 7, and upload the hw-07.qmd files into the folder your just made."
  },
  {
    "objectID": "hw/hw-07.html#packages",
    "href": "hw/hw-07.html#packages",
    "title": "HW 07: Intro to Logistic regression",
    "section": "",
    "text": "The following packages will be used for this assignment.\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(ggformula)\nlibrary(Stat2Data)\nlibrary(knitr)\n\n# add other packages as needed"
  },
  {
    "objectID": "hw/hw-07.html#exercise-1",
    "href": "hw/hw-07.html#exercise-1",
    "title": "HW 07: Intro to Logistic regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nWhy do you think the authors chose to only include data from people who were eligible to vote for at least four election cycles?"
  },
  {
    "objectID": "hw/hw-07.html#exercise-2",
    "href": "hw/hw-07.html#exercise-2",
    "title": "HW 07: Intro to Logistic regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nLet’s prepare the data for analysis and modeling.\n\nCreate a new variable called frequent_voter that takes the value 1 if the voter_category is “always” and 0 otherwise.\nMake a table of the distribution of frequent_voter.\nWhat percentage of the respondents in the data say they voted “in all or all-but-one of the elections they were eligible in”?"
  },
  {
    "objectID": "hw/hw-07.html#exercise-3",
    "href": "hw/hw-07.html#exercise-3",
    "title": "HW 07: Intro to Logistic regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nThe variable Q30 contains the respondent’s political party identification. Make a new variable, party_id, that simplifies Q30 into three categories: “Democrat”, “Republican”, “Independent/Neither”. The category “Independent/Neither” will also include respondents who did not answer the question. Make party_id a factor and relevel it so that it is consistent with the ordering of the responses in Question 30 of the survey.\n\nMake a plot of the distribution of party_id.\nWhich category of party_id occurs most frequently in this data set?"
  },
  {
    "objectID": "hw/hw-07.html#exercise-4",
    "href": "hw/hw-07.html#exercise-4",
    "title": "HW 07: Intro to Logistic regression",
    "section": "Exercise 4",
    "text": "Exercise 4\nIn the FiveThirtyEight article, the authors include visualizations of the relationship between the voter category and demographic variables such as race, age, education, etc.\n\nMake a segmented barplot (also known as a standardized barplot) displaying the distribution of frequent_voter for each category of party_id. Make the plot such that the proportions (instead of counts) are displayed:\n\nUse the function gf_props.\nPut party_id on the x-axis.\nAttach voter_category to the fill aesthetic (use a tilde).\nAdd the argument position = \"fill\"\n\nUse the plot to describe the relationship between these two variables.\n\n\n\n\n\n\n\nTip\n\n\n\nSee the plots of demographic information by voting history in the FiveThirtyEight article for examples of segmented bar plots."
  },
  {
    "objectID": "hw/hw-07.html#exercise-5",
    "href": "hw/hw-07.html#exercise-5",
    "title": "HW 07: Intro to Logistic regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nConsider the plot from the previous question. A logistic regression model predicting frequent_voter from party_id is visible in this plot. Explain what that means. Why should the response variable be attached the fill aesthetic instead of the explanatory variable."
  },
  {
    "objectID": "hw/hw-07.html#exercise-6",
    "href": "hw/hw-07.html#exercise-6",
    "title": "HW 07: Intro to Logistic regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nCompute the empirical log-odds that someone is a frequent_voter based on their party_id."
  },
  {
    "objectID": "hw/hw-07.html#exercise-7",
    "href": "hw/hw-07.html#exercise-7",
    "title": "HW 07: Intro to Logistic regression",
    "section": "Exercise 7",
    "text": "Exercise 7\nFit a model using party_id to predict the probability that a person is a frequent voter. Neatly display the model using the tidy function. What is the predicted probability of voting for each of the three categories in party_id?"
  },
  {
    "objectID": "hw/hw-07.html#exercise-8",
    "href": "hw/hw-07.html#exercise-8",
    "title": "HW 07: Intro to Logistic regression",
    "section": "Exercise 8",
    "text": "Exercise 8\nFit a model using ppage to predict the probability that a person is a frequent voter. Interpret the coefficient of ppage in the context of the data in terms of the log-odds a person is a frequent voter."
  },
  {
    "objectID": "hw/hw-07.html#exercise-9",
    "href": "hw/hw-07.html#exercise-9",
    "title": "HW 07: Intro to Logistic regression",
    "section": "Exercise 9",
    "text": "Exercise 9\nDetermine whether you think the three conditions for a logistic regression are met."
  },
  {
    "objectID": "hw/hw-07.html#exercise-10",
    "href": "hw/hw-07.html#exercise-10",
    "title": "HW 07: Intro to Logistic regression",
    "section": "Exercise 10",
    "text": "Exercise 10\n\nUse your first model to predict the probability that an independent will vote.\nUse your second model to predict the probability that someone your age will vote."
  },
  {
    "objectID": "hw/hw-07.html#footnotes",
    "href": "hw/hw-07.html#footnotes",
    "title": "HW 07: Intro to Logistic regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Grammar & Writing” grade is decided based on your grammar and writing. This is typically decided by choosing one of the questions and assessing the writing.↩︎\nThe “Workflow & formatting” grade is to assess the reproducible workflow and document format. This includes having a neatly organized document with readable code and your name and the date in the YAML.↩︎"
  },
  {
    "objectID": "hw/hw-05.html",
    "href": "hw/hw-05.html",
    "title": "HW 05: County Health",
    "section": "",
    "text": "In this homework, you’ll use simple and multiple linear regression to analyze the relationship between the number of doctors in a county, the number of beds, and the number of hospitals."
  },
  {
    "objectID": "hw/hw-05.html#data-county-health",
    "href": "hw/hw-05.html#data-county-health",
    "title": "HW 05: County Health",
    "section": "Data: County Health",
    "text": "Data: County Health\nThe data set for this homework is from the Stat2Data R package which is the companion package for this course’s textbook. It is the same data set that we used in AE-08. The data was originally generated by the American Medical Association and concerns the availability of health care in counties in the United States. You can find information here by searching for the County Health Resources dataset.\n\ndata(\"CountyHealth\") # Loads the data from the package\n\nIt is relatively easy to count the number of hospitals a county has, whereas counting the number of doctors is much more difficult. We’d like to build a linear model to predict the number of doctors, contained in the variable MDs, from the number of hospitals, Hospitals and the number of beds, Beds."
  },
  {
    "objectID": "hw/hw-05.html#exercise-1",
    "href": "hw/hw-05.html#exercise-1",
    "title": "HW 05: County Health",
    "section": "Exercise 1",
    "text": "Exercise 1\nDescribe what an observational unit represents for this data set. How many are there?"
  },
  {
    "objectID": "hw/hw-05.html#exercise-2",
    "href": "hw/hw-05.html#exercise-2",
    "title": "HW 05: County Health",
    "section": "Exercise 2",
    "text": "Exercise 2\nIn Example 1.7 of Stat2, they consider a simple linear model to predict the number of doctors (MDs) from the number of hospitals (Hospitals) in a metropolitan area. In that example, they found that a square root transformation on the response variable produced a more linear relationship. Create a new variable in the CountyHealth data frame called sqrtMDs. Hint: use the sqrt function inside the mutate function."
  },
  {
    "objectID": "hw/hw-05.html#exercise-3",
    "href": "hw/hw-05.html#exercise-3",
    "title": "HW 05: County Health",
    "section": "Exercise 3",
    "text": "Exercise 3\nUse the function ggpairs from the package GGally to generate a grid scatter plots and correlations. Note that you will need to select the variables you want to use. Which explanatory variable (Hospitals or Beds) has the highest correlation with SqrtMDs? Is this consistent with your visual assessment?"
  },
  {
    "objectID": "hw/hw-05.html#exercise-4",
    "href": "hw/hw-05.html#exercise-4",
    "title": "HW 05: County Health",
    "section": "Exercise 4",
    "text": "Exercise 4\nFit a simple linear model using SqrtMDs as the response variable and Hospitals as the predictor. You may use sqrt(MDs) in your lm call instead of SqrtMDs if you like. How much of the variability in the SqrtMDs values is explained by Hospitals? How much of the variability in MDs is explained by the model you just fit. To figure this out:\n\nAugment your model.\nConvert the fitted and observed response variables back to number of MDs rather than square-root of the number of MDs.\nCompute the \\(R^2\\).\n\nWhy are these two numbers different?"
  },
  {
    "objectID": "hw/hw-05.html#exercise-5",
    "href": "hw/hw-05.html#exercise-5",
    "title": "HW 05: County Health",
    "section": "Exercise 5",
    "text": "Exercise 5\nDo you think taking the square root of Beds would improve this model? Support your argument with plots and/or numbers."
  },
  {
    "objectID": "hw/hw-05.html#exercise-6",
    "href": "hw/hw-05.html#exercise-6",
    "title": "HW 05: County Health",
    "section": "Exercise 6",
    "text": "Exercise 6\nRepeat exercise 4 above with Beds as the predictor instead of Hospitals."
  },
  {
    "objectID": "hw/hw-05.html#exercise-7",
    "href": "hw/hw-05.html#exercise-7",
    "title": "HW 05: County Health",
    "section": "Exercise 7",
    "text": "Exercise 7\nFor the model you just fit, are there any high-influence outliers? Justify your answer using something from the lecture on outliers. There appear to be at least two high-leverage points. Which observations are these?"
  },
  {
    "objectID": "hw/hw-05.html#exercise-8",
    "href": "hw/hw-05.html#exercise-8",
    "title": "HW 05: County Health",
    "section": "Exercise 8",
    "text": "Exercise 8\nFit a multiple linear model using SqrtMDs as the response variable and both Hospitals and Beds as the predictors. Interpret both slopes and the intercept in the context of the problem."
  },
  {
    "objectID": "hw/hw-05.html#exercise-9",
    "href": "hw/hw-05.html#exercise-9",
    "title": "HW 05: County Health",
    "section": "Exercise 9",
    "text": "Exercise 9\nHow much of the variation in MDs is explained by the model you just fit? Which model would you say is the “best”, given what we’ve learned through the first lecture on multiple linear regression."
  },
  {
    "objectID": "hw/hw-05.html#exercise-10",
    "href": "hw/hw-05.html#exercise-10",
    "title": "HW 05: County Health",
    "section": "Exercise 10",
    "text": "Exercise 10\nUsing the “best” model, predict the average number of doctors in a metro area with 1,000 beds and 4 hospitals. Report a 95% prediction interval and interpret your results in context."
  },
  {
    "objectID": "hw/hw-05.html#grading",
    "href": "hw/hw-05.html#grading",
    "title": "HW 05: County Health",
    "section": "Grading",
    "text": "Grading\nTotal points available: 20 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1\n1\n\n\nEx 2\n1\n\n\nEx 3\n2\n\n\nEx 4\n2\n\n\nEx 5\n2\n\n\nEx 6\n2\n\n\nEx 7\n2\n\n\nEx 8\n2\n\n\nEx 9\n2\n\n\nEx 10\n2\n\n\nGrammar & Writing\n11\n\n\nWorkflow & formatting\n12"
  },
  {
    "objectID": "hw/hw-05.html#footnotes",
    "href": "hw/hw-05.html#footnotes",
    "title": "HW 05: County Health",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Grammar & Writing” grade is decided based on your grammar and writing. This is typically decided by choosing one of the questions and assessing the writing.↩︎\nThe “Workflow & formatting” grade is to assess the reproducible workflow and document format. This includes having a neatly organized document with readable code and your name and the date in the YAML.↩︎"
  },
  {
    "objectID": "hw/hw-03.html",
    "href": "hw/hw-03.html",
    "title": "Homework 03: Coffee grades",
    "section": "",
    "text": "In this homework you will analyze data from over 1,000 different coffees to explore the relationship between a coffee’s aroma and its flavor grade.\n\n\nBy the end of the homework you will…\n\nbe able to use mathematical models to conduct inference for the slope\nbe able to assess conditions for simple linear regression"
  },
  {
    "objectID": "hw/hw-03.html#learning-goals",
    "href": "hw/hw-03.html#learning-goals",
    "title": "Homework 03: Coffee grades",
    "section": "",
    "text": "By the end of the homework you will…\n\nbe able to use mathematical models to conduct inference for the slope\nbe able to assess conditions for simple linear regression"
  },
  {
    "objectID": "hw/hw-03.html#grading-23-pts",
    "href": "hw/hw-03.html#grading-23-pts",
    "title": "Homework 03: Coffee grades",
    "section": "Grading (23 pts)",
    "text": "Grading (23 pts)\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1\n2\n\n\nEx 2\n1\n\n\nEx 3\n2\n\n\nEx 4\n2\n\n\nEx 5\n2\n\n\nEx 6\n2\n\n\nEx 7\n2\n\n\nEx 8\n2\n\n\nEx 9\n2\n\n\nEx 10\n2\n\n\nEx 11\n2\n\n\nGrammar & Writing\n11\n\n\nWorkflow & formatting\n12"
  },
  {
    "objectID": "hw/hw-03.html#footnotes",
    "href": "hw/hw-03.html#footnotes",
    "title": "Homework 03: Coffee grades",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Grammar & Writing” grade is decided based on your grammar and writing. This is typically decided by choosing one of the questions and assessing the writing.↩︎\nThe “Workflow & formatting” grade is to assess the reproducible workflow and document format. This includes having a neatly organized document with readable code and your name and the date in the YAML.↩︎"
  },
  {
    "objectID": "hw/hw-01.html",
    "href": "hw/hw-01.html",
    "title": "HW 01: Park access",
    "section": "",
    "text": "This HW will go through much of the same workflow we’ve demonstrated in class. The main goal is to reinforce our demo of R and RStudio, which we will be using throughout the course both to learn the statistical concepts and to analyze real data and come to informed conclusions.\n\n\n\n\n\n\nNote\n\n\n\nR is the name of the programming language itself and RStudio is a convenient interface.\n\n\n\n\nBy the end of this homework, you will…\n\nBe familiar with the workflow using RStudio\nGain practice writing a reproducible report using Quarto\nBe able to create data visualizations using ggformula and use those visualizations to describe distributions\nBe able to fit, interpret, and evaluate simple linear regression models"
  },
  {
    "objectID": "hw/hw-01.html#learning-goals",
    "href": "hw/hw-01.html#learning-goals",
    "title": "HW 01: Park access",
    "section": "",
    "text": "By the end of this homework, you will…\n\nBe familiar with the workflow using RStudio\nGain practice writing a reproducible report using Quarto\nBe able to create data visualizations using ggformula and use those visualizations to describe distributions\nBe able to fit, interpret, and evaluate simple linear regression models"
  },
  {
    "objectID": "hw/hw-01.html#r-and-r-studio",
    "href": "hw/hw-01.html#r-and-r-studio",
    "title": "HW 01: Park access",
    "section": "R and R Studio",
    "text": "R and R Studio\nHere are the components of the RStudio IDE and here are the components of an Quarto (.qmd) file.\n\nYAML\nThe top portion of your Quarto file (between the three dashed lines) is called YAML. It stands for ““Yet Another Markup Language”. It is a human friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document.\n\n\n\n\n\n\nImportant\n\n\n\nOpen the Quarto (.qmd) file in your project, change the author name to your name, and render the document. Examine the rendered document. You can now work off of that file."
  },
  {
    "objectID": "hw/hw-01.html#footnotes",
    "href": "hw/hw-01.html#footnotes",
    "title": "HW 01: Park access",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Grammar & Writing” grade is decided based on your grammar and writing. This is typically decided by choosing one of the questions and assessing the writing.↩︎\nThe “Workflow & formatting” grade is to assess the reproducible workflow and document format. This includes having a neatly organized document with readable code and your name and the date in the YAML.↩︎"
  },
  {
    "objectID": "exam/exam-02-review.html#exam-02",
    "href": "exam/exam-02-review.html#exam-02",
    "title": "Exam 02 Overview",
    "section": "Exam 02",
    "text": "Exam 02\n\n20% of final course grade\nMay receive Exam 1 “subscore” which will replace your first exam if it is higher (maximum 90%)",
    "crumbs": [
      "Exams",
      "Exam 2 Overview"
    ]
  },
  {
    "objectID": "exam/exam-02-review.html#content-week-2",
    "href": "exam/exam-02-review.html#content-week-2",
    "title": "Exam 02 Overview",
    "section": "Content: Week 2",
    "text": "Content: Week 2\n\nTransformations and Outliers for simple linear regression\nFitting and interpreting multiple linear regression models\nInference for multiple linear regression\nConditions for multiple linear regression",
    "crumbs": [
      "Exams",
      "Exam 2 Overview"
    ]
  },
  {
    "objectID": "exam/exam-02-review.html#practice",
    "href": "exam/exam-02-review.html#practice",
    "title": "Exam 02 Overview",
    "section": "Practice",
    "text": "Practice\n\n📋 Exam 2 Review",
    "crumbs": [
      "Exams",
      "Exam 2 Overview"
    ]
  },
  {
    "objectID": "exam/exam-01-review.html#exam-01",
    "href": "exam/exam-01-review.html#exam-01",
    "title": "Exam 01 Overview",
    "section": "Exam 01",
    "text": "Exam 01\n\n\n\n20% of final course grade\nExploratory data analysis\nFitting and interpreting simple linear regression models\nStatistical models and regression equations\nSimulation-based inference (SBI)\n\nConcepts not code for SBI\n\n\n\n\nMathematical models for inference\nPrediction\nModel conditions\nModel evaluation",
    "crumbs": [
      "Exams",
      "Exam 1 Overview"
    ]
  },
  {
    "objectID": "computing-r-resources.html",
    "href": "computing-r-resources.html",
    "title": "Resources for learning R",
    "section": "",
    "text": "R for Data Science by Hadley Wickham & Garrett Grolemund\nTidy Modeling with R by Max Kuhn & Julia Silge",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "computing-r-resources.html#books-free-online",
    "href": "computing-r-resources.html#books-free-online",
    "title": "Resources for learning R",
    "section": "",
    "text": "R for Data Science by Hadley Wickham & Garrett Grolemund\nTidy Modeling with R by Max Kuhn & Julia Silge",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "computing-r-resources.html#rstudio-primers",
    "href": "computing-r-resources.html#rstudio-primers",
    "title": "Resources for learning R",
    "section": "RStudio Primers",
    "text": "RStudio Primers\n\nInteractive LearnR Tutorial\nR Primers for full list of tutorials. Note that we will be using a simplified version of ggplot2 in this course.",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "computing-r-resources.html#additional-resources",
    "href": "computing-r-resources.html#additional-resources",
    "title": "Resources for learning R",
    "section": "Additional resources",
    "text": "Additional resources\n\nRStudio Cheatsheets\nR Fun workshops and videos by Duke Center for Data and Visualization Sciences.",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "computing-access.html",
    "href": "computing-access.html",
    "title": "Computing access",
    "section": "",
    "text": "To access RStudio at the College of Idaho, go to the CofI Posit Workbench website and sign-in using the CofI email and password.",
    "crumbs": [
      "Computing",
      "Computing access"
    ]
  },
  {
    "objectID": "ae/ae-12-multiple-logistic.html",
    "href": "ae/ae-12-multiple-logistic.html",
    "title": "AE 12: Multiple Logistic Regression",
    "section": "",
    "text": "Important\n\n\n\n\nOpen RStudio and create a subfolder in your AE folder called “AE-12”.\nGo to the Canvas and locate your AE-12 assignment to get started.\nUpload the ae-12.qmd and framingham.csv files into the folder you just created."
  },
  {
    "objectID": "ae/ae-12-multiple-logistic.html#packages",
    "href": "ae/ae-12-multiple-logistic.html#packages",
    "title": "AE 12: Multiple Logistic Regression",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(knitr)\n\n\nData: Framingham study\nThis data set is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to predict if a randomly selected adult is high risk for heart disease in the next 10 years.\n\n\nResponse variable\n\nTenYearCHD:\n\n1: Patient developed heart disease within 10 years of exam\n0: Patient did not develop heart disease within 10 years of exam\n\n\n\n\nWhat are my predictor variables?\nBased on your group, use the following as your predictor variables.\n\nGroup 1:\n\nBMI: patient’s body mass index\ndiabetes: 1 if patient has diabetes, 0 otherwise\n\nGroup 2:\n\ncigsPerDay: number of cigarettes patient smokes per day\ncurrentSmoker: 1 if current smoker, 0 otherwise\n\nGroup 3:\n\nsysBP: systolic blood pressure (mmHg)\nBPMeds: 1 if they are on blood pressure medication, 0 otherwise\n\nGroup 4:\n\ntotChol: total cholesterol (mg/dL)\nprevalentHyp: 1 if patient was hypertensive, 0 otherwise"
  },
  {
    "objectID": "ae/ae-12-multiple-logistic.html#exercise-0",
    "href": "ae/ae-12-multiple-logistic.html#exercise-0",
    "title": "AE 12: Multiple Logistic Regression",
    "section": "Exercise 0",
    "text": "Exercise 0\nEdit the code below to drop any missing values, convert the categorical predictors into factors, and select only the eight variables listed above along with TenYearCHD. Make sure to do those thing in an order that makes sense:\n\nheart_disease &lt;- read_csv(\"../data/framingham.csv\")\n\nRows: 4240 Columns: 16\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (16): male, age, education, currentSmoker, cigsPerDay, BPMeds, prevalent...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "ae/ae-12-multiple-logistic.html#exercise-1",
    "href": "ae/ae-12-multiple-logistic.html#exercise-1",
    "title": "AE 12: Multiple Logistic Regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nFit a logistic regression model predicting TenYearCHD using your two assigned predictors. Write your resulting model on the board in any format you choose."
  },
  {
    "objectID": "ae/ae-12-multiple-logistic.html#exercise-2",
    "href": "ae/ae-12-multiple-logistic.html#exercise-2",
    "title": "AE 12: Multiple Logistic Regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nFor both of your coefficients complete the madlibs below interpreting the value, you may need to edit a bit to make the final sentence grammatically correct:\nQuantitative variable: When [variable name] increases by [number], the typical [increase/decrease] in [quantity] is [number], [qualifying statement that makes this different than when we have one predictor].\nQuantitative variable: When [variable name] increases by [number], the typical odds-ratio is multiplied by a factor of [number], [qualifying statement that makes this different than when we have one predictor].\nCategorical variable: For patients who are [insert category] at the time of examination, the typical [increase/decrease] in [quantity] is [number] vs patients [insert opposite category], [qualifying statement that makes this different than when we have one predictor].\nCategorical variable: For patients who are [insert category] at the time of examination, the odds-ratio typically [increases/decreases] by a factor of [number] vs patients [insert opposite category], [qualifying statement that makes this different than when we have one predictor]."
  },
  {
    "objectID": "ae/ae-12-multiple-logistic.html#exercise-3",
    "href": "ae/ae-12-multiple-logistic.html#exercise-3",
    "title": "AE 12: Multiple Logistic Regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nCompute p-values for both of your coefficients using tidy."
  },
  {
    "objectID": "ae/ae-12-multiple-logistic.html#exercise-4",
    "href": "ae/ae-12-multiple-logistic.html#exercise-4",
    "title": "AE 12: Multiple Logistic Regression",
    "section": "Exercise 4",
    "text": "Exercise 4\nFor both of your coefficients write the null and alternative hypotheses (in both symbols and words), the test statistics, and the p-values on the board (just pick them out from the tidy output). Then, complete the madlibs below interpreting the p-values, you may need to edit the final statements to make them grammatically correct:\nQuantitative variable: The [quantity] is [size], so we [do something to] \\(H_0\\). The data [do/do not] provide sufficient evidence that [variable name] is a statistically significant predictor of [variable], [qualifying statement that makes this different than when we have one predictor].\nCategorical variable: The [quantity] is [size], so we [do something to] \\(H_0\\). The data [do/do not] provide sufficient evidence that [variable name] is a statistically significant predictor of [variable], [qualifying statement that makes this different than when we have one predictor]."
  },
  {
    "objectID": "ae/ae-12-multiple-logistic.html#exercise-5",
    "href": "ae/ae-12-multiple-logistic.html#exercise-5",
    "title": "AE 12: Multiple Logistic Regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nUse tidy to compute 99% confidence intervals for both of your coefficients. Write them on the board."
  },
  {
    "objectID": "ae/ae-12-multiple-logistic.html#exercise-6",
    "href": "ae/ae-12-multiple-logistic.html#exercise-6",
    "title": "AE 12: Multiple Logistic Regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nInterpret the the confidence intervals below:\nLog-odds interpretation (Quantitative):\nOdds-ratio interpretation (Quantitative):\nLog-odds interpretation (Categorical):\nOdds-ratio interpretation (Categorical):"
  },
  {
    "objectID": "ae/ae-12-multiple-logistic.html#exercise-7",
    "href": "ae/ae-12-multiple-logistic.html#exercise-7",
    "title": "AE 12: Multiple Logistic Regression",
    "section": "Exercise 7",
    "text": "Exercise 7\n\nFit the full model. I.e. the model using all eight predictors included in the data set.\nUse glance to compute the deviance of both models. Which has the better deviance?\nUse the anova function to conduct a likelihood ratio test between the model containing your two predictors and the full model.\nOn the white board, write the null and alternative hypotheses (in both symbols and words), the test statistic, the p-value on the board. 5 What is the conclusion of your test in context?"
  },
  {
    "objectID": "ae/ae-12-multiple-logistic.html#submission",
    "href": "ae/ae-12-multiple-logistic.html#submission",
    "title": "AE 12: Multiple Logistic Regression",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the HTML file with all of your work from today’s class.\nUpload your QMD and HTML files to the Canvas assignment."
  },
  {
    "objectID": "ae/ae-10-logistic-inference.html",
    "href": "ae/ae-10-logistic-inference.html",
    "title": "AE 10: Logistic Regression Inference",
    "section": "",
    "text": "Important\n\n\n\n\nOpen RStudio and create a subfolder in your AE folder called “AE-10”.\nGo to the Canvas and locate your AE-10 assignment to get started.\nUpload the ae-10.qmd and framingham.csv files into the folder you just created."
  },
  {
    "objectID": "ae/ae-10-logistic-inference.html#packages",
    "href": "ae/ae-10-logistic-inference.html#packages",
    "title": "AE 10: Logistic Regression Inference",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(knitr)\n\nheart_disease &lt;- read_csv(\"../data/framingham.csv\") |&gt;\n  select(totChol, TenYearCHD, age, BMI, cigsPerDay, heartRate, sysBP, diabetes) |&gt;\n  drop_na()\n\n\nData: Framingham study\nThis data set is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to predict if a randomly selected adult is high risk for heart disease in the next 10 years.\n\n\nResponse variable\n\nTenYearCHD:\n\n1: Patient developed heart disease within 10 years of exam\n0: Patient did not develop heart disease within 10 years of exam\n\n\n\n\nWhat’s my predictor variable?\nBased on your group, use the following as your predictor variable.\n\nGroup 1 - totChol: total cholesterol (mg/dL)\nGroup 2 -BMI: patient’s body mass index\nGroup 3 -cigsPerDay: number of cigarettes patient smokes per day\nGroup 4 -heartRate: Heart rate (beats per minute)\n\n\n\nAdditional Variables\n\nsysbp - the patients systolic blood pressure at the time of examination\ndiabetes - 1 if the patient had diabetes and 0 if the patient didn’t have diabetes at the time of examination"
  },
  {
    "objectID": "ae/ae-10-logistic-inference.html#exercise-0",
    "href": "ae/ae-10-logistic-inference.html#exercise-0",
    "title": "AE 10: Logistic Regression Inference",
    "section": "Exercise 0",
    "text": "Exercise 0\nFit a logistic regression model predicting TenYearCHD from your group’s predictor variable. Note that we won’t return to this model until Exercise 3."
  },
  {
    "objectID": "ae/ae-10-logistic-inference.html#exercise-1",
    "href": "ae/ae-10-logistic-inference.html#exercise-1",
    "title": "AE 10: Logistic Regression Inference",
    "section": "Exercise 1",
    "text": "Exercise 1\nBefore you work on your own model, lets consider the variable sysBP which represents the patients systolic blood pressure (the top number). You have five minutes, find the \\(\\beta\\)’s that result in the largest log-likelihood for a logistic regression model predicting the risk of coronary heart disease from sysBP:\n\n# Change these\nbeta0 &lt;- 0\nbeta1 &lt;- 0\n\n\n# Don't change anything below this line\npredicted_probabilities &lt;- exp(beta0 + beta1*heart_disease$sysBP)/(1+exp(beta0 + beta1*heart_disease$sysBP))\nlog_likelihoods &lt;- heart_disease$TenYearCHD*log(predicted_probabilities) +\n                        (1-heart_disease$TenYearCHD)*log(1-predicted_probabilities)\n\n# Final log likelihood\nsum(log_likelihoods)\n\n[1] -2871.016"
  },
  {
    "objectID": "ae/ae-10-logistic-inference.html#exercise-2",
    "href": "ae/ae-10-logistic-inference.html#exercise-2",
    "title": "AE 10: Logistic Regression Inference",
    "section": "Exercise 2",
    "text": "Exercise 2\nCompute the empirical logit for each level of diabetes:\n\nUse the function tally to compute the count the number of successes and failures for each level of diabetes.\nCompute the empirical odds.\nCompute the log of these odds."
  },
  {
    "objectID": "ae/ae-10-logistic-inference.html#exercise-3",
    "href": "ae/ae-10-logistic-inference.html#exercise-3",
    "title": "AE 10: Logistic Regression Inference",
    "section": "Exercise 3",
    "text": "Exercise 3\nIs linearity satisfied for the model you fit in Exercise 0?"
  },
  {
    "objectID": "ae/ae-10-logistic-inference.html#exercise-4",
    "href": "ae/ae-10-logistic-inference.html#exercise-4",
    "title": "AE 10: Logistic Regression Inference",
    "section": "Exercise 4",
    "text": "Exercise 4\nFor your quantitative predictor, conduct a hypothesis test to determine whether the slope of your variable is statistically significant. On the white board:\n\nSpecify the null and alternative hypotheses.\nCompute the test statistic.\nGive the p-value.\nInterpret the result."
  },
  {
    "objectID": "ae/ae-10-logistic-inference.html#exercise-5",
    "href": "ae/ae-10-logistic-inference.html#exercise-5",
    "title": "AE 10: Logistic Regression Inference",
    "section": "Exercise 5",
    "text": "Exercise 5\nFor your quantitative variable. Construct a 99% confidence interval. Hint, you can use the conf.level argument in tidy to change the confidence level. On the white board:\n\nInterpret your confidence interval in log-odds.\nInterpret your confidence interval in odds."
  },
  {
    "objectID": "ae/ae-10-logistic-inference.html#exercise-6",
    "href": "ae/ae-10-logistic-inference.html#exercise-6",
    "title": "AE 10: Logistic Regression Inference",
    "section": "Exercise 6",
    "text": "Exercise 6\nConduct a hypothesis test for your \\(\\beta_1\\). Interpret it in context and be prepared to discuss with the class. Note that you need not write down the entire hypothesis testing framework."
  },
  {
    "objectID": "ae/ae-10-logistic-inference.html#exercise-7",
    "href": "ae/ae-10-logistic-inference.html#exercise-7",
    "title": "AE 10: Logistic Regression Inference",
    "section": "Exercise 7",
    "text": "Exercise 7\nWhy do you think it doesn’t quite make sense to talk about prediction intervals or confidence intervals in the context of a logistic regression model?"
  },
  {
    "objectID": "ae/ae-10-logistic-inference.html#exercise-8",
    "href": "ae/ae-10-logistic-inference.html#exercise-8",
    "title": "AE 10: Logistic Regression Inference",
    "section": "Exercise 8",
    "text": "Exercise 8\nCompute the deviance for the model you fit above. Do larger or smaller values of deviance indicate a better model?"
  },
  {
    "objectID": "ae/ae-10-logistic-inference.html#exercise-9",
    "href": "ae/ae-10-logistic-inference.html#exercise-9",
    "title": "AE 10: Logistic Regression Inference",
    "section": "Exercise 9",
    "text": "Exercise 9\nIn Exercise 6, we used a ____ test to compute our p-value. Test the same hypotheses, this time using a likelihood ratio test. Write out all the steps on the white board, following the slides :\n\nSpecify the null and alternative hypotheses.\nCompute the test statistic.\nGive the p-value. Draw a picture!\nInterpret the result."
  },
  {
    "objectID": "ae/ae-10-logistic-inference.html#exercise-10",
    "href": "ae/ae-10-logistic-inference.html#exercise-10",
    "title": "AE 10: Logistic Regression Inference",
    "section": "Exercise 10",
    "text": "Exercise 10\nThe test statistic for a LRT is the difference in deviance between your full and reduced model:\n\nDo larger or smaller values of your test statistic provide more evidence for the alternative hypothesis?\nDo you think your test statistic can ever be negative? Why? Do not use the Chi-Squared distribution to justify your answer."
  },
  {
    "objectID": "ae/ae-10-logistic-inference.html#exercise-11",
    "href": "ae/ae-10-logistic-inference.html#exercise-11",
    "title": "AE 10: Logistic Regression Inference",
    "section": "Exercise 11",
    "text": "Exercise 11\nUse the anova function to recreate your p-value from the previous problem."
  },
  {
    "objectID": "ae/ae-10-logistic-inference.html#exercise-12",
    "href": "ae/ae-10-logistic-inference.html#exercise-12",
    "title": "AE 10: Logistic Regression Inference",
    "section": "Exercise 12",
    "text": "Exercise 12\nAre the p-values you got from Exercises 6, 9, and 11 all the EXACT same? Make sure you are displaying enough digits so that your p-values aren’t rounding to zero. If they are different, which one do you think is the most reliable? Why?"
  },
  {
    "objectID": "ae/ae-10-logistic-inference.html#submission",
    "href": "ae/ae-10-logistic-inference.html#submission",
    "title": "AE 10: Logistic Regression Inference",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the HTML file with all of your work from today’s class.\nUpload your QMD and HTML files to the Canvas assignment."
  },
  {
    "objectID": "ae/ae-08-mlr-inference.html",
    "href": "ae/ae-08-mlr-inference.html",
    "title": "AE 08: Inference for Multiple Linear Regression",
    "section": "",
    "text": "Important\n\n\n\n\nOpen RStudio and create a subfolder in your AE folder called “AE-08”.\nGo to the Canvas and locate your AE-08 assignment to get started.\nUpload the ae-08.qmd and rail-trail.csv files into the folder you just created."
  },
  {
    "objectID": "ae/ae-08-mlr-inference.html#packages-data",
    "href": "ae/ae-08-mlr-inference.html#packages-data",
    "title": "AE 08: Inference for Multiple Linear Regression",
    "section": "Packages + data",
    "text": "Packages + data\n\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(broom)\nlibrary(knitr)\nlibrary(mosaic)\nlibrary(mosaicData)\n\nrail_trail &lt;- read_csv(\"../data/rail_trail.csv\")\n\nThe data for this AE is based on data from the Pioneer Valley Planning Commission (PVPC) and is included in the mosaicData package. The PVPC collected data for ninety days from April 5, 2005 to November 15, 2005. Data collectors set up a laser sensor, with breaks in the laser beam recording when a rail-trail user passed the data collection station. More information can be found here, here, and here.\nResponse\n\nvolume: Number bikes that broke the laser beam"
  },
  {
    "objectID": "ae/ae-08-mlr-inference.html#analysis-goal",
    "href": "ae/ae-08-mlr-inference.html#analysis-goal",
    "title": "AE 08: Inference for Multiple Linear Regression",
    "section": "Analysis goal",
    "text": "Analysis goal\nThe goals of this activity are to:\n\nPerform inference for multiple linear regression\nConduct/interpret hypothesis tests\nConstruct/interpret confidence intervals\nDetermine whether the conditions for inference are satisfied in this multi-predictor setting."
  },
  {
    "objectID": "ae/ae-08-mlr-inference.html#exercise-0",
    "href": "ae/ae-08-mlr-inference.html#exercise-0",
    "title": "AE 08: Inference for Multiple Linear Regression",
    "section": "Exercise 0",
    "text": "Exercise 0\nImport GGally and use ggpairs to plot all combinations of variables."
  },
  {
    "objectID": "ae/ae-08-mlr-inference.html#exercise-1",
    "href": "ae/ae-08-mlr-inference.html#exercise-1",
    "title": "AE 08: Inference for Multiple Linear Regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nChoose two explanatory variables which you think will best predict Volume based on the plot above. Write your two variables on the board. You may not use the same combination as another group. If you choose a temperature, only select one."
  },
  {
    "objectID": "ae/ae-08-mlr-inference.html#exercise-2",
    "href": "ae/ae-08-mlr-inference.html#exercise-2",
    "title": "AE 08: Inference for Multiple Linear Regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nFit and display two linear regression models predicting volume from the two predictors you chose. In one, include an interaction term between the two variables and in the other, do not."
  },
  {
    "objectID": "ae/ae-08-mlr-inference.html#exercise-3",
    "href": "ae/ae-08-mlr-inference.html#exercise-3",
    "title": "AE 08: Inference for Multiple Linear Regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nConsider the model without an interaction term. Perform a hypothesis test on one of your explanatory variables (fill in the blanks where appropriate:\n\nSet hypothesis: \\(H_0: \\beta_{fill in} [fill in]\\) vs. \\(\\beta_{fill in} [fill in]\\). Restate these hypothesis in words: [fill in]\nCalculate test statistics and p-value: The test statistic is \\([fill in]\\). The p-value is \\([fill in]\\).\nState the conclusion: [fill in]"
  },
  {
    "objectID": "ae/ae-08-mlr-inference.html#exercise-4",
    "href": "ae/ae-08-mlr-inference.html#exercise-4",
    "title": "AE 08: Inference for Multiple Linear Regression",
    "section": "Exercise 4",
    "text": "Exercise 4\nConsider the model with an interaction term. Interpret the the p-value associated with the interaction term."
  },
  {
    "objectID": "ae/ae-08-mlr-inference.html#exercise-5",
    "href": "ae/ae-08-mlr-inference.html#exercise-5",
    "title": "AE 08: Inference for Multiple Linear Regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nGenerate 95% confidence intervals for the model without an interaction term. Hint: use the tidy function with the argument conf.int = TRUE. Interpret the confidence interval for the same explanatory variable as Exercise 3 and explain why the combination of p-value and confidence interval makes sense."
  },
  {
    "objectID": "ae/ae-08-mlr-inference.html#exercise-6",
    "href": "ae/ae-08-mlr-inference.html#exercise-6",
    "title": "AE 08: Inference for Multiple Linear Regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nWhat does it mean for two things to be independent in statistics (feel free to use Google)? Do we think our p-values/confidence intervals are independent across variables?"
  },
  {
    "objectID": "ae/ae-08-mlr-inference.html#exercise-7",
    "href": "ae/ae-08-mlr-inference.html#exercise-7",
    "title": "AE 08: Inference for Multiple Linear Regression",
    "section": "Exercise 7",
    "text": "Exercise 7\n\n\n\n\n\n\nImportant\n\n\n\nIf the intercept term was significant, use that model for the rest of this activity, otherwise use the model without the interaction term.\n\n\nGenerate a scatter plot of the residuals vs. the fitted values for this model."
  },
  {
    "objectID": "ae/ae-08-mlr-inference.html#exercise-8",
    "href": "ae/ae-08-mlr-inference.html#exercise-8",
    "title": "AE 08: Inference for Multiple Linear Regression",
    "section": "Exercise 8",
    "text": "Exercise 8\nPlot the residuals vs. each of your predictors (two plots total)."
  },
  {
    "objectID": "ae/ae-08-mlr-inference.html#exercise-9",
    "href": "ae/ae-08-mlr-inference.html#exercise-9",
    "title": "AE 08: Inference for Multiple Linear Regression",
    "section": "Exercise 9",
    "text": "Exercise 9\nBased on the three plots you’ve made, do you think the linearity condition is satisfied?"
  },
  {
    "objectID": "ae/ae-08-mlr-inference.html#exerise-10",
    "href": "ae/ae-08-mlr-inference.html#exerise-10",
    "title": "AE 08: Inference for Multiple Linear Regression",
    "section": "Exerise 10",
    "text": "Exerise 10\nWe check the constant variance assumption in the same way we do with SLR. To what extent does the constant variance condition seem to be satisfied?"
  },
  {
    "objectID": "ae/ae-08-mlr-inference.html#exercise-11",
    "href": "ae/ae-08-mlr-inference.html#exercise-11",
    "title": "AE 08: Inference for Multiple Linear Regression",
    "section": "Exercise 11",
    "text": "Exercise 11\nGenerate a histogram and QQ-plot of the residuals. To what extent do you believe the normality condition is satisfied?"
  },
  {
    "objectID": "ae/ae-08-mlr-inference.html#exercise-12",
    "href": "ae/ae-08-mlr-inference.html#exercise-12",
    "title": "AE 08: Inference for Multiple Linear Regression",
    "section": "Exercise 12",
    "text": "Exercise 12\nHow do you think would you go about checking the independence condition?"
  },
  {
    "objectID": "ae/ae-08-mlr-inference.html#to-submit-the-ae",
    "href": "ae/ae-08-mlr-inference.html#to-submit-the-ae",
    "title": "AE 08: Inference for Multiple Linear Regression",
    "section": "To submit the AE",
    "text": "To submit the AE\n\n\n\n\n\n\nImportant\n\n\n\n\nRender the document to produce the HTML file with all of your work from today’s class.\nUpload your QMD and HTML files to the Canvas assignment."
  },
  {
    "objectID": "ae/ae-06-transformations.html",
    "href": "ae/ae-06-transformations.html",
    "title": "AE 06: Transformations",
    "section": "",
    "text": "Important\n\n\n\n\nOpen RStudio and create a subfolder in your AE folder called “AE-06”.\nGo to the Canvas and locate your AE 06 assignment to get started.\nUpload the ae-06.qmd files into the folder you just created. The .qmd and HTML responses are due in Canvas. You can check the due date on the Canvas assignment.\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(yardstick)\nlibrary(mosaic)\nlibrary(broom)\nlibrary(knitr)\nlibrary(patchwork) #arrange plots in a grid"
  },
  {
    "objectID": "ae/ae-06-transformations.html#data",
    "href": "ae/ae-06-transformations.html#data",
    "title": "AE 06: Transformations",
    "section": "Data",
    "text": "Data\nThe data set for this assignment contains house sale prices for King County, which includes Seattle, from homes sold between May 2014 and May 2015. It was originally Obtained from Kaggle.com. Our goal will be to try and predict the sale price of a house from the houses square footage."
  },
  {
    "objectID": "ae/ae-06-transformations.html#variables",
    "href": "ae/ae-06-transformations.html#variables",
    "title": "AE 06: Transformations",
    "section": "Variables",
    "text": "Variables\n\nOutcome\n\nprice: the sale price in dollars\n\nPredictor\n\nsqft_living: the square footage of the home"
  },
  {
    "objectID": "ae/ae-06-transformations.html#exercise-0",
    "href": "ae/ae-06-transformations.html#exercise-0",
    "title": "AE 06: Transformations",
    "section": "Exercise 0",
    "text": "Exercise 0\nThe dataset that we’ll be using today is called house_prices and is contained in the moderndive package. Load the moderndive package."
  },
  {
    "objectID": "ae/ae-06-transformations.html#exercise-1",
    "href": "ae/ae-06-transformations.html#exercise-1",
    "title": "AE 06: Transformations",
    "section": "Exercise 1",
    "text": "Exercise 1\nGenerate a scatter plot of the data. If you finish before other groups, visualize what a linear model would look like if fitted to this data. Based on what you see, do you think the conditions required of linear regression will be met?"
  },
  {
    "objectID": "ae/ae-06-transformations.html#exercise-2",
    "href": "ae/ae-06-transformations.html#exercise-2",
    "title": "AE 06: Transformations",
    "section": "Exercise 2",
    "text": "Exercise 2\nGenerate a scatter plot of the data but take the log of price. You can do this by putting log(price) ~ sqft_living instead of price ~ sqft_living in your call to gf_point. Do you think the data is ready for a linear regression model?"
  },
  {
    "objectID": "ae/ae-06-transformations.html#exercise-3",
    "href": "ae/ae-06-transformations.html#exercise-3",
    "title": "AE 06: Transformations",
    "section": "Exercise 3",
    "text": "Exercise 3\nGenerate a scatter plot of the data but take the log of price and a power transformation of sqrt_living. You can do this by putting log(price) ~ sqft_living^(power) instead of price ~ sqft_living in your call to gf_point. Once you’re done, your data should be ready for modeling."
  },
  {
    "objectID": "ae/ae-06-transformations.html#exercise-4",
    "href": "ae/ae-06-transformations.html#exercise-4",
    "title": "AE 06: Transformations",
    "section": "Exercise 4",
    "text": "Exercise 4\nFit the model above by using the lm function. Output the coefficients of your model. Can do this by using the formula (the thing with the ~), just put I() around any power. For example I(sqrt_living^(1/2)) instead of sqrt_living^(1/2)."
  },
  {
    "objectID": "ae/ae-06-transformations.html#exercise-5-difficult",
    "href": "ae/ae-06-transformations.html#exercise-5-difficult",
    "title": "AE 06: Transformations",
    "section": "Exercise 5 (Difficult)",
    "text": "Exercise 5 (Difficult)\nWrite your final model in the board in two ways. One with \\(\\log(Y)\\) and one with \\(Y\\) is on the right side of the equal sign. You will want to use the properties of logs."
  },
  {
    "objectID": "ae/ae-06-transformations.html#exercise-6-time-permitting",
    "href": "ae/ae-06-transformations.html#exercise-6-time-permitting",
    "title": "AE 06: Transformations",
    "section": "Exercise 6 (Time Permitting)",
    "text": "Exercise 6 (Time Permitting)\nAnalyze the residuals for the model you created model."
  },
  {
    "objectID": "ae/ae-06-transformations.html#to-submit-the-ae",
    "href": "ae/ae-06-transformations.html#to-submit-the-ae",
    "title": "AE 06: Transformations",
    "section": "To submit the AE:",
    "text": "To submit the AE:\n\n\n\n\n\n\nImportant\n\n\n\n\nRender the document to produce the HTML file with all of your work from today’s class.\nUpload your QMD and HTM files to the Canvas assignment."
  },
  {
    "objectID": "ae/ae-04-math-models.html",
    "href": "ae/ae-04-math-models.html",
    "title": "AE 04: Mathematical Models",
    "section": "",
    "text": "Important\n\n\n\n\nOpen RStudio and create a subfolder in your AE folder called “AE-04”\nGo to the Canvas and locate your AE 04 assignment to get started.\nUpload the ae-04.qmd and spotify-popular.csv files into the folder you just created.\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(broom)\nlibrary(knitr)\nlibrary(patchwork) #arrange plots in a grid"
  },
  {
    "objectID": "ae/ae-04-math-models.html#data",
    "href": "ae/ae-04-math-models.html#data",
    "title": "AE 04: Mathematical Models",
    "section": "Data",
    "text": "Data\nThe data set for this assignment is a subset from the Spotify Songs Tidy Tuesday data set. The data were originally obtained from Spotify using the spotifyr R package.\nIt contains numerous characteristics for each song. You can see the full list of variables and definitions here. This analysis will focus specifically on the following variables:\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ntrack_id\ncharacter\nSong unique ID\n\n\ntrack_name\ncharacter\nSong Name\n\n\ntrack_artist\ncharacter\nSong Artist\n\n\ntrack_popularity\ndouble\nSong Popularity (0-100) where higher is better\n\n\ndanceability\ndouble\nDanceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.\n\n\nenergy\ndouble\nEnergy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\n\n\nloudness\ndouble\nThe overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db.\n\n\nvalence\ndouble\nA measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\n\n\ntempo\ndouble\nThe overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.\n\n\nduration_ms\ndouble\nDuration of song in milliseconds\n\n\n\n\nspotify &lt;- read_csv(\"../data/spotify-popular.csv\")\n\nWhat makes a song danceable? To answer this question, we’ll analyze data on some of the most popular songs on Spotify, i.e. those with track_popularity &gt;= 80. We’ll use linear regression to fit some models to predict a song’s dancebility. Each group will be assigned a different predictor variable.\n\nPredictor Assignment:\nBelow is your assigned predictor. Look at the table above for a definition.\n\nGroup 1: energy\nGroup 2: loudness\nGroup 3: valence\nGroup 4: tempo"
  },
  {
    "objectID": "ae/ae-04-math-models.html#exercise-0",
    "href": "ae/ae-04-math-models.html#exercise-0",
    "title": "AE 04: Mathematical Models",
    "section": "Exercise 0",
    "text": "Exercise 0\nBelow are plots as part of the exploratory data analysis. Change duration_ms and the axis labels to match your explanatory variable.\n\np1 &lt;- gf_histogram(~duration_ms, data = spotify) |&gt;  \n  gf_labs(title = \"Distribution of Song Duration\", \n       subtitle = \" for Popular songs on Spotify\", \n       x = \"Duration (ms)\")\n\np2 &lt;- gf_histogram(~danceability, data = spotify) |&gt; \n  gf_labs(title = \"Distribution of Danceability\", \n       subtitle = \"for Popular songs on Spotify\", \n       x = \"Danceability\")\np1 + p2 # The patchwork package will arrange your plots for you\n\n\n\n\n\n\n\n\n\ngf_point(danceability ~ duration_ms, data = spotify) |&gt; \n  gf_labs(title = \"Danceability vs. Duration\", \n       subtitle = \"Popular songs on Spotify\", \n       x = \"Duration (ms)\", \n       y = \"Danceability\")"
  },
  {
    "objectID": "ae/ae-04-math-models.html#exercise-1",
    "href": "ae/ae-04-math-models.html#exercise-1",
    "title": "AE 04: Mathematical Models",
    "section": "Exercise 1",
    "text": "Exercise 1\nFit a model using your assigned explanatory variable to predict a songs danceability. Use tidy and kable to neatly display your model and have your reporter write you model on the white board. Be prepared to verbally interpret the slope.\n\n## add code"
  },
  {
    "objectID": "ae/ae-04-math-models.html#exercise-2",
    "href": "ae/ae-04-math-models.html#exercise-2",
    "title": "AE 04: Mathematical Models",
    "section": "Exercise 2",
    "text": "Exercise 2\nWrite down the the null and alternative hypotheses to test whether your explanatory variable is a useful predictor."
  },
  {
    "objectID": "ae/ae-04-math-models.html#exercise-3",
    "href": "ae/ae-04-math-models.html#exercise-3",
    "title": "AE 04: Mathematical Models",
    "section": "Exercise 3",
    "text": "Exercise 3\nIdentify the standard error of \\(\\hat{\\beta}_1\\) and your T-statistic from the output above. Interpret your test statistic. Do you think this provides evidence that your explanatory variable is a useful predictor of danceability?"
  },
  {
    "objectID": "ae/ae-04-math-models.html#exercise-4",
    "href": "ae/ae-04-math-models.html#exercise-4",
    "title": "AE 04: Mathematical Models",
    "section": "Exercise 4",
    "text": "Exercise 4\nIdentify and interpret the p-value in the output from Exercise 1. On the white board, draw a sketch of your p-value."
  },
  {
    "objectID": "ae/ae-04-math-models.html#exercise-5",
    "href": "ae/ae-04-math-models.html#exercise-5",
    "title": "AE 04: Mathematical Models",
    "section": "Exercise 5",
    "text": "Exercise 5\nBased on your p-value, how strong is the evidence that your assigned explanatory variable is a useful predictor of danceability?"
  },
  {
    "objectID": "ae/ae-04-math-models.html#exercise-6",
    "href": "ae/ae-04-math-models.html#exercise-6",
    "title": "AE 04: Mathematical Models",
    "section": "Exercise 6",
    "text": "Exercise 6\nWrite a conclusion for your test in the context of the problem."
  },
  {
    "objectID": "ae/ae-04-math-models.html#exercise-7",
    "href": "ae/ae-04-math-models.html#exercise-7",
    "title": "AE 04: Mathematical Models",
    "section": "Exercise 7",
    "text": "Exercise 7\nUse tidy to compute a (95 - your group number)% confidence interval for your slope."
  },
  {
    "objectID": "ae/ae-04-math-models.html#exercise-8",
    "href": "ae/ae-04-math-models.html#exercise-8",
    "title": "AE 04: Mathematical Models",
    "section": "Exercise 8",
    "text": "Exercise 8\nOn the white board, draw a picture representing the critical value for you confidence interval."
  },
  {
    "objectID": "ae/ae-04-math-models.html#exercise-9",
    "href": "ae/ae-04-math-models.html#exercise-9",
    "title": "AE 04: Mathematical Models",
    "section": "Exercise 9",
    "text": "Exercise 9\nInterpret your confidence interval in the context of the problem."
  },
  {
    "objectID": "ae/ae-04-math-models.html#exercise-10",
    "href": "ae/ae-04-math-models.html#exercise-10",
    "title": "AE 04: Mathematical Models",
    "section": "Exercise 10",
    "text": "Exercise 10\nPredict the average danceability for a song with:\n\nan energy score of 0.859\na loudness score of -5.03\na valence of 0.52\na tempo of 125\n\nReport and interpret a 90% confidence interval for the average danceability. Have your reporter write you interval and estimate on the board."
  },
  {
    "objectID": "ae/ae-04-math-models.html#exercise-11",
    "href": "ae/ae-04-math-models.html#exercise-11",
    "title": "AE 04: Mathematical Models",
    "section": "Exercise 11",
    "text": "Exercise 11\nReport and interpret a 90% confidence interval for a single song with:\n\nan energy score of 0.859\na loudness score of -5.03\na valence of 0.52\na tempo of 125"
  },
  {
    "objectID": "ae/ae-04-math-models.html#exercise-12",
    "href": "ae/ae-04-math-models.html#exercise-12",
    "title": "AE 04: Mathematical Models",
    "section": "Exercise 12",
    "text": "Exercise 12\nWould you expect you interval to get wider or narrower if you were making a prediction for all songs with the above characteristics?\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the HTML file with all of your work from today’s class.\nUpload your QMD and HTML files to the Canvas assignment."
  },
  {
    "objectID": "ae/ae-02-bikeshare.html",
    "href": "ae/ae-02-bikeshare.html",
    "title": "AE 02: Bike rentals in Washington, DC",
    "section": "",
    "text": "Important\n\n\n\n\nOpen RStudio and create a subfolder in your AE folder called “AE-02”\nGo to the Canvas and locate the AE-2 assignment. That link should take you directly there.\nUpload the ae-02.qmd and dcbikeshare.csv files into the folder you just created.\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(ggformula)\nlibrary(patchwork)"
  },
  {
    "objectID": "ae/ae-02-bikeshare.html#data",
    "href": "ae/ae-02-bikeshare.html#data",
    "title": "AE 02: Bike rentals in Washington, DC",
    "section": "Data",
    "text": "Data\nOur data set contains daily rentals from the Capital Bikeshare in Washington, DC in 2011 and 2012. It was obtained from the dcbikeshare data set in the dsbox R package.\nWe will focus on the following variables in the analysis:\n\ncount: total bike rentals\ntemp_orig: Temperature in degrees Celsius\nseason: 1 - winter, 2 - spring, 3 - summer, 4 - fall\n\nClick here for the full list of variables and definitions."
  },
  {
    "objectID": "ae/ae-02-bikeshare.html#exercises",
    "href": "ae/ae-02-bikeshare.html#exercises",
    "title": "AE 02: Bike rentals in Washington, DC",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 0\nThe following code will load the data. Note that you will likely need to change the file path. You can do this easily by deleting the existing path, pressing the TAB button while your cursor is inbetween the quotes, and then selecting the file you want to load.\n\nbikeshare &lt;- read_csv(\"../data/dcbikeshare.csv\")\n\n\n\nExercise 1\nBelow is code to create visualizations of the distributions of daily bike rentals and temperature. Create a scatter plot of the number of rentals vs the temperature and store it in a variable called p3. Make sure to include good appropriate labels. Finally, uncomment the last line of code in this chunk. What do you think this last line does? Comment on what you see.\n\np1 &lt;- gf_histogram(~ count, data = bikeshare, binwidth = 250) |&gt; \n  gf_labs(x = \"Daily bike rentals\")\n\np2 &lt;- gf_histogram(~temp_orig, data = bikeshare) |&gt;  \n  gf_labs(x = \"Temperature (Celsius)\")\n\n# (p1 | p2) / p3\n\n[Add your answer here]\n\n\nExercise 2\nIn the raw data, seasons are coded as 1, 2, 3, 4 as numerical values, corresponding to winter, spring, summer, and fall respectively. Recode the season variable to make it a categorical variable (a factor) with levels corresponding to season names, making sure that the levels appear in a reasonable order in the variable (i.e., not alphabetical).\n\n# add code developed during livecoding here\n\n\n\nExercise 3\nWe want to evaluate whether the relationship between temperature and daily bike rentals is the same for each season. To answer this question, first create a scatter plot of daily bike rentals vs. temperature faceted by season.\n\n# add code developed during livecoding here\n\n\n\nExercise 4\n\nWhich season appears to have the strongest relationship between temperature and daily bike rentals? Why do you think the relationship is strongest in this season?\nWhich season appears to have the weakest relationship between temperature and daily bike rentals? Why do you think the relationship is weakest in this season?\n\n[Add your answer here]\n\n\nExercise 5\nFilter your data for the season with the strongest apparent relationship between temperature and daily bike rentals. Give the resulting data set a new name.\n\n# add code developed during livecoding here\n\n\n\nExercise 6\nGoing back to Exercise 1, there appears to be one day with a very small number of bike rentals. Use filter to figure out what that day was? Why were the number of bike rentals so low on that day? Hint: You can Google the date to figure out what was going on that day.\n\n\nExercise 7\nUsing the subset of the data from Exercise 5, fit a linear model for predicting daily bike rentals from temperature for this season.\n\n# add code developed during livecoding here\n\n\n\nExercise 8\nUse the output to write out the estimated regression equation.\n[Add your answer here]\n\n\nExercise 9\nInterpret the slope in the context of the data.\n\n\nExercise 10\nInterpret the intercept in the context of the data.\n\n\nExercise 11\nCreate a visualization of the model we just created.\n\n# add code developed during livecoding here\n\n\n\nExercise 12\nUsing only addition, subtraction, multiplication, and division. Compute the number of rentals our model would predict if it were 10 degrees Celcius outside.\n\n\nExercise 13\nVerify your answer using the predict function.\n\n# add code developed during livecoding here\n\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the HTML with all of your work from today’s class.\nThe driver for your group should upload your .qmd and .html files to the Canvas assignment."
  },
  {
    "objectID": "ae/ae-01-multivariate-relationships.html",
    "href": "ae/ae-01-multivariate-relationships.html",
    "title": "AE 01: Multivariate Relationships",
    "section": "",
    "text": "Important\n\n\n\n\nFor this exercise, you will work in groups, but everyone will work through the document and submit their solution.\nOpen RStudio and create a subfolder in your AE folder called “AE-01”\nGo to the Canvas and locate your AE 01 assignment to get started.\nUpload the ae-01.qmd files into the folder you just created. The .qmd and .html responses are due in Canvas no later than Monday, January 6th at 1:00pm."
  },
  {
    "objectID": "ae/ae-01-multivariate-relationships.html#quarto",
    "href": "ae/ae-01-multivariate-relationships.html#quarto",
    "title": "AE 01: Multivariate Relationships",
    "section": "Quarto",
    "text": "Quarto\nThe thing you are reading right now is an Quarto document (very similar to R Markdown if you’ve ever used that). Quarto runs inside R Studio. Quarto is a simple formatting syntax for authoring web pages, word documents, pdfs, and many more file types. You can find the link to a useful Quarto guide here.\nWhen you click the Render button at the top of this window an html file will be generated that includes both content as well as the output of any embedded R code chunks within the document. Not only can you embed the R code, you can embed the output produced by the R code. In this way, your analysis is fully reproducible and updatable. All of your homework assignments must be prepared using Quarto.\nThe nice thing about Quarto is that you can write prose (as I am doing now), mathematical equations using LaTeX syntax (like \\(y_i = a + b x_i\\)), and R code/output/plots all in one synthesized document. This makes it approximately 10,000 times easier to use than doing the same thing in Word or LaTeX. As you go through this introduction, I recommend that you look also at the .qmd file and your output file side-by-side to get an idea of how Quarto works. (Since you’re going to have to use Quarto this semester, you might as well start learning it now!)\nIn the future, I will typically give you a Quarto template to fill out. However, occasionally you will need to create your own Quarto document. When you want to start a new Quarto document, click “File &gt; New File &gt; Quarto Document…” Put yourself as author, and make sure to give it a descriptive title!\n\nExercise 1\nClick Render at the top of this document. Show Dr. Friedlander before moving on."
  },
  {
    "objectID": "ae/ae-01-multivariate-relationships.html#using-r-as-a-calculator-and-running-code-within-a-quarto-document",
    "href": "ae/ae-01-multivariate-relationships.html#using-r-as-a-calculator-and-running-code-within-a-quarto-document",
    "title": "AE 01: Multivariate Relationships",
    "section": "Using R as a Calculator and Running Code within a Quarto Document",
    "text": "Using R as a Calculator and Running Code within a Quarto Document\nWith R by your side, you will never need your TI-84 again. Consider the simple arithmetic in the chunk below. You can run this code chunk all at once by clicking the Run button, the sideways green arrow in the top right of the chunk. Notice the interactive nature of the output. You can click through to see the different pieces of output.\nAs you work through this document, you should Run each chunk as you come to it.\n\nExercise 2\nRun the code chunk below.\n\n5 + 3\n\n[1] 8\n\n5.3 * 23.4\n\n[1] 124.02\n\nsqrt(16) # this is a comment.  R will not 'see' anything past a #, but you can use it to explain your code\n\n[1] 4\n\n\nLook closely at how Quarto denotes the R code and the output. Also note in the .qmd file how I include R code as separate from prose. These are called “chunks”. The easiest way to add a new chunk is to click on the green “c” icon with a plus in the corner above, then choose “R”. R code that is not inside of a chunk will not be run by Quarto!\n\n\nExercise 3\nCreate an R code chunk and compute the log of 10 using the function log. Why do you think the answer isn’t 1?\n\n\nExercise 4\nYou can also save values to named variables, to be used later:\n\nproduct &lt;- 15.3 * 23.4 #save result\n\nIf you save something like this, R will not show the output unless you expressly ask for it:\n\nproduct #show result\n\n[1] 358.02\n\n\nThe symbol “&lt;-” is the assignment operator. If you’ve ever programmed before, it’s essentially the same as “=” in this instance. There are cases where “&lt;-” works and “=” doesn’t, so it’s good to get into the habit of using “&lt;-” now.\nOnce variables are defined, they can be referenced with other operators and functions. Try executing each line of code individually by placing your cursor on the first line of the chunk below and pressing Ctrl+Enter (Cmd +Enter for Mac users); then do the same for the second line. (This is how you can run a single line within a larger chunk.)\n\n.5 * product # half of the product\n\n[1] 179.01\n\nproduct^(1/4) # fourth root of the product\n\n[1] 4.349875\n\n\n\n\nExercise 5\nYou can also use in-line R code in Quarto, which can be useful when calling defined variables. Did you know that the natural log of 358.02 is 5.8805889?\n\n\nExercise 6\nOnly R code (and comments) should be inside chunks. Prose (interpretations/explanations/descriptions) should never be put inside a chunk; prose should be below or above the chunk, as I have done above (and continue to do throughout this document). You should also never cut-and-paste output or graphs into the chunks. The whole point of code chunks is that they contain the code and they’ll run the code (resulting in the output and/or graphs).\nIf you want to run something in R but don’t want it to appear in the Quarto document, simply run it in the Console in the lower-left quadrant of RStudio. Type the last line of R code above into the Console and see what happens."
  },
  {
    "objectID": "ae/ae-01-multivariate-relationships.html#carbohydrates-and-starbucks",
    "href": "ae/ae-01-multivariate-relationships.html#carbohydrates-and-starbucks",
    "title": "AE 01: Multivariate Relationships",
    "section": "Carbohydrates and Starbucks",
    "text": "Carbohydrates and Starbucks\nStarbucks often displays the total calories in their food items but not the other nutritional information. Carbohydrates are a body’s main fuel source. The Dietary Guidelines for America recommend that carbohydrates make up 45% to 65% of total daily calories. Our goal is to understand the relationship between the amount of carbohydrates and calories in Starbucks food items. We’d also like to assess if the relationship differs based on the type of food item (bakery, salad, sandwich, etc.)\nYou will now be introduced to the functions we will use most frequently for exploratory data analysis (EDA) in this class. If you are moderate to advanced R user, feel free to use whatever functions you’d like to accomplish the tasks. However, you should be prepared to answer questions about any uncited code which is not covered in this class on your oral exams. The first part of this document contains explanations for beginning R users, but all students should work through the entire document.\n\n\n\n\n\n\nImportant\n\n\n\nIn general, I suggest “rendering as you go”: rendering every few chunks, to make sure things are rendering correctly, rather than waiting until the end of a document to render the whole thing (and potentially encountering lots of errors that you have to unpack)! I also ALWAYS suggest “saving as you go”: it’s a good idea to save every couple of minutes. (This is good practice for all your files.)\n\n\n\nExercise 7\nThe code below loads the package called tidyverse which provides many functions that will be useful for manipulating data and the package called mosaic which is useful for teaching statistics. Add another line that loads a package called ggformula which we will use to plot our data and a third that loads the package openintro which will contain our data for this activity. Note that #| warning: FALSE causes the code to run without displaying any associated warnings when you render. You will often see #| eval: FALSE which prevents the code chunk from running when you render the document. Throughout this course, make sure you remove these lines or change them to #| eval: TRUE whenever you reach them. The reason that are included in the first place is so I can write scaffolded and fill-in-the-blank type exercises for you. These will cause errors (since they are incomplete code) until you finish writing them.\n\n# load packages we typically use for this class.\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(_________) # load ggformula\n# load openintro"
  },
  {
    "objectID": "ae/ae-01-multivariate-relationships.html#loading-data",
    "href": "ae/ae-01-multivariate-relationships.html#loading-data",
    "title": "AE 01: Multivariate Relationships",
    "section": "Loading data",
    "text": "Loading data\nDoing statistics requires data. Sometimes we will have to load data from a file and sometimes we can get our data from an R package. Today’s dataset, called starbucks is from the openintro package. To load a this data from within R you can use the data command.\n\nExercise 8\nUse the code below to load the dataset starbucks, which is part of the openintro package. Don’t forget to change eval: FALSE to eval: TRUE… Why do you think we need eval: FALSE if you aren’t actually rewriting any code?\n\ndata(\"starbucks\")\n\nThis is a great chance to remind you that R cares about letter case. This means that data(\"starbucks\") works but data(\"Starbucks\") doesn’t!\n\n\nInspecting the data source\nNow you’re ready to learn a little bit about the starbucks data set.\n\n\nExercise 9\nRun the code below and then edit the bulleted list below to add a short description in your own words describing what each function does. Hint: if it isn’t clear based on the output you can use ? before the function name in the console to bring up the documentation on that function. Notify Dr. Friedlander when you’re finished.\n\n# Inspecting the data source\nglimpse(starbucks)\nhead(starbucks)\nnames(starbucks)\n\n\nglimpse() : this function…\nhead() : this function…\nnames() : this function…\n\n\n\nExercise 10\nBased on the output above, how many observations and how many variables does this data set have?\n\n\nExercise 11\nLet’s do a little bit of a data prep. In the console type ?starbucks and look at the definition of type.\nThe following is a little bit of data wrangling to get the source data in shape for our purposes. What do you think this code is doing? Make sure you change eval: FALSE to eval: TRUE.\n\nstarbucks &lt;- starbucks |&gt; \n  mutate(bakery = factor(if_else(type == \"bakery\", \"bakery\", \"non-bakery\")))\n\nStatisticians should always know something about the data domain in order to be useful. Why do you think we are changing our data like this?"
  },
  {
    "objectID": "ae/ae-01-multivariate-relationships.html#exploratory-data-analysis",
    "href": "ae/ae-01-multivariate-relationships.html#exploratory-data-analysis",
    "title": "AE 01: Multivariate Relationships",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nFor the purposes of our class, it’s useful to learn a model-centric approach to R. The pseudo-code below is going to be our foundation for the rest of the class:\nfunction( Y ~ X, data = DataSetName )\nHere’s a short description of each part in the pseudo-code above:\n\nfunction is an R function that dictates something you want to do with your data; for example,\n\nmean calculates the mean\nlm fits a linear regression model\n\nY is the outcome of interest (response variable)\nX is some explanatory variable; you can use 1 as a placeholder if there is no explanatory variable\nDataSetName is the name of a data set loaded into the R environment\n\nAlways start with clear research questions. Our question for this exercise:\n\nHow do the number of carbs compare for bakery and non-bakery items?\n\nThe purpose of the exploratory data analysis (EDA) is to learn as much as you can about your research question before doing any statistical modeling. We basically want to try and answer the research question with EDA if possible…or at least have a guess as to what the answer “should” be. Then we use statistical models to formally accommodate variability in the data and calculate the uncertainty of our conclusions.\n\nExercise 12\nUse the R code chunk below to calculate the mean carbs by for bakery and non-bakery items. Summarize your observations below the code chunk.\n\nmean(carb ~ bakery, data = starbucks)\n\n\n\nExercise 13\nOf course, there are lots of other ways to summarize a numerical variable besides the mean. Use the R code chunk to calculate the other summary statistics for the number of carbs for bakery and non-bakery items using the function favstats. Summarize your observations below the code chunk.\n(Hint: Do you not know how favstats works? Well, it’s a function just like any other: it follows the syntax described at the top of this section! Also, you can always find details and examples by searching the Help menu in the lower-right quadrant.)\n\n\nExercise 14\nLet’s generate some univariate (i.e. one-variable) EDA plots for our three variables. In general, can use histograms or boxplots (gf_histogram or gf_boxplot) for quantitative variables and bar charts gf_bar for categorical variables. Fill in the blanks in the code below to generate the corresponding plots. Then create a boxplot of carb. Finally, share your observations below the code:\n\n# make a histogram of carb\ngf_histogram(~ carb, data = starbucks)\n\n# make a histogram of calories\ngf_histgoram(~ _____, data = ______)\n\n# make a bar chart of bakery\ngf_bar(~ _____, data = _____)\n\n# make a boxplot of carb\n\n\n\nExercise 15\nLet’s not generate some bivariate (i.e. two-variable) EDA plots for our three variables. For two-quantitative variables we typically use a scatter plot gf_point. If we are comparing a quantitative and a categorical variable will use side-by-side boxplots gf_boxplot. Generate and plot to visualize carb and calories and one to visualize carb and bakery. Comment on your observations."
  },
  {
    "objectID": "ae/ae-03-sbi.html",
    "href": "ae/ae-03-sbi.html",
    "title": "AE 03: Simulation-Based Inference",
    "section": "",
    "text": "Important\n\n\n\n\nOpen RStudio and create a subfolder in your AE folder called “AE-03”\nGo to the Canvas and locate your AE 03 assignment to get started.\nUpload the ae-03-sbi.qmd and dcbikeshare.csv files into the folder you just created.\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(broom)\nlibrary(openintro)\nlibrary(kableExtra)"
  },
  {
    "objectID": "ae/ae-03-sbi.html#data",
    "href": "ae/ae-03-sbi.html#data",
    "title": "AE 03: Simulation-Based Inference",
    "section": "Data",
    "text": "Data\nOur dataset contains daily rentals from the Capital Bikeshare in Washington, DC in 2011 and 2012. It was obtained from the dcbikeshare data set in the dsbox R package.\nWe will focus on the following variables in the analysis:\n\ncount: total bike rentals\ntemp_orig: Temperature in degrees Celsius\n\n\nbikeshare &lt;- read_csv(\"../data/dcbikeshare.csv\") |&gt;\n  mutate(season = case_when(\n    season == 1 ~ \"winter\",\n    season == 2 ~ \"spring\",\n    season == 3 ~ \"summer\",\n    season == 4 ~ \"fall\"\n  ),\n  season = factor(season))\n\nRows: 731 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (16): instant, season, yr, mnth, holiday, weekday, workingday, weathers...\ndate  (1): dteday\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(bikeshare)\n\nRows: 731\nColumns: 17\n$ instant    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ dteday     &lt;date&gt; 2011-01-01, 2011-01-02, 2011-01-03, 2011-01-04, 2011-01-05…\n$ season     &lt;fct&gt; winter, winter, winter, winter, winter, winter, winter, win…\n$ yr         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ mnth       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ holiday    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,…\n$ weekday    &lt;dbl&gt; 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4,…\n$ workingday &lt;dbl&gt; 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,…\n$ weathersit &lt;dbl&gt; 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2,…\n$ temp       &lt;dbl&gt; 0.3441670, 0.3634780, 0.1963640, 0.2000000, 0.2269570, 0.20…\n$ atemp      &lt;dbl&gt; 0.3636250, 0.3537390, 0.1894050, 0.2121220, 0.2292700, 0.23…\n$ hum        &lt;dbl&gt; 0.805833, 0.696087, 0.437273, 0.590435, 0.436957, 0.518261,…\n$ windspeed  &lt;dbl&gt; 0.1604460, 0.2485390, 0.2483090, 0.1602960, 0.1869000, 0.08…\n$ casual     &lt;dbl&gt; 331, 131, 120, 108, 82, 88, 148, 68, 54, 41, 43, 25, 38, 54…\n$ registered &lt;dbl&gt; 654, 670, 1229, 1454, 1518, 1518, 1362, 891, 768, 1280, 122…\n$ count      &lt;dbl&gt; 985, 801, 1349, 1562, 1600, 1606, 1510, 959, 822, 1321, 126…\n$ temp_orig  &lt;dbl&gt; 14.110847, 14.902598, 8.050924, 8.200000, 9.305237, 8.37826…"
  },
  {
    "objectID": "ae/ae-03-sbi.html#exercises",
    "href": "ae/ae-03-sbi.html#exercises",
    "title": "AE 03: Simulation-Based Inference",
    "section": "Exercises",
    "text": "Exercises\nIn this activity, each group will be assigned a season to use:\n\nGroup 1: winter\nGroup 2: spring\nGroup 3: summer\nGroup 4: fall\n\n\nExercise 0\nFilter the bikeshare data set so that it only contains observations from your assigned season. Make sure to give the new data set a different name.\n\n\nExercise 1\nConduct a little EDA. Generate a scatter plot of the number of bike rentals vs the temperature for your season by filling in the blanks in the code below. What do you think alpha does?\n\ngf______(______, data = ______, alpha = 0.7) |&gt; \n  gf_labs(\n    x = \"Temperature (C)\",\n    y = \"Number of Bike Rentals\",\n  )\n\n\n\nExercise 2\nFit a simple linear regression model, display the results to two decimal places, and be prepared to discuss the interpretation of the resulting slope and intercept.\n\nmodel_fit &lt;- lm(_____ ~ ______, data = _____)\n\ntidy(_____) |&gt;\n  kable(digits = _____)\n\n\n\nExercise 3\nLoad the infer package and calculate the observed fit (slope)\n\nobserved_fit &lt;- ____ |&gt;\n  specify(count ~ temp_orig) |&gt;\n  fit()\n\nobserved_fit\n\n\n\nExercise 4\nTake n bootstrap samples and fit models to each one.\nFill in the code, then set eval: true .\n\nn &lt;- 100\nset.seed(212)\n\nboot_fits &lt;- ______ |&gt;\n  specify(______) |&gt;\n  generate(reps = ____, type = \"bootstrap\") |&gt;\n  fit()\n\nboot_fits\n\n\nWhy do we set a seed before taking the bootstrap samples?\n\n\n\nExercise 5 (Challenging)\nMake a histogram of the bootstrap samples to visualize the bootstrap distribution.\n\n# Code for histogram\n\n\n\nExercise 6\nCompute and interpret the 95% confidence interval as the middle 95% of the bootstrap distribution.\nFill in the code, then set eval: true .\n\nget_confidence_interval(\n  boot_fits,\n  point_estimate = _____,\n  level = ____,\n  type = \"percentile\"\n)\n\n\n\nExercise 7\nModify the code from Exercise 6 to create a 90% confidence interval.\n\n# Paste code for 90% confidence interval\n\n\n\nExercise 8\nModify the code from Exercise 6 to create a 90% confidence interval.\n\n# Paste code for 90% confidence interval\n\n\n\nExercise 9\n\nWhich confidence level produces the most accurate confidence interval (90%, 95%, 99%)? Explain\nWhich confidence level produces the most precise confidence interval (90%, 95%, 99%)? Explain\nIf we want to be very certain that we capture the population parameter, should we use a wider or a narrower interval? What drawbacks are associated with using a wider interval?\n\n\n\nExercise 10\n\nIf your sample size \\(n\\) is increased, what impact do you think this will have on accuracy and precision?\nWhat about if you increase the number of bootstrapped replicates?\n\n\n\nExercise 11\nYour professor is interested in calculating the average amount of time CofI students spend doing homework.\n\nIf he collects a set of data and asks 100 students to compute 95% confidence intervals from that data, how many of those would you expect to contain the true average?\nIf, instead, he has each of those 100 students collect their own data and compute 95% confidence intervals from their own data, how many would you expect to contain the true average?\n\n\n\nExercise 12\nWrite down your research question in words then state the null and alternative hypotheses in both words and mathematical notation. You can use dollar signs to engage “math mode”.\n\\[\nH_0:\n\\]\n\n\nExercise 13\nGenerate null distribution using permutation\nFill in the code, then set eval: true .\n\nn = 100\nset.seed(212)\n\nnull_dist &lt;- _____ |&gt;\n  specify(______) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = _____, type = \"permute\") |&gt;\n  fit()\n\n\n\nExercise 14\nVisualize the null distribution. Does your slope seem unusual if we assume the null hypothesis is the truth?\n\n# Code for histogram of null distribution\n\n\n\nExercise 15\nFill in the code below to compute the p-value. Have your reporter write the value on the board.\n\n# get observed fit \nobserved_fit &lt;- _____ |&gt;\n  specify(____) |&gt;\n  fit()\n\n# calculate p-value\nget_p_value(\n  ____,\n  obs_stat = ____,\n  direction = \"two-sided\"\n)\n\n\n\nExercise 16\n\nDo larger or smaller p-values provide evidence for the alternative hypothesis?\nDo larger or smaller p-values provide evidence for your research question?\nInterpret your p-value in the context of the problem. Do you think your data provides strong evidence for your research question?\n\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\nRender the document to produce the HTML with all of your work from today’s class.\nThe driver for your group should upload your .qmd and .html files to the Canvas assignment."
  },
  {
    "objectID": "ae/ae-05-conditions.html",
    "href": "ae/ae-05-conditions.html",
    "title": "AE 05: Model Conditions and Evaluation",
    "section": "",
    "text": "Important\n\n\n\n\nOpen RStudio and create a subfolder in your AE folder called “AE-05”\nGo to the Canvas and locate your AE 05 assignment to get started.\nUpload the ae-05.qmd and spotify-popular.csv files into the folder you just created.\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(broom)\nlibrary(knitr)\nlibrary(patchwork) #arrange plots in a grid"
  },
  {
    "objectID": "ae/ae-05-conditions.html#introduction",
    "href": "ae/ae-05-conditions.html#introduction",
    "title": "AE 05: Model Conditions and Evaluation",
    "section": "Introduction",
    "text": "Introduction\nThis is a continuation of AE-04. The Data section below is the same as in that exercise. Feel free to skip it if you feel you remember everything about the data set or simply use it as a reference when needed."
  },
  {
    "objectID": "ae/ae-05-conditions.html#data",
    "href": "ae/ae-05-conditions.html#data",
    "title": "AE 05: Model Conditions and Evaluation",
    "section": "Data",
    "text": "Data\nThe data set for this assignment is a subset from the Spotify Songs Tidy Tuesday data set. The data were originally obtained from Spotify using the spotifyr R package.\nIt contains numerous characteristics for each song. You can see the full list of variables and definitions here. This analysis will focus specifically on the following variables:\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ntrack_id\ncharacter\nSong unique ID\n\n\ntrack_name\ncharacter\nSong Name\n\n\ntrack_artist\ncharacter\nSong Artist\n\n\ntrack_popularity\ndouble\nSong Popularity (0-100) where higher is better\n\n\ndanceability\ndouble\nDanceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.\n\n\nenergy\ndouble\nEnergy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\n\n\nloudness\ndouble\nThe overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db.\n\n\nvalence\ndouble\nA measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\n\n\ntempo\ndouble\nThe overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.\n\n\nduration_ms\ndouble\nDuration of song in milliseconds\n\n\n\n\nspotify &lt;- read_csv(\"../data/spotify-popular.csv\")\n\nWhat makes a song danceable? To answer this question, we’ll analyze data on some of the most popular songs on Spotify, i.e. those with track_popularity &gt;= 80. We’ll use linear regression to fit some models to predict a song’s dancebility. Each group will be assigned a different predictor variable.\n\nPredictor Assignment:\nBelow is your assigned predictor. Look at the table above for a definition.\n\nGroup 1: energy\nGroup 2: loudness\nGroup 3: valence\nGroup 4: tempo"
  },
  {
    "objectID": "ae/ae-05-conditions.html#exercise-0",
    "href": "ae/ae-05-conditions.html#exercise-0",
    "title": "AE 05: Model Conditions and Evaluation",
    "section": "Exercise 0",
    "text": "Exercise 0\nFit a model using your assigned explanatory variable to predict a songs danceability. Use tidy and kable to neatly display your model and have your reporter write you model on the white board along with a confidence interval and p-value for the slope. Be prepared to discuss the interpretation of these quantities.\n\n## add code"
  },
  {
    "objectID": "ae/ae-05-conditions.html#exercise-1",
    "href": "ae/ae-05-conditions.html#exercise-1",
    "title": "AE 05: Model Conditions and Evaluation",
    "section": "Exercise 1",
    "text": "Exercise 1\nUse the augment function from the broom package to create an augmented data frame from the model you fit in Exercise 0."
  },
  {
    "objectID": "ae/ae-05-conditions.html#exercise-2",
    "href": "ae/ae-05-conditions.html#exercise-2",
    "title": "AE 05: Model Conditions and Evaluation",
    "section": "Exercise 2",
    "text": "Exercise 2\nMake a plot of the residual vs. fitted values."
  },
  {
    "objectID": "ae/ae-05-conditions.html#exercise-3",
    "href": "ae/ae-05-conditions.html#exercise-3",
    "title": "AE 05: Model Conditions and Evaluation",
    "section": "Exercise 3",
    "text": "Exercise 3\nTo what extent does the linearity condition appear to be met for your data?"
  },
  {
    "objectID": "ae/ae-05-conditions.html#exercise-4",
    "href": "ae/ae-05-conditions.html#exercise-4",
    "title": "AE 05: Model Conditions and Evaluation",
    "section": "Exercise 4",
    "text": "Exercise 4\nTo what extent does the constant variance assumption appear to be met for your data?"
  },
  {
    "objectID": "ae/ae-05-conditions.html#exercise-5",
    "href": "ae/ae-05-conditions.html#exercise-5",
    "title": "AE 05: Model Conditions and Evaluation",
    "section": "Exercise 5",
    "text": "Exercise 5\nFill in the code to make a histogram of the residuals and a normal QQ-plot.\n\nresid_hist &lt;- gf_histogram(~____, data = ____)) |&gt; \n  gf_labs(x = \"_____\", \n       y = \"_____\", \n       title = \"____\")\n\nresid_qq &lt;- gf_qq(~____, data = ____)  |&gt;\n  gf_qqline() |&gt; \n  gf_labs(x = \"_____\", \n       y = \"_____\", \n       title = \"____\")\n\nresid_hist + resid_qq"
  },
  {
    "objectID": "ae/ae-05-conditions.html#exercise-6",
    "href": "ae/ae-05-conditions.html#exercise-6",
    "title": "AE 05: Model Conditions and Evaluation",
    "section": "Exercise 6",
    "text": "Exercise 6\nTo what extent does the normality condition of your data appear to be met?"
  },
  {
    "objectID": "ae/ae-05-conditions.html#exercise-7",
    "href": "ae/ae-05-conditions.html#exercise-7",
    "title": "AE 05: Model Conditions and Evaluation",
    "section": "Exercise 7",
    "text": "Exercise 7\nOur data was collected from Spotify… to what extent do you think the independence condition is met? Do you think there is a serial effect? Do you think there is a cluster effect?"
  },
  {
    "objectID": "ae/ae-05-conditions.html#exercise-8",
    "href": "ae/ae-05-conditions.html#exercise-8",
    "title": "AE 05: Model Conditions and Evaluation",
    "section": "Exercise 8",
    "text": "Exercise 8\nUsing the anova function, compute the SSE, SSM, and SST from your model. Note that the function will give you two of those values and you’ll have to use subtraction to compute the other two."
  },
  {
    "objectID": "ae/ae-05-conditions.html#exercise-9",
    "href": "ae/ae-05-conditions.html#exercise-9",
    "title": "AE 05: Model Conditions and Evaluation",
    "section": "Exercise 9",
    "text": "Exercise 9\nLoad the yardstick package and compute the \\(R^2\\) of your model. Then verify that you get the same number when you divide your SSM by your SST. How do you interpret this value? Have your reporter write the resulting value on the board."
  },
  {
    "objectID": "ae/ae-05-conditions.html#exericse-10",
    "href": "ae/ae-05-conditions.html#exericse-10",
    "title": "AE 05: Model Conditions and Evaluation",
    "section": "Exericse 10",
    "text": "Exericse 10\nCompute the RMSE for your model. Interpret this value."
  },
  {
    "objectID": "ae/ae-05-conditions.html#exercise-11",
    "href": "ae/ae-05-conditions.html#exercise-11",
    "title": "AE 05: Model Conditions and Evaluation",
    "section": "Exercise 11",
    "text": "Exercise 11\nWhich group has the best model?\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the HTML file with all of your work from today’s class.\nUpload your QMD and HTML files to the Canvas assignment."
  },
  {
    "objectID": "ae/ae-07-mlr.html",
    "href": "ae/ae-07-mlr.html",
    "title": "AE 07: Multiple Linear Regression",
    "section": "",
    "text": "Important\n\n\n\n\nOpen RStudio and create a subfolder in your AE folder called “AE-07”.\nGo to the Canvas and locate your AE-07 assignment to get started.\nUpload the ae-07.qmd file into the folder you just created."
  },
  {
    "objectID": "ae/ae-07-mlr.html#packages-data",
    "href": "ae/ae-07-mlr.html#packages-data",
    "title": "AE 07: Multiple Linear Regression",
    "section": "Packages + data",
    "text": "Packages + data\n\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(broom)\nlibrary(mosaic)\nlibrary(knitr)\nlibrary(ISLR2)\nlibrary(GGally)\nlibrary(yardstick)\n\nThe data for this AE is from the Credit data set in the ISLR2 R package. It is a simulated data set of 400 credit card customers. We will focus on the following variables:\nResponse\n\nLimit: Credit limit\n\nPredictors\n\nEveryone:\n\nRating: Credit Rating\n\nGroup 1:\n\nAge\nOwn\n\nGroup 2:\n\nBalance\nStudent\n\nGroup 3:\n\nEducation\nRegion\n\nGroup 4:\n\nIncome\nMarried"
  },
  {
    "objectID": "ae/ae-07-mlr.html#analysis-goal",
    "href": "ae/ae-07-mlr.html#analysis-goal",
    "title": "AE 07: Multiple Linear Regression",
    "section": "Analysis goal",
    "text": "Analysis goal\nThe goals of this analysis is to fit a multiple linear regression model that that predicts a person’s credit limit."
  },
  {
    "objectID": "ae/ae-07-mlr.html#exercise-0",
    "href": "ae/ae-07-mlr.html#exercise-0",
    "title": "AE 07: Multiple Linear Regression",
    "section": "Exercise 0",
    "text": "Exercise 0\nLook at the documentation for the data using ?Credit and Google to figure out what each of your variables represents. What is a credit rating and what is a credit limit as it applies to a credit card? The primary credit rating in the US is called a FICO score. Use favstats on Rating. Do you think that Rating corresponds to the borrower’s FICO score?\nWithout writing any more code (i.e. just using what you know about the world):\n\nDo you think Rating will be a good predictor of Limit?\nIn addition to Limit, everyone has been assigned two additional variables, one quantitative and one categorical. Do you think these two variables will be predictive of Limit?"
  },
  {
    "objectID": "ae/ae-07-mlr.html#exercise-1",
    "href": "ae/ae-07-mlr.html#exercise-1",
    "title": "AE 07: Multiple Linear Regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nUse the function ggpairs from the GGally package (already loaded) to create a matrix of plots and correlations for your four variables of interest. Note that you will have to use select to select the four columns you are interested in. Which variable do you think will be the best predictor of Limit?\n\nCredit |&gt; \n  select(Limit, Rating) |&gt; # add your extra variables here.\n  ggpairs()"
  },
  {
    "objectID": "ae/ae-07-mlr.html#exercise-2",
    "href": "ae/ae-07-mlr.html#exercise-2",
    "title": "AE 07: Multiple Linear Regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nFit a linear model with just Rating as the predictor and get the p-value associated with it’s coefficient. Is it statistically significant?"
  },
  {
    "objectID": "ae/ae-07-mlr.html#exercise-3",
    "href": "ae/ae-07-mlr.html#exercise-3",
    "title": "AE 07: Multiple Linear Regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nFit a linear model with just your second quantitative predictor and get the p-value associated with it’s coefficient. Is it statistically significant?"
  },
  {
    "objectID": "ae/ae-07-mlr.html#exercise-4",
    "href": "ae/ae-07-mlr.html#exercise-4",
    "title": "AE 07: Multiple Linear Regression",
    "section": "Exercise 4",
    "text": "Exercise 4\nFit a model with both Income and your second quantitative variable as predictors. Find a spot on the white board to write down an equation representing the fitted model. How do the coefficients and p-values of Income and Rating compare to those in the two models above? Discuss what you see and the possible reasons you see them."
  },
  {
    "objectID": "ae/ae-07-mlr.html#exercise-5",
    "href": "ae/ae-07-mlr.html#exercise-5",
    "title": "AE 07: Multiple Linear Regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nInterpret all coefficients in the model."
  },
  {
    "objectID": "ae/ae-07-mlr.html#exercise-6",
    "href": "ae/ae-07-mlr.html#exercise-6",
    "title": "AE 07: Multiple Linear Regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nUse head to look at the first observation in the data set. Use addition, subtraction, multiplication, and division to figure out what credit limit your model would predict for a person with the same characteristics of this person."
  },
  {
    "objectID": "ae/ae-07-mlr.html#exercise-7",
    "href": "ae/ae-07-mlr.html#exercise-7",
    "title": "AE 07: Multiple Linear Regression",
    "section": "Exercise 7",
    "text": "Exercise 7\nNow use the predict function to verify the value you computed in the previous exercise and compute a 90% prediction interval. How would you interpret this interval in context?"
  },
  {
    "objectID": "ae/ae-07-mlr.html#exercise-8",
    "href": "ae/ae-07-mlr.html#exercise-8",
    "title": "AE 07: Multiple Linear Regression",
    "section": "Exercise 8",
    "text": "Exercise 8\nFit a linear model predicting Limit from ONLY your categorical variable. What is the reference level?"
  },
  {
    "objectID": "ae/ae-07-mlr.html#exercise-9",
    "href": "ae/ae-07-mlr.html#exercise-9",
    "title": "AE 07: Multiple Linear Regression",
    "section": "Exercise 9",
    "text": "Exercise 9\nWITHOUT WRITING ANY CODE except for addition, subtraction, multiplication, and addition, what would the model predict the average Limit to be for each of the level of your categorical variable?"
  },
  {
    "objectID": "ae/ae-07-mlr.html#exercise-10",
    "href": "ae/ae-07-mlr.html#exercise-10",
    "title": "AE 07: Multiple Linear Regression",
    "section": "Exercise 10",
    "text": "Exercise 10\nCompute the average Limit for each level in your categorical variable. Hint: you can use “formulas” (i.e. repsonse ~ explanatory) in the mean function from the mosaic package. How do these answers compare to your answers from Exercise 9?"
  },
  {
    "objectID": "ae/ae-07-mlr.html#exercise-11",
    "href": "ae/ae-07-mlr.html#exercise-11",
    "title": "AE 07: Multiple Linear Regression",
    "section": "Exercise 11",
    "text": "Exercise 11\nFit a linear model using your quantitative and your categorical variable as predictors (don’t use Rating). Use the function plotModel to make a nifty plot for your model. Your syntax should look like plotModel(model_name)."
  },
  {
    "objectID": "ae/ae-07-mlr.html#exercise-12",
    "href": "ae/ae-07-mlr.html#exercise-12",
    "title": "AE 07: Multiple Linear Regression",
    "section": "Exercise 12",
    "text": "Exercise 12\nYour model contains one line for every level of your categorical variable. On the board, write out the equations for two of them."
  },
  {
    "objectID": "ae/ae-07-mlr.html#exercise-13",
    "href": "ae/ae-07-mlr.html#exercise-13",
    "title": "AE 07: Multiple Linear Regression",
    "section": "Exercise 13",
    "text": "Exercise 13\nHow do you think the plot above will change if you add in an interaction term between your variables? AFTER thinking about it, add in an in the interaction term and plot the model using plotModel."
  },
  {
    "objectID": "ae/ae-07-mlr.html#exercise-14",
    "href": "ae/ae-07-mlr.html#exercise-14",
    "title": "AE 07: Multiple Linear Regression",
    "section": "Exercise 14",
    "text": "Exercise 14\nYour model contains one line for every level of your categorical variable. On the board, write out the equations for two of them."
  },
  {
    "objectID": "ae/ae-07-mlr.html#exercise-15",
    "href": "ae/ae-07-mlr.html#exercise-15",
    "title": "AE 07: Multiple Linear Regression",
    "section": "Exercise 15",
    "text": "Exercise 15\nFit a model with your quantitative predictor, Rating, and an interaction term. Write your model on the board."
  },
  {
    "objectID": "ae/ae-07-mlr.html#exercise-16-optional",
    "href": "ae/ae-07-mlr.html#exercise-16-optional",
    "title": "AE 07: Multiple Linear Regression",
    "section": "Exercise 16 (Optional)",
    "text": "Exercise 16 (Optional)\nLook back at all of the models that you’ve fit. If you compare the p-values for the same variable across multiple models, what do you notice?"
  },
  {
    "objectID": "ae/ae-07-mlr.html#exercise-17-optional",
    "href": "ae/ae-07-mlr.html#exercise-17-optional",
    "title": "AE 07: Multiple Linear Regression",
    "section": "Exercise 17 (Optional)",
    "text": "Exercise 17 (Optional)\nNote that this data set only considers borrowers who have actually been granted loans. How does this impact the generalizability of our analysis?"
  },
  {
    "objectID": "ae/ae-07-mlr.html#to-submit-the-ae",
    "href": "ae/ae-07-mlr.html#to-submit-the-ae",
    "title": "AE 07: Multiple Linear Regression",
    "section": "To submit the AE",
    "text": "To submit the AE\n\n\n\n\n\n\nImportant\n\n\n\n\nRender the document to produce the HTML file with all of your work from today’s class.\nUpload your QMD and HTML files to the Canvas assignment."
  },
  {
    "objectID": "ae/ae-09-logistic-intro.html",
    "href": "ae/ae-09-logistic-intro.html",
    "title": "AE 09: Logistic regression introduction",
    "section": "",
    "text": "Important\n\n\n\n\nOpen RStudio and create a subfolder in your AE folder called “AE-09”.\nGo to the Canvas and locate your AE-09 assignment to get started.\nUpload the ae-09.qmd and framingham.csv files into the folder you just created."
  },
  {
    "objectID": "ae/ae-09-logistic-intro.html#packages",
    "href": "ae/ae-09-logistic-intro.html#packages",
    "title": "AE 09: Logistic regression introduction",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(knitr)\n\nheart_disease &lt;- read_csv(\"../data/framingham.csv\") |&gt;\n  select(totChol, TenYearCHD, age, BMI, cigsPerDay, heartRate)\n\n\nData: Framingham study\nThis data set is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to use the total cholesterol to predict if a randomly selected adult is high risk for heart disease in the next 10 years.\n\n\nResponse variable\n\nTenYearCHD:\n\n1: Patient developed heart disease within 10 years of exam\n0: Patient did not develop heart disease within 10 years of exam\n\n\n\n\nWhat’s my predictor variable?\nBased on your group, use the following as your predictor variable.\n\nGroup 1 - totChol: total cholesterol (mg/dL)\nGroup 2 -BMI: patient’s body mass index\nGroup 3 -cigsPerDay: number of cigarettes patient smokes per day\nGroup 4 -heartRate: Heart rate (beats per minute)"
  },
  {
    "objectID": "ae/ae-09-logistic-intro.html#exercise-0",
    "href": "ae/ae-09-logistic-intro.html#exercise-0",
    "title": "AE 09: Logistic regression introduction",
    "section": "Exercise 0",
    "text": "Exercise 0\nLook up the drop_na function. Describe how it works. Drop any rows with missing data in TenYearCHD or your aassigned predictor."
  },
  {
    "objectID": "ae/ae-09-logistic-intro.html#exercise-1",
    "href": "ae/ae-09-logistic-intro.html#exercise-1",
    "title": "AE 09: Logistic regression introduction",
    "section": "Exercise 1",
    "text": "Exercise 1\nGenerate a plot and table of TenYearCHD, and a plot to visualize the relationship between TenYearCHD and totChol. Hint: none of these should be scatterplots."
  },
  {
    "objectID": "ae/ae-09-logistic-intro.html#exercise-2",
    "href": "ae/ae-09-logistic-intro.html#exercise-2",
    "title": "AE 09: Logistic regression introduction",
    "section": "Exercise 2",
    "text": "Exercise 2\nGenerate a scatterplot of totChol vs. TenYearCHD. Use gf_lm() to add a line to your plot. Do you think this is a good model? Why or why not?"
  },
  {
    "objectID": "ae/ae-09-logistic-intro.html#exercise-3",
    "href": "ae/ae-09-logistic-intro.html#exercise-3",
    "title": "AE 09: Logistic regression introduction",
    "section": "Exercise 3",
    "text": "Exercise 3\nState whether a linear regression model or logistic regression model is more appropriate for each of your research projects."
  },
  {
    "objectID": "ae/ae-09-logistic-intro.html#exercise-4",
    "href": "ae/ae-09-logistic-intro.html#exercise-4",
    "title": "AE 09: Logistic regression introduction",
    "section": "Exercise 4",
    "text": "Exercise 4\nUsing the table you generated in Exercise 1 (answer 1-3 on the board):\n\nBased on our data, what is considered “success”?\nWhat is the probability a randomly selected person will be a “success”?\nWhat are the odds a randomly selected person will be a “success”?\nWhat are the log-odds a randomly selected person will be a “success”?"
  },
  {
    "objectID": "ae/ae-09-logistic-intro.html#exercise-5",
    "href": "ae/ae-09-logistic-intro.html#exercise-5",
    "title": "AE 09: Logistic regression introduction",
    "section": "Exercise 5",
    "text": "Exercise 5\nOn the whiteboard, show that the formula for log-odds (see the slide) corresponds with the formula of probability on the slide."
  },
  {
    "objectID": "ae/ae-09-logistic-intro.html#exercise-6",
    "href": "ae/ae-09-logistic-intro.html#exercise-6",
    "title": "AE 09: Logistic regression introduction",
    "section": "Exercise 6",
    "text": "Exercise 6\nGenerate a plot of TenYearCHD vs. your groups predictor variable. Based on this plot, what do you think the relationship between this variable and TenYearCHD is?"
  },
  {
    "objectID": "ae/ae-09-logistic-intro.html#exercise-7",
    "href": "ae/ae-09-logistic-intro.html#exercise-7",
    "title": "AE 09: Logistic regression introduction",
    "section": "Exercise 7",
    "text": "Exercise 7\nFit a logistic regression model predicting the probability of developing heart disease within the next 10 years from your assigned predictor. Have your reporter write your model on the white board in both the logistic and probability form. Interpret the coefficient of your predictor in context."
  },
  {
    "objectID": "ae/ae-09-logistic-intro.html#exercise-8",
    "href": "ae/ae-09-logistic-intro.html#exercise-8",
    "title": "AE 09: Logistic regression introduction",
    "section": "Exercise 8",
    "text": "Exercise 8\nLook at the first row in heart_disease, what log-odds and probability would you predict for this observation. Find your response variable and plug it into the model you just wrote down. Only use the exp function along with addition, subtraction, multiplication, and division to compute your estimate."
  },
  {
    "objectID": "ae/ae-09-logistic-intro.html#exercise-9",
    "href": "ae/ae-09-logistic-intro.html#exercise-9",
    "title": "AE 09: Logistic regression introduction",
    "section": "Exercise 9",
    "text": "Exercise 9\n\nUse predict to generate a vector of predicted probability for the whole data set.\nUse mutate to add this vector of predicted probabilities to the original data set.\nPlot the predicted probabilities against your explanatory variable."
  },
  {
    "objectID": "ae/ae-09-logistic-intro.html#submission",
    "href": "ae/ae-09-logistic-intro.html#submission",
    "title": "AE 09: Logistic regression introduction",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the HTML file with all of your work from today’s class.\nUpload your QMD and HTML files to the Canvas assignment."
  },
  {
    "objectID": "ae/ae-11-comparison.html",
    "href": "ae/ae-11-comparison.html",
    "title": "AE 11: Model Comparison",
    "section": "",
    "text": "Important\n\n\n\n\nOpen RStudio and create a subfolder in your AE folder called “AE-11”.\nGo to the Canvas and locate your AE-11 assignment to get started.\nUpload the ae-11.qmd and tip-data.csv files into the folder you just created. The .qmd and PDF responses are due in Canvas. You can check the due date on the Canvas assignment."
  },
  {
    "objectID": "ae/ae-11-comparison.html#packages-data",
    "href": "ae/ae-11-comparison.html#packages-data",
    "title": "AE 11: Model Comparison",
    "section": "Packages + data",
    "text": "Packages + data\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(yardstick)\nlibrary(ggformula)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\n\ntips &lt;- read_csv(\"../data/tip-data.csv\") |&gt; \n  select(-`Tip Percentage`, -`W/Tip`)\n\nWhat factors are associated with the amount customers tip at a restaurant? To answer this question, we will use data collected in 2011 by a student at St. Olaf who worked at a local restaurant.1\nThe variables we’ll focus on for this analysis are\n\nTip: amount of the tip\nParty: Number of people in the party\nMeal: Time of day (Lunch, Dinner, Late Night)\nAge: Age category of person paying the bill (Yadult, Middle, SenCit)\nDay: Day of the week\nPayment: the type of payment used\nGiftCard: whether a gift card was used\n\nComps: Whether any food was comped\n\nAlcohol: Whether any alcohol was ordered\nBday: whether it was someones birthday\n\nBill: the size of the bill"
  },
  {
    "objectID": "ae/ae-11-comparison.html#analysis-goal",
    "href": "ae/ae-11-comparison.html#analysis-goal",
    "title": "AE 11: Model Comparison",
    "section": "Analysis goal",
    "text": "Analysis goal\nThe goals of this activity are to:\n\nUse ANOVA to determine whether our model is useful as a whole\nBegin thinking about \\(R^2\\) in a multivariate setting"
  },
  {
    "objectID": "ae/ae-11-comparison.html#exercise-0",
    "href": "ae/ae-11-comparison.html#exercise-0",
    "title": "AE 11: Model Comparison",
    "section": "Exercise 0",
    "text": "Exercise 0\nComplete the following to clean and then observe your data:\n\nUse drop_na to remove any rows where Party is missing.\n\n\ntips &lt;- tips |&gt; \n  _______   # drop missing values from party\n\n\nGenerate a bar chart of the variable Meal.\n\n\n# insert code here\n\n\nRun the following code and generate the same bar chart as above. You can even copy and paste your code. What’s the difference between the two plots? What do you think fct_relevel does?\n\n\ntips &lt;- tips |&gt; \n  mutate(\n    Meal = fct_relevel(Meal, \"Lunch\", \"Dinner\", \"Late Night\"),\n    Age  = fct_relevel(Age, \"Yadult\", \"Middle\", \"SenCit\")\n  )\n\n# Generate plot here"
  },
  {
    "objectID": "ae/ae-11-comparison.html#exercise-1",
    "href": "ae/ae-11-comparison.html#exercise-1",
    "title": "AE 11: Model Comparison",
    "section": "Exercise 1",
    "text": "Exercise 1\nFir a model called tip_fit_1 to predict Tip from Party, Age, and Meal"
  },
  {
    "objectID": "ae/ae-11-comparison.html#exercise-2",
    "href": "ae/ae-11-comparison.html#exercise-2",
    "title": "AE 11: Model Comparison",
    "section": "Exercise 2",
    "text": "Exercise 2\nPipe the model you generated in the previous Exercise into the function anova."
  },
  {
    "objectID": "ae/ae-11-comparison.html#exercise-3",
    "href": "ae/ae-11-comparison.html#exercise-3",
    "title": "AE 11: Model Comparison",
    "section": "Exercise 3",
    "text": "Exercise 3\nBased on the output above, compute \\(SSTotal\\), \\(SSError\\), and \\(SSModel\\). You should only need to use addition and/or subtraction."
  },
  {
    "objectID": "ae/ae-11-comparison.html#exercise-4",
    "href": "ae/ae-11-comparison.html#exercise-4",
    "title": "AE 11: Model Comparison",
    "section": "Exercise 4",
    "text": "Exercise 4\nPipe your model into the glance function. Identify, the F-statistic and p-value for an F-test of this model. Interpret the outcome of your test in the context of the problem. Be prepared to discuss the difference between this p-value and the p-values from the individual model coefficients."
  },
  {
    "objectID": "ae/ae-11-comparison.html#exercise-5",
    "href": "ae/ae-11-comparison.html#exercise-5",
    "title": "AE 11: Model Comparison",
    "section": "Exercise 5",
    "text": "Exercise 5\nFit a model tip_fit_2 to predict Tip from Party, Age, Meal, and Day."
  },
  {
    "objectID": "ae/ae-11-comparison.html#exercise-6",
    "href": "ae/ae-11-comparison.html#exercise-6",
    "title": "AE 11: Model Comparison",
    "section": "Exercise 6",
    "text": "Exercise 6\nOf the two models you just fit, which is nested inside the other?"
  },
  {
    "objectID": "ae/ae-11-comparison.html#exercise-7",
    "href": "ae/ae-11-comparison.html#exercise-7",
    "title": "AE 11: Model Comparison",
    "section": "Exercise 7",
    "text": "Exercise 7\nFit a third model called tip_fit_3 so that tip_fit_1 is nested inside tip_fit_3. Try to choose a model that you think will do a better job than tip_fit_1 of modeling the data. Feel free to include interaction or polynomial terms. Have your reporter write the model on the board."
  },
  {
    "objectID": "ae/ae-11-comparison.html#exercise-8",
    "href": "ae/ae-11-comparison.html#exercise-8",
    "title": "AE 11: Model Comparison",
    "section": "Exercise 8",
    "text": "Exercise 8\nChange eval: FALSE to eval: TRUE. Alter the code below so that you’re comparing tip_fit_3 to tip_fit_1.\n\nanova(tip_fit_1, tip_fit_2) |&gt; # Enter reduced model first\n  tidy() |&gt; \n  kable()\n\nWrite down the hypotheses, test statistic, and p-value of your test. Interpret the output in the context of the problem. Have your reporter write the p-value on the board."
  },
  {
    "objectID": "ae/ae-11-comparison.html#exercise-9",
    "href": "ae/ae-11-comparison.html#exercise-9",
    "title": "AE 11: Model Comparison",
    "section": "Exercise 9",
    "text": "Exercise 9\nBased on your test, choose tip_fit_1 or tip_fit_3. What is the \\(R^2\\) value for this model?"
  },
  {
    "objectID": "ae/ae-11-comparison.html#exercise-10",
    "href": "ae/ae-11-comparison.html#exercise-10",
    "title": "AE 11: Model Comparison",
    "section": "Exercise 10",
    "text": "Exercise 10\nFit the full model. What is it’s \\(R^2\\)? Does this model have a higher or lower \\(R^2\\) than the previous model? Does this mean it is a better model? Be prepared to discuss."
  },
  {
    "objectID": "ae/ae-11-comparison.html#exercise-11",
    "href": "ae/ae-11-comparison.html#exercise-11",
    "title": "AE 11: Model Comparison",
    "section": "Exercise 11",
    "text": "Exercise 11\nThe following code converts the numerical variable Bill into a new categorical variable Bill_factor. Essentially, each different number in Bill is treated as it’s own category. Fit a model predicting Tip from Bill_factor. What is your R^2? Think about the implications of this and what it means for the usefulness of \\(R^2\\).\n\ntips &lt;- tips |&gt; \n  mutate(Bill_factor = factor(Bill))"
  },
  {
    "objectID": "ae/ae-11-comparison.html#exercise-12",
    "href": "ae/ae-11-comparison.html#exercise-12",
    "title": "AE 11: Model Comparison",
    "section": "Exercise 12",
    "text": "Exercise 12\nApply glance to both models above to find the \\(R^2\\) and \\(R^2_{adj}\\) values?"
  },
  {
    "objectID": "ae/ae-11-comparison.html#exercise-13",
    "href": "ae/ae-11-comparison.html#exercise-13",
    "title": "AE 11: Model Comparison",
    "section": "Exercise 13",
    "text": "Exercise 13\n\nWhich model would we choose based on \\(R^2\\)?\nWhich model would we choose based on Adjusted \\(R^2\\)?\nWhich statistic should we use to choose the final model - \\(R^2\\) or Adjusted \\(R^2\\)? Why?"
  },
  {
    "objectID": "ae/ae-11-comparison.html#exercise-14",
    "href": "ae/ae-11-comparison.html#exercise-14",
    "title": "AE 11: Model Comparison",
    "section": "Exercise 14",
    "text": "Exercise 14\nReference the output from Exercise 9.\n\nWhich model would we choose based on AIC?\nWhich model would we choose based on BIC?"
  },
  {
    "objectID": "ae/ae-11-comparison.html#exercise-15",
    "href": "ae/ae-11-comparison.html#exercise-15",
    "title": "AE 11: Model Comparison",
    "section": "Exercise 15",
    "text": "Exercise 15\nNavigate to the website for olsrr here. Choose a selection technique other than best subset and apply it. Fit your resulting model."
  },
  {
    "objectID": "ae/ae-11-comparison.html#to-submit-the-ae",
    "href": "ae/ae-11-comparison.html#to-submit-the-ae",
    "title": "AE 11: Model Comparison",
    "section": "To submit the AE",
    "text": "To submit the AE\n\n\n\n\n\n\nImportant\n\n\n\n\nRender the document to produce the PDF with all of your work from today’s class.\nUpload your QMD and PDF files to the Canvas assignment."
  },
  {
    "objectID": "ae/ae-11-comparison.html#footnotes",
    "href": "ae/ae-11-comparison.html#footnotes",
    "title": "AE 11: Model Comparison",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDahlquist, Samantha, and Jin Dong. 2011. “The Effects of Credit Cards on Tipping.” Project for Statistics 212-Statistics for the Sciences, St. Olaf College.↩︎"
  },
  {
    "objectID": "ae/ae-13-multicollinearity.html",
    "href": "ae/ae-13-multicollinearity.html",
    "title": "AE 17: Multicollinearity",
    "section": "",
    "text": "Important\n\n\n\n\nOpen RStudio and create a subfolder in your AE folder called “AE-17”.\nGo to the Canvas and locate your AE-17 assignment to get started.\nUpload the ae-17.qmd and rail_trail.csv files into the folder you just created. The .qmd and PDF responses are due in Canvas. You can check the due date on the Canvas assignment."
  },
  {
    "objectID": "ae/ae-13-multicollinearity.html#packages-data",
    "href": "ae/ae-13-multicollinearity.html#packages-data",
    "title": "AE 17: Multicollinearity",
    "section": "Packages + data",
    "text": "Packages + data\n\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(GGally)\nlibrary(broom)\nlibrary(knitr)\nlibrary(mosaic)\nlibrary(mosaicData)\n\nrail_trail &lt;- read_csv(\"../data/rail_trail.csv\")\n\nThe data for this AE is based on data from the Pioneer Valley Planning Commission (PVPC) and is included in the mosaicData package. The PVPC collected data for ninety days from April 5, 2005 to November 15, 2005. Data collectors set up a laser sensor, with breaks in the laser beam recording when a rail-trail user passed the data collection station. More information can be found here."
  },
  {
    "objectID": "ae/ae-13-multicollinearity.html#analysis-goal",
    "href": "ae/ae-13-multicollinearity.html#analysis-goal",
    "title": "AE 17: Multicollinearity",
    "section": "Analysis goal",
    "text": "Analysis goal\nThe goals of this activity are to:\n\nDetect and remedy multicollinearity in our data"
  },
  {
    "objectID": "ae/ae-13-multicollinearity.html#exercise-0",
    "href": "ae/ae-13-multicollinearity.html#exercise-0",
    "title": "AE 17: Multicollinearity",
    "section": "Exercise 0",
    "text": "Exercise 0\nFit a linear model to predict volume from ALL of the other predictors. The resulting model is called the “full model”."
  },
  {
    "objectID": "ae/ae-13-multicollinearity.html#exercise-1",
    "href": "ae/ae-13-multicollinearity.html#exercise-1",
    "title": "AE 17: Multicollinearity",
    "section": "Exercise 1",
    "text": "Exercise 1\nCreate a new data frame called rail_trail_new with two new columns, hightemp_c and avgtemp_c. These two new columns should store the hightemp and avgtemp measured in Celsius rather than Fahrenheit. Note that the formula to convert between Fahrenheit and Celsius is \\(C = (F-32)\\times \\frac{5}{9}\\). Make sure hightemp and avgtemp are still in your data set."
  },
  {
    "objectID": "ae/ae-13-multicollinearity.html#exercise-2",
    "href": "ae/ae-13-multicollinearity.html#exercise-2",
    "title": "AE 17: Multicollinearity",
    "section": "Exercise 2",
    "text": "Exercise 2\nTry to refit the full model on your data. Do you notice anything odd about your coefficients? If so, what do you think is happening?"
  },
  {
    "objectID": "ae/ae-13-multicollinearity.html#exercise-3",
    "href": "ae/ae-13-multicollinearity.html#exercise-3",
    "title": "AE 17: Multicollinearity",
    "section": "Exercise 3",
    "text": "Exercise 3\nAdd a comment next to each line of code explaining what that line of code is doing.\n\nlibrary(corrplot)\n\nrail_trail |&gt; # Take the rail_trail data set and then\n  select(-volume) |&gt; # What do I do\n  select(where(is.numeric)) |&gt; # What do I do\n  cor() |&gt;   # What do I do\n  corrplot(method=\"number\") # What do I do\n\n\n\n\n\n\n\nrail_trail |&gt; \n  select(-volume) |&gt; # What do I do\n  ggpairs()"
  },
  {
    "objectID": "ae/ae-13-multicollinearity.html#exercise-4",
    "href": "ae/ae-13-multicollinearity.html#exercise-4",
    "title": "AE 17: Multicollinearity",
    "section": "Exercise 4",
    "text": "Exercise 4\nBased on the output from the previous exercise, do you have reason to believe that our data is exhibiting multicollinearity?"
  },
  {
    "objectID": "ae/ae-13-multicollinearity.html#exercise-5",
    "href": "ae/ae-13-multicollinearity.html#exercise-5",
    "title": "AE 17: Multicollinearity",
    "section": "Exercise 5",
    "text": "Exercise 5\nPipe the model you fit in Exercise 0 (i.e. not including the variable measured in Celcius) into the function vif. Are any of the vif’s concerning? If so, which ones."
  },
  {
    "objectID": "ae/ae-13-multicollinearity.html#exercise-6",
    "href": "ae/ae-13-multicollinearity.html#exercise-6",
    "title": "AE 17: Multicollinearity",
    "section": "Exercise 6",
    "text": "Exercise 6\nFit two models:\n\nm1: use all predictors except for avgtemp\nm2: use all predictors except for hightemp\nm3: average avgtemp and hightemp together and use it instead of either avgtemp and hightemp"
  },
  {
    "objectID": "ae/ae-13-multicollinearity.html#exercise-7",
    "href": "ae/ae-13-multicollinearity.html#exercise-7",
    "title": "AE 17: Multicollinearity",
    "section": "Exercise 7",
    "text": "Exercise 7\nUse glance to compute the \\(R^2_{adj}\\), AIC, and BIC for both of these models. Which should we choose?"
  },
  {
    "objectID": "ae/ae-13-multicollinearity.html#to-submit-the-ae",
    "href": "ae/ae-13-multicollinearity.html#to-submit-the-ae",
    "title": "AE 17: Multicollinearity",
    "section": "To submit the AE",
    "text": "To submit the AE\n\n\n\n\n\n\nImportant\n\n\n\n\nRender the document to produce the PDF with all of your work from today’s class.\nUpload your QMD and PDF files to the Canvas assignment."
  },
  {
    "objectID": "computing-inferencesummary.html",
    "href": "computing-inferencesummary.html",
    "title": "R Cheatsheet",
    "section": "",
    "text": "Histogram: gf_histogram( ~ variable, data = data_name)\nBoxplot: gf_boxplot( ~ variable, data = data_name)\nDotplot: gf_dotplot( ~ variable, data = data_name)\nBarchart: gf_bar( ~ variable, data = data_name)\nSide-by-side “dodged” barchart: gf_bar( ~ variable1, fill = ~ variable2, data = state_data, position=\"dodge\")\nStacked barchart: gf_bar( ~ variable1, fill = ~ variable2, data = state_data)\nStandardized barchart: gf_bar( ~ variable1, fill = ~ variable2, data = state_data, position=\"fill\")\nSide-by-side boxplot: gf_boxplot( variable1 ~ variable2, data = data_name)\nScatterplot: gf_point(variable1 ~ variable2, data = data_name)\nScatterplot with a third variable as color: gf_point(variable1 ~ variable2, color = ~ variable3, data = data_name)",
    "crumbs": [
      "Computing",
      "R Cheatsheet"
    ]
  },
  {
    "objectID": "computing-inferencesummary.html#useful-code-for-calculating-summary-statistics-requires-mosaic",
    "href": "computing-inferencesummary.html#useful-code-for-calculating-summary-statistics-requires-mosaic",
    "title": "R Cheatsheet",
    "section": "Useful code for calculating summary statistics: (requires mosaic)",
    "text": "Useful code for calculating summary statistics: (requires mosaic)\n\nMeans: mean( ~ variable, data = data_name) or mean(variable1 ~ variable2, data = data_name)\nCounts/Tables: tally( ~ variable, data = data_name) or tally(variable1 ~ variable2, data = data_name)\n5-number-summary etc: favstats( ~ variable, data = data_name) or favstats(variable1 ~ variable2, data = data_name)\n\n\nUseful functions fitting and analyzing models\n\n\n\nModel type\nAction\nFunction\n\n\n\n\nSimple Linear Regression\nFit Model\nlm(Y ~ X, data = data_name)",
    "crumbs": [
      "Computing",
      "R Cheatsheet"
    ]
  },
  {
    "objectID": "exam/exam-01-practice.html",
    "href": "exam/exam-01-practice.html",
    "title": "Exam 01 Practice",
    "section": "",
    "text": "Important\n\n\n\n\nOn the exam, you will not be tested on your ability to use Quarto. You will do most of your coding using an R Script (think of it as one big chunk). You can open one of these by clicking File &gt; New File &gt; R Script. You can then run your code there. The data set tips.csv can be found in Canvas here.\nWhile I will want you to understand simulation-based inference, I will not ask you to code up any simulation-based inference on the exam\nThis practice is meant to be as exhaustive as possible so it is longer than the exam will be",
    "crumbs": [
      "Exams",
      "Exam 1 Practice"
    ]
  },
  {
    "objectID": "exam/exam-01-practice.html#restaurant-tips",
    "href": "exam/exam-01-practice.html#restaurant-tips",
    "title": "Exam 01 Practice",
    "section": "Restaurant tips",
    "text": "Restaurant tips\nWhat factors are associated with the amount customers tip at a restaurant? To answer this question, we will use data collected in 2011 by a student at St. Olaf who worked at a local restaurant.1\nThe variables we’ll focus on for this analysis are\n\nTip: amount of the tip\nMeal: which meal this was (Lunch, Late Lunch, Dinner)\nParty: number of people in the party",
    "crumbs": [
      "Exams",
      "Exam 1 Practice"
    ]
  },
  {
    "objectID": "exam/exam-01-practice.html#exercise-1",
    "href": "exam/exam-01-practice.html#exercise-1",
    "title": "Exam 01 Practice",
    "section": "Exercise 1",
    "text": "Exercise 1\nLoad the following packages tidyverse, broom, ggformula, yardstick and mosaic.",
    "crumbs": [
      "Exams",
      "Exam 1 Practice"
    ]
  },
  {
    "objectID": "exam/exam-01-practice.html#exercise-2",
    "href": "exam/exam-01-practice.html#exercise-2",
    "title": "Exam 01 Practice",
    "section": "Exercise 2",
    "text": "Exercise 2\nLoad and look at the tips.csv data set.",
    "crumbs": [
      "Exams",
      "Exam 1 Practice"
    ]
  },
  {
    "objectID": "exam/exam-01-practice.html#exercise-3",
    "href": "exam/exam-01-practice.html#exercise-3",
    "title": "Exam 01 Practice",
    "section": "Exercise 3",
    "text": "Exercise 3\nGenerate appropriate plots AND numerical summaries for the following variables:\n\nUnivariate:\n\nTip\nParty\nMeal\n\nBivariate:\n\nTip vs. Meal\nTip vs.\n\n\nThe goal is to fit a model that uses the number of diners in the party to understand variability in the tips. For Exercise 4-8, assume we are only using Party and not Meal to predict Tip.",
    "crumbs": [
      "Exams",
      "Exam 1 Practice"
    ]
  },
  {
    "objectID": "exam/exam-01-practice.html#exercise-4",
    "href": "exam/exam-01-practice.html#exercise-4",
    "title": "Exam 01 Practice",
    "section": "Exercise 4",
    "text": "Exercise 4\nWrite the statistical model that we will be trying to estimate. Use Greek letters and include an error term. (This will be completed on a white board on the exam).",
    "crumbs": [
      "Exams",
      "Exam 1 Practice"
    ]
  },
  {
    "objectID": "exam/exam-01-practice.html#exercise-5",
    "href": "exam/exam-01-practice.html#exercise-5",
    "title": "Exam 01 Practice",
    "section": "Exercise 5",
    "text": "Exercise 5\nFit the regression model corresponding to the statistical model in the previous exercise. Use tidy to get a 99% confidence interval for the slope.\n\nWrite the regression equation and interpret the slope and intercept in the context of the data.\nWrite down and interpret the confidence interval for the slope from above.",
    "crumbs": [
      "Exams",
      "Exam 1 Practice"
    ]
  },
  {
    "objectID": "exam/exam-01-practice.html#exercise-6",
    "href": "exam/exam-01-practice.html#exercise-6",
    "title": "Exam 01 Practice",
    "section": "Exercise 6",
    "text": "Exercise 6\nOne family for four walks into your restaurant. On a piece of paper use your model to predict the tip. Then use R to create a confidence interval for your predictions. Did you use a “prediction interval” or not? Explain.",
    "crumbs": [
      "Exams",
      "Exam 1 Practice"
    ]
  },
  {
    "objectID": "exam/exam-01-practice.html#exercise-7",
    "href": "exam/exam-01-practice.html#exercise-7",
    "title": "Exam 01 Practice",
    "section": "Exercise 7",
    "text": "Exercise 7\n\nDefine \\(R^2\\), compute the \\(R^2\\) for your model, and interpret it in the context of the data.\nDefine RMSE, compute the RMSE for your model, and interpret it in the context of the data.",
    "crumbs": [
      "Exams",
      "Exam 1 Practice"
    ]
  },
  {
    "objectID": "exam/exam-01-practice.html#exercise-8",
    "href": "exam/exam-01-practice.html#exercise-8",
    "title": "Exam 01 Practice",
    "section": "Exercise 8",
    "text": "Exercise 8\nThe following code can be used to create a bootstrap distribution for the slope (and the intercept, though we’ll focus primarily on the slope in our inference) for the coefficient of Party in our linear model. Use the plot below to (visually) construct a 90% confidence interval for the slope:\n\nDescribe why you chose the values you chose for your interval.\nInterpret the interval in the context of the data.\nHow would increasing the number of repetitions change the size of the confidence interval?\nHow would increasing the sample size change the size of the confidence interval?\nHow would increasing the confidence level change the size of the confidence interval?\n\n\nset.seed(1234)\n\nboot_dist &lt;- tips |&gt;\n  specify(Tip ~ Party) |&gt;\n  generate(reps = 1000, type = \"bootstrap\") |&gt;\n  fit()\n\nboot_dist |&gt;\n  filter(term == \"Party\") |&gt; \n  gf_histogram(~estimate)",
    "crumbs": [
      "Exams",
      "Exam 1 Practice"
    ]
  },
  {
    "objectID": "exam/exam-01-practice.html#exercise-9",
    "href": "exam/exam-01-practice.html#exercise-9",
    "title": "Exam 01 Practice",
    "section": "Exercise 9",
    "text": "Exercise 9\nSet-up a hypothesis test for the slope of Party. Make sure to include:\n\nBoth hypotheses in mathematical notation and words.\nThe test statistic.\nThe distribution of the test statistic.\nThe p-value.\nThe result of your test at a significance level of \\(\\alpha= 0.05\\).\n\nYou may want to refer to the output in Exercise 5.",
    "crumbs": [
      "Exams",
      "Exam 1 Practice"
    ]
  },
  {
    "objectID": "exam/exam-01-practice.html#exercise-10",
    "href": "exam/exam-01-practice.html#exercise-10",
    "title": "Exam 01 Practice",
    "section": "Exercise 10",
    "text": "Exercise 10\nList the conditions necessary for conducting inference. Include how you would test each one and how you would determine if they were satisfied.\nBased on the context of this problem, you should expect that the constant variance assumption is likely to be violated… why? Think about how people tip.",
    "crumbs": [
      "Exams",
      "Exam 1 Practice"
    ]
  },
  {
    "objectID": "exam/exam-01-practice.html#footnotes",
    "href": "exam/exam-01-practice.html#footnotes",
    "title": "Exam 01 Practice",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDahlquist, Samantha, and Jin Dong. 2011. “The Effects of Credit Cards on Tipping.” Project for Statistics 212-Statistics for the Sciences, St. Olaf College.↩︎",
    "crumbs": [
      "Exams",
      "Exam 1 Practice"
    ]
  },
  {
    "objectID": "exam/exam-02-practice.html",
    "href": "exam/exam-02-practice.html",
    "title": "Exam 02 Review",
    "section": "",
    "text": "Important\n\n\n\n\nOn the exam, you will not be tested on your ability to use Quarto. You will do most of your coding using an R Script (think of it as one big chunk). You can open one of these by clicking File &gt; New File &gt; R Script. You can then run your code there. The data set is contained in the ISLR2 package so you don’t need any csv file.\nWhile I will want you to understand simulation-based inference, I will not ask you to code up any simulation-based inference on the exam\nThis practice is meant to be as exhaustive as possible so it is longer than the exam will be",
    "crumbs": [
      "Exams",
      "Exam 2 Practice"
    ]
  },
  {
    "objectID": "exam/exam-02-practice.html#packages",
    "href": "exam/exam-02-practice.html#packages",
    "title": "Exam 02 Review",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(Stat2Data)\nlibrary(ISLR2)\nlibrary(broom)\nlibrary(knitr)\nlibrary(rms)",
    "crumbs": [
      "Exams",
      "Exam 2 Practice"
    ]
  },
  {
    "objectID": "exam/exam-02-practice.html#data",
    "href": "exam/exam-02-practice.html#data",
    "title": "Exam 02 Review",
    "section": "Data",
    "text": "Data\nThe data for this analysis is about credit card customers. The following variables are in the data set:\n\nIncome: Income in $1,000’s\nLimit: Credit limit\nRating: Credit rating\nCards: Number of credit cards\nAge: Age in years\nEducation: Number of years of education\nOwn: A factor with levels No and Yes indicating whether the individual owns their home\nStudent: A factor with levels No and Yes indicating whether the individual was a student\nMarried: A factor with levels No and Yes indicating whether the individual was married\nRegion: A factor with levels South, East, and West indicating the region of the US the individual is from\nBalance: Average credit card balance in $.",
    "crumbs": [
      "Exams",
      "Exam 2 Practice"
    ]
  },
  {
    "objectID": "exam/exam-02-practice.html#exercise-1",
    "href": "exam/exam-02-practice.html#exercise-1",
    "title": "Exam 02 Review",
    "section": "Exercise 1",
    "text": "Exercise 1\nFit a model using limit and age to predict balance. Write the regression equation corresponding to this model on the board. Interpret all coefficients in the output. Draw of picture of the resulting model.",
    "crumbs": [
      "Exams",
      "Exam 2 Practice"
    ]
  },
  {
    "objectID": "exam/exam-02-practice.html#exercise-2",
    "href": "exam/exam-02-practice.html#exercise-2",
    "title": "Exam 02 Review",
    "section": "Exercise 2",
    "text": "Exercise 2\nWhat is the p-value associated with the slope of age? Describe what conclusions you can draw from it. What is this p-value the probability of and why can we use it in hypothesis testing? Would you necessarily draw the same conclusion to a hypothesis test for the slope of age if income were not in the model.",
    "crumbs": [
      "Exams",
      "Exam 2 Practice"
    ]
  },
  {
    "objectID": "exam/exam-02-practice.html#exercise-3",
    "href": "exam/exam-02-practice.html#exercise-3",
    "title": "Exam 02 Review",
    "section": "Exercise 3",
    "text": "Exercise 3\nCompute, describe, and interpret the confidence interval associated with the slope of age? Does it make sense when compared to p-value for age?",
    "crumbs": [
      "Exams",
      "Exam 2 Practice"
    ]
  },
  {
    "objectID": "exam/exam-02-practice.html#exercise-4",
    "href": "exam/exam-02-practice.html#exercise-4",
    "title": "Exam 02 Review",
    "section": "Exercise 4",
    "text": "Exercise 4\nFit a model using income, limit, and student to predict balance, but include an interaction term between limit and student.\nDescribe and interpret the interaction term from the above model. Be sure to give the value of the coefficient and describe what it means. Draw of picture of the resulting model. Explain how it is different than the picture you drew in Exercise 1.",
    "crumbs": [
      "Exams",
      "Exam 2 Practice"
    ]
  },
  {
    "objectID": "exam/exam-02-practice.html#exercise-5",
    "href": "exam/exam-02-practice.html#exercise-5",
    "title": "Exam 02 Review",
    "section": "Exercise 5",
    "text": "Exercise 5\nBoth models you have fit have two lines nested in them. Write the equations for all four lines (two for the first model, two for the second).",
    "crumbs": [
      "Exams",
      "Exam 2 Practice"
    ]
  },
  {
    "objectID": "exam/exam-02-practice.html#exercise-6",
    "href": "exam/exam-02-practice.html#exercise-6",
    "title": "Exam 02 Review",
    "section": "Exercise 6",
    "text": "Exercise 6\nHow would you determine which model is better? What are two quantities you could compute?",
    "crumbs": [
      "Exams",
      "Exam 2 Practice"
    ]
  },
  {
    "objectID": "exam/exam-02-practice.html#exercise-7",
    "href": "exam/exam-02-practice.html#exercise-7",
    "title": "Exam 02 Review",
    "section": "Exercise 7",
    "text": "Exercise 7\nWhy is it more difficult to visualize an interaction between two quantitative variables than the interaction between a quantitative and a categorical variable?",
    "crumbs": [
      "Exams",
      "Exam 2 Practice"
    ]
  },
  {
    "objectID": "exam/exam-02-practice.html#exercise-8",
    "href": "exam/exam-02-practice.html#exercise-8",
    "title": "Exam 02 Review",
    "section": "Exercise 8",
    "text": "Exercise 8\nList the four conditions for conducting multiple linear regression. For each one, discuss how you assess them, and what, if anything, you do differently than for simple linear regression.",
    "crumbs": [
      "Exams",
      "Exam 2 Practice"
    ]
  },
  {
    "objectID": "exam/exam-02-practice.html#exercise-9",
    "href": "exam/exam-02-practice.html#exercise-9",
    "title": "Exam 02 Review",
    "section": "Exercise 9",
    "text": "Exercise 9\nBe ready to talk about transformations. Specifically, be ready to talk about \\(\\log\\) transformations and power transformations. Know what they are good for and when you would use them. Also be ready to explain why \\(\\log\\) transformations are so powerful.",
    "crumbs": [
      "Exams",
      "Exam 2 Practice"
    ]
  },
  {
    "objectID": "exam/exam-rubric.html",
    "href": "exam/exam-rubric.html",
    "title": "Exam Rubric",
    "section": "",
    "text": "The is an oral exam designed to last 50 minutes. You will be asked a combination of questions requiring both R and a conceptual understanding of the material. At the end of 50 minutes we will spend 10 minutes jointly deciding your grade using the following rubric as a guide:\n\n100: Student demonstrates complete understanding of the material including the ability to synthesize and extend the material from this course. There are no conceptual mistakes and any coding mistakes are complete superficial. You have truly impressed Dr. F.\n95: Student demonstrates complete understanding of the material including the ability to synthesize the material from this course and is able to extend the material with minimal prompting from the instructor. Any mistakes are minor and infrequent. The student requires only minimal help with coding, for example pointing out missing comma’s.\n90: Student demonstrates that they know and understand all or almost all of the content but without true synthesis. Basic questions come to this students easily. The student is able to answer questions of moderate difficulty with some trouble but no help. However, the student struggles to place the material from the course in context without prompting from the instructor or the student makes one major mistake or many minor mistakes. The student may require more than minimal help with coding, for example reminders about the names of functions.\n85: Student has at least one minor gap in their understand of the material. They are able to answer most but not all problems without the help of the instructor and do not demonstrate the ability to synthesize and extend the course content. The student may take enough time on easier problems that we are unable to assess harder content. The student may require more than minimal help with coding, for example reminders about the names of functions.\n80: Student has several minor gaps in their understanding of the material. They can answer some questions without help but frequently need prompting from the instructor. The student may take enough time on easier problems that we are unable to assess harder content. The student may require more than minimal help with coding, for example reminders about the names of functions.\n75: Student has at least one major gap and/or many minor gaps in their understanding of the material. They can solve some problems without help but frequently need prompting from the instructor. The student does take enough time on easier problems that we are unable to assess harder content. The student may requires frequent help with coding but it is clear that they have been working through the homeworks and grasp some of the concepts.\n70: Student has several major gaps in their understanding of the material. They can answer some problems without help but frequently need prompting from the instructor. Furthermore, there are some problems they don’t know how to start. The student may requires frequent help with coding but it is clear that they have been working through the homeworks and grasp some of the concepts.\n65: Student is familiar with the basic definitions in the course. Student can get through many questions but only with frequent help from the instructor. The student may requires frequent help with coding but it is clear that they have been working through the homeworks and grasp some of the concepts.\n60: Student is familiar with the basic definitions in the course. The student may requires frequent help with coding but it is clear that they have been working through the homeworks and grasp some of the concepts.\n&lt;60: If a grade below 60 is assigned, Dr. Friedlander will decide the grade that is earned based on the students knowledge of the material.",
    "crumbs": [
      "Exams",
      "Oral Exam Rubric"
    ]
  },
  {
    "objectID": "hw/hw-02.html",
    "href": "hw/hw-02.html",
    "title": "HW 02: Education & median income in US Counties",
    "section": "",
    "text": "In this homework, you will use simple linear regression to examine the association between the percent of adults with a bachelor’s degree and the median household income for counties in the United States.\n\n\nIn this assignment, you will…\n\nFit and interpret simple linear regression models.\nConduct simulation-based statistical inference for the population slope, \\(\\beta_1\\)\nContinue developing a workflow for reproducible data analysis."
  },
  {
    "objectID": "hw/hw-02.html#learning-goals",
    "href": "hw/hw-02.html#learning-goals",
    "title": "HW 02: Education & median income in US Counties",
    "section": "",
    "text": "In this assignment, you will…\n\nFit and interpret simple linear regression models.\nConduct simulation-based statistical inference for the population slope, \\(\\beta_1\\)\nContinue developing a workflow for reproducible data analysis."
  },
  {
    "objectID": "hw/hw-02.html#part-1-exploratory-data-analysis",
    "href": "hw/hw-02.html#part-1-exploratory-data-analysis",
    "title": "HW 02: Education & median income in US Counties",
    "section": "Part 1: Exploratory data analysis",
    "text": "Part 1: Exploratory data analysis\n\nExercise 1\nCreate a histogram of the distribution of the predictor variable bachelorsand calculate appropriate summary statistics. Use the visualization and summary statistics to describe the distribution. Include an informative title and axis labels on the plot.\n\n\nExercise 2\nCreate a visualization of the relationship between bachelors and median_household_income and use the cor function to calculate the correlation. You will likely want to load the mosaic package. It is considered good form to load your packages at the beginning of the document so scroll up to the top and load mosaic there. Use the visualization and correlation to describe the relationship between the two variables.\n\n\n\n\n\n\nTip\n\n\n\nRecall the analysis objective stated at the beginning of the Exercises section.\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you haven’t yet done so, now is a good time to render your document."
  },
  {
    "objectID": "hw/hw-02.html#part-2-modeling",
    "href": "hw/hw-02.html#part-2-modeling",
    "title": "HW 02: Education & median income in US Counties",
    "section": "Part 2: Modeling",
    "text": "Part 2: Modeling\n\nExercise 3\nWe will use a linear regression model to better quantify the relationship between bachelors and median_household_income.\nWrite the form of the statistical model the researchers would like to estimate using the template below. Use mathematical notation (i.e. with Greek letters) and variable names (bachelors and median_household_income) in the equation. Note, I’m not asking you to fit the model yet. Replace each word in the equation below with either the name of a variable or a Greek letter.\n\\[Response = intercept + slope\\times predictor + error\\]\n\n\n\n\n\n\nTip\n\n\n\nClick here for a guide on writing mathematical symbols using LaTeX. You will need to use a backslash (\\) before each underscore in the LaTeX code. For example, avg_air_temp will be written as avg\\_air\\_temp.\n\n\n\n\nExercise 4\n\nFit the regression line corresponding to the statistical model in the previous exercise. Use tidy and kable to neatly display the model output using 3 digits. Note you will need to load the broom package before you can use the tidy function.\nWrite the equation of the fitted model using mathematical notation. Use variable names (bachelors and median_household_income) in the equation. Hint: Copy and paste your answer from the previous exercise, fill in the \\(\\beta\\)’s with numbers, and remove the error term.\n\n\n\nExercise 5\n\nInterpret the slope. The interpretation should be written in a way that is meaningful in the context of the data.\nIs it useful to interpret the intercept for this data? If so, write the interpretation in the context of the data. Otherwise, briefly explain why not.\n\n\n\n\n\n\n\nTip\n\n\n\nNow is a good time to render your document again if you haven’t done so recently."
  },
  {
    "objectID": "hw/hw-02.html#part-3-inference-for-the-u.s.",
    "href": "hw/hw-02.html#part-3-inference-for-the-u.s.",
    "title": "HW 02: Education & median income in US Counties",
    "section": "Part 3: Inference for the U.S.",
    "text": "Part 3: Inference for the U.S.\nWe want to use the data from these 600 randomly selected counties to draw conclusions about the relationship between the percent of adults age 25 and older with a bachelor’s degree and median household income for the over 3,000 counties in the United States.\n\nExercise 6\n\nWhat is the population of interest? What is the sample?\nIs it reasonable to treat the sample in this analysis as representative of the population? Briefly explain why or why not. If you have never taken a statistics course before, I encourage you to read subsections 2.1.4 and 2.1.5 from Introduction to Modern Statistics. Don’t worry, they’re very short.\n\n\n\nExercise 7\nConduct a hypothesis test for the slope to assess whether there is sufficient evidence of a linear relationship between the percent of adults age 25 and older with a bachelor’s degree and the median household income in a county. Use a randomization (permutation) test. Remember that you’ll need to load the infer package. In your response:\n\nState the null and alternative hypotheses in words and mathematical notation\nShow all relevant code and output used to conduct the test. Use set.seed(2025) and 1000 iterations to construct the appropriate distribution.\nState the conclusion in the context of the data.\n\n\n\nExercise 8\nNext, construct a 95% confidence interval for the slope using bootstrapping with set.seed(2025) and 1000 iterations.\n\nShow all relevant code and output used to calculate the interval.\nInterpret the confidence interval in the context of the data.\nIs the confidence interval consistent with the results of the test from the previous exercise? Briefly explain why or why not.\n\n\n\n\n\n\n\nTip\n\n\n\nNow is a good time to render your document again if you haven’t done so recently."
  },
  {
    "objectID": "hw/hw-02.html#conceptual-questions",
    "href": "hw/hw-02.html#conceptual-questions",
    "title": "HW 02: Education & median income in US Counties",
    "section": "Conceptual Questions",
    "text": "Conceptual Questions\nPlease try to answer these concisely and adhere to the sentence maximum.\n\nExercise 9\nShould all of your classmates get the same 95% confidence interval as you? Max one sentence.\n\n\nExercise 10\nWhy was it necessary to set a seed in the previous two exercises and how does it relate to the concept of reproducibility? Max two sentences.\n\n\nExercise 11\nWould taking a larger sample of counties most likely result in a more or less precise confidence interval? What about a wider or narrower confidence interval? Max one sentence each but you should be able to answer with a single sentence.\n\n\nExercise 12\nWhy would taking a larger sample of counties not impact the accuracy of the 95% confidence interval? How can we increase the accuracy? Max two sentences.\n\n\nExercise 13\nExplain why the following statement doesn’t make sense: “There is a 95% probability that the population slope is between the two numbers I stated in Exercise 8. Max three sentences."
  },
  {
    "objectID": "hw/hw-02.html#reproducibility-conceptual-questions",
    "href": "hw/hw-02.html#reproducibility-conceptual-questions",
    "title": "HW 02: Education & median income in US Counties",
    "section": "Reproducibility & Conceptual Questions",
    "text": "Reproducibility & Conceptual Questions\n\nExercise 14 (Extra Credit)\nYou are asked to use a reproducible workflow for all of your work in the class, and the goal of this question to is better understand potential real-world implications of doing (or not) doing so. Here are some real-life examples in which having a non-reproducible workflow resulted in errors that impacted research and public records.\nChoose one of the scenarios from the table and read the linked article discussing what went wrong. Then,\n\nBriefly describe what went wrong, i.e., what part of the process was not reproducible and what error or impact that had.\nThen, describe how the researchers could make the process reproducible.\n\n\n\n\n\n\n\nTip\n\n\n\nNow is a good time to render your document again. Make sure that everything is formatted the way you want it to be."
  },
  {
    "objectID": "hw/hw-02.html#footnotes",
    "href": "hw/hw-02.html#footnotes",
    "title": "HW 02: Education & median income in US Counties",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Grammar & Writing” grade is decided based on your grammar and writing. This is typically decided by choosing one of the questions and assessing the writing.↩︎\nThe “Workflow & formatting” grade is to assess the reproducible workflow and document format. This includes having a neatly organized document with readable code and your name and the date in the YAML.↩︎"
  },
  {
    "objectID": "hw/hw-04.html",
    "href": "hw/hw-04.html",
    "title": "HW 04: Alumni jobs",
    "section": "",
    "text": "In this homework, you’ll use simple linear regression to analyze the relationship between the median early career salary and percent of alumni who perceive their job as making the world a better place for colleges and universities in the United States."
  },
  {
    "objectID": "hw/hw-04.html#exercise-1",
    "href": "hw/hw-04.html#exercise-1",
    "title": "HW 04: Alumni jobs",
    "section": "Exercise 1",
    "text": "Exercise 1\nFit a linear model that can be used to predict the typical early career pay based on the percent of STEM degrees. Display the model output using 3 digits for numerical values."
  },
  {
    "objectID": "hw/hw-04.html#exercise-2",
    "href": "hw/hw-04.html#exercise-2",
    "title": "HW 04: Alumni jobs",
    "section": "Exercise 2",
    "text": "Exercise 2\nCalculate the predicted values and residuals from your model and save these results in a data frame. Print the first five rows of the new data frame."
  },
  {
    "objectID": "hw/hw-04.html#exercise-3",
    "href": "hw/hw-04.html#exercise-3",
    "title": "HW 04: Alumni jobs",
    "section": "Exercise 3",
    "text": "Exercise 3\nBefore using the model for prediction, let’s check the model conditions. To figure out how to do so, we type the following prompt into ChatGPT (“an AI powered language model”) “How do you check the model conditions for simple linear regression?”\nThe response from ChatGPT shown below would receive partial credit. For each condition, state whether it is correct or incorrect/incomplete. If it incorrect/incomplete, state what is wrong or missing, then write the correct response. Please put all answers after the ChatGPT text.\n\nIn simple linear regression, you’re trying to establish a linear relationship between two variables: a dependent variable (Y) and an independent variable (X). The model conditions or assumptions for simple linear regression are important to check to ensure that your model is valid and reliable. Here are the key assumptions and how to check them:\n1. Linearity Assumption:\n\nAssumption: The relationship between the independent variable (X) and the dependent variable (Y) is linear.\nHow to Check: Create a scatterplot of Y against X. If the points roughly form a straight line, the linearity assumption is likely met.\n\n2. Independence Assumption:\n\nAssumption: The residuals (the differences between observed Y and predicted Y) are independent of each other.\nHow to Check: Plot the residuals against the independent variable (X) or the predicted values. Look for patterns or trends in the residuals. There should be no clear structure or autocorrelation in the residuals.\n\n3. Homoscedasticity [Constant variance] Assumption:\n\nAssumption: The variance of the residuals is constant across all levels of the independent variable.\nHow to Check: Plot the residuals against the predicted values or the independent variable (X). The spread of the residuals should remain roughly constant across the range of X.\n\n4. Normality Assumption:\n\nAssumption: The residuals follow a normal distribution.\nHow to Check: Create a histogram or a normal probability plot (Q-Q plot) of the residuals. If the residuals are approximately normally distributed, they should follow a bell-shaped curve in the histogram and lie along a straight line in the Q-Q plot.\n\nIt’s important to note that these assumptions may not always hold perfectly in real-world data, and minor violations may not necessarily invalidate your model. However, if you detect significant violations of these assumptions, you may need to consider alternative modeling techniques or transformations of your data to address the issues."
  },
  {
    "objectID": "hw/hw-04.html#exercise-4",
    "href": "hw/hw-04.html#exercise-4",
    "title": "HW 04: Alumni jobs",
    "section": "Exercise 4",
    "text": "Exercise 4\nDetermine to what extent the model conditions hold. Briefly describe how that will impact any statistical inference."
  },
  {
    "objectID": "hw/hw-04.html#exercise-5",
    "href": "hw/hw-04.html#exercise-5",
    "title": "HW 04: Alumni jobs",
    "section": "Exercise 5",
    "text": "Exercise 5\n\n31 percent of the alumni at Johns Hopkins University received a STEM degree. Calculate the predicted early career pay at this university and the appropriate 95% interval. Interpret the interval in the context of the data.\nThe actual early career pay at this university is $67,200. Is this what you expected based on your analysis? Briefly explain your reasoning."
  },
  {
    "objectID": "hw/hw-04.html#exercise-6",
    "href": "hw/hw-04.html#exercise-6",
    "title": "HW 04: Alumni jobs",
    "section": "Exercise 6",
    "text": "Exercise 6\nNext, let’s consider how well the model fits the relationship between the early career pay and percent with a STEM degree.\n\nCalculate \\(R^2\\) and interpret it in the context of the data.\nCalculate RMSE and interpret it in the context of the data."
  },
  {
    "objectID": "hw/hw-04.html#exercise-7",
    "href": "hw/hw-04.html#exercise-7",
    "title": "HW 04: Alumni jobs",
    "section": "Exercise 7",
    "text": "Exercise 7\nDo you think the model is useful for understanding and predicting the typical early career pay for alumni at a university? Briefly explain your reasoning."
  },
  {
    "objectID": "hw/hw-04.html#exercise-8",
    "href": "hw/hw-04.html#exercise-8",
    "title": "HW 04: Alumni jobs",
    "section": "Exercise 8",
    "text": "Exercise 8\nFit another model which predicts the typical early career pay using only the percentage of alumni that believe their career will make the world a better place.\n\nNeatly display this model with 3 digits.\nWhich model do you think is better - this model or the model fit in Exercise 1? Explain your response showing any analysis used to make the decision."
  },
  {
    "objectID": "hw/hw-04.html#footnotes",
    "href": "hw/hw-04.html#footnotes",
    "title": "HW 04: Alumni jobs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Grammar & Writing” grade is decided based on your grammar and writing. This is typically decided by choosing one of the questions and assessing the writing.↩︎\nThe “Workflow & formatting” grade is to assess the reproducible workflow and document format. This includes having a neatly organized document with readable code and your name and the date in the YAML.↩︎"
  },
  {
    "objectID": "hw/hw-06.html",
    "href": "hw/hw-06.html",
    "title": "Homework 06: Candy Competition",
    "section": "",
    "text": "In today’s homework you will analyze data about candy that was collected from an online experiment conducted at FiveThirtyEight.\n\n\nBy the end of the homework you will be able to\n\nFit a linear model with multiple predictors and an interaction term\nFit a linear model with categorical predictors\nConduct inference on multiple linear models"
  },
  {
    "objectID": "hw/hw-06.html#introduction",
    "href": "hw/hw-06.html#introduction",
    "title": "Homework 06: Candy Competition",
    "section": "",
    "text": "In today’s homework you will analyze data about candy that was collected from an online experiment conducted at FiveThirtyEight.\n\n\nBy the end of the homework you will be able to\n\nFit a linear model with multiple predictors and an interaction term\nFit a linear model with categorical predictors\nConduct inference on multiple linear models"
  },
  {
    "objectID": "hw/hw-06.html#getting-started",
    "href": "hw/hw-06.html#getting-started",
    "title": "Homework 06: Candy Competition",
    "section": "Getting started",
    "text": "Getting started\n\nGo to RStudio and login with your College of Idaho Email and Password.\nMake a subfolder in your hw directory to store this homework.\nLog into Canvas, navigate to Homework 6 and upload the hw-06.qmd file into the folder your just made."
  },
  {
    "objectID": "hw/hw-06.html#packages",
    "href": "hw/hw-06.html#packages",
    "title": "Homework 06: Candy Competition",
    "section": "Packages",
    "text": "Packages\nThe following packages are used in the lab.\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(ggformula)\nlibrary(fivethirtyeight)\nlibrary(knitr)\nlibrary(yardstick)\n\n# add other packages as needed"
  },
  {
    "objectID": "hw/hw-06.html#data-candy",
    "href": "hw/hw-06.html#data-candy",
    "title": "Homework 06: Candy Competition",
    "section": "Data: Candy",
    "text": "Data: Candy\nThe data from this lab comes from the the article FiveThirtyEight The Ultimate Halloween Candy Power Ranking by Walt Hickey. To collect data, Hickey and collaborators at FiveThirtyEight set up an experiment people could vote on a series of randomly generated candy matchups (e.g. Reese’s vs. Skittles). Click here to check out some of the match ups.\nThe data set contains the characteristics and win percentage from 85 candies in the experiment. The variables are\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nchocolate\nDoes it contain chocolate?\n\n\nfruity\nIs it fruit flavored?\n\n\ncaramel\nIs there caramel in the candy?\n\n\npeanutyalmondy\nDoes it contain peanuts, peanut butter or almonds?\n\n\nnougat\nDoes it contain nougat?\n\n\ncrispedricewafer\nDoes it contain crisped rice, wafers, or a cookie component?\n\n\nhard\nIs it a hard candy?\n\n\nbar\nIs it a candy bar?\n\n\npluribus\nIs it one of many candies in a bag or box?\n\n\nsugarpercent\nThe percentile of sugar it falls under within the data set. Values 0 - 1.\n\n\npricepercent\nThe unit price percentile compared to the rest of the set. Values 0 - 1.\n\n\nwinpercent\nThe overall win percentage according to 269,000 matchups. Values 0 - 100.\n\n\n\nUse the code below to get a glimpse of the candy_rankings data frame in the fivethirtyeight R package.\n\nglimpse(candy_rankings)\n\nRows: 85\nColumns: 13\n$ competitorname   &lt;chr&gt; \"100 Grand\", \"3 Musketeers\", \"One dime\", \"One quarter…\n$ chocolate        &lt;lgl&gt; TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, F…\n$ fruity           &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE…\n$ caramel          &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,…\n$ peanutyalmondy   &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, …\n$ nougat           &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,…\n$ crispedricewafer &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ hard             &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ bar              &lt;lgl&gt; TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, F…\n$ pluribus         &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE…\n$ sugarpercent     &lt;dbl&gt; 0.732, 0.604, 0.011, 0.011, 0.906, 0.465, 0.604, 0.31…\n$ pricepercent     &lt;dbl&gt; 0.860, 0.511, 0.116, 0.511, 0.511, 0.767, 0.767, 0.51…\n$ winpercent       &lt;dbl&gt; 66.97173, 67.60294, 32.26109, 46.11650, 52.34146, 50.…"
  },
  {
    "objectID": "hw/hw-06.html#exercises",
    "href": "hw/hw-06.html#exercises",
    "title": "Homework 06: Candy Competition",
    "section": "Exercises",
    "text": "Exercises\nThe goal of this analysis is to use multiple linear regression to understand the factors that make a good candy."
  },
  {
    "objectID": "hw/hw-06.html#exercise-1",
    "href": "hw/hw-06.html#exercise-1",
    "title": "Homework 06: Candy Competition",
    "section": "Exercise 1",
    "text": "Exercise 1\nNotice that the values of pricepercent and sugarpercent are proportions. User mutate to change the scale so that they are percentages."
  },
  {
    "objectID": "hw/hw-06.html#exercise-2",
    "href": "hw/hw-06.html#exercise-2",
    "title": "Homework 06: Candy Competition",
    "section": "Exercise 2",
    "text": "Exercise 2\n\nOur response variable in this homework will be winpercentage. Choose two additional variables, one quantitative and one categorical. Generate a SINGLE plot that visualizes all three variables. Hint: remember that you can tie any aesthetic in your plot (e.g. color) to a variable be writing aesthetic = ~variable_name. For example color = ~variable1 will color the points based on variable1.\nWrite two observations from your plot."
  },
  {
    "objectID": "hw/hw-06.html#exercise-3",
    "href": "hw/hw-06.html#exercise-3",
    "title": "Homework 06: Candy Competition",
    "section": "Exercise 3",
    "text": "Exercise 3\nFit a linear model including both variables you chose above."
  },
  {
    "objectID": "hw/hw-06.html#exercise-4",
    "href": "hw/hw-06.html#exercise-4",
    "title": "Homework 06: Candy Competition",
    "section": "Exercise 4",
    "text": "Exercise 4\nInterpret the following in the context of the data:\n\nIntercept\nCoefficient of your quantitative variable\nCoefficient(s) of your categorical variables"
  },
  {
    "objectID": "hw/hw-06.html#exercise-5",
    "href": "hw/hw-06.html#exercise-5",
    "title": "Homework 06: Candy Competition",
    "section": "Exercise 5",
    "text": "Exercise 5\nChoose one of the coefficients from your model and write out all the steps in the hypothesis test that corresponds to it’s p-value as in this slide."
  },
  {
    "objectID": "hw/hw-06.html#exercise-6",
    "href": "hw/hw-06.html#exercise-6",
    "title": "Homework 06: Candy Competition",
    "section": "Exercise 6",
    "text": "Exercise 6\nInterpret the other p-values in your model in the context of the data. You do not need to write out the full hypothesis testing framework as you did in Exercise 5."
  },
  {
    "objectID": "hw/hw-06.html#exercise-7",
    "href": "hw/hw-06.html#exercise-7",
    "title": "Homework 06: Candy Competition",
    "section": "Exercise 7",
    "text": "Exercise 7\nGenerate 95% confidence intervals for all of the slopes in your model and interpret them in the context of the data."
  },
  {
    "objectID": "hw/hw-06.html#exercise-8",
    "href": "hw/hw-06.html#exercise-8",
    "title": "Homework 06: Candy Competition",
    "section": "Exercise 8",
    "text": "Exercise 8\nNow fit a model using the same explanatory variables, but include an interaction term. Interpret the coefficient of your interaction term in the context of the data."
  },
  {
    "objectID": "hw/hw-06.html#exercise-9",
    "href": "hw/hw-06.html#exercise-9",
    "title": "Homework 06: Candy Competition",
    "section": "Exercise 9",
    "text": "Exercise 9\nInterpret the p-value associated with the interaction term."
  },
  {
    "objectID": "hw/hw-06.html#exercise-10",
    "href": "hw/hw-06.html#exercise-10",
    "title": "Homework 06: Candy Competition",
    "section": "Exercise 10",
    "text": "Exercise 10\nBased on the results from exercise 9, would you choose the model with or without the interaction term?"
  },
  {
    "objectID": "hw/hw-06.html#grading",
    "href": "hw/hw-06.html#grading",
    "title": "Homework 06: Candy Competition",
    "section": "Grading",
    "text": "Grading\nTotal points available: 15 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1\n1\n\n\nEx 2\n2\n\n\nEx 3\n1\n\n\nEx 4\n1\n\n\nEx 5\n1\n\n\nEx 6\n1\n\n\nEx 7\n2\n\n\nEx 8\n2\n\n\nEx 9\n1\n\n\nEx 10\n1\n\n\nGrammar & Writing\n11\n\n\nWorkflow & formatting\n12"
  },
  {
    "objectID": "hw/hw-06.html#footnotes",
    "href": "hw/hw-06.html#footnotes",
    "title": "Homework 06: Candy Competition",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Grammar & Writing” grade is decided based on your grammar and writing. This is typically decided by choosing one of the questions and assessing the writing.↩︎\nThe “Workflow & formatting” grade is to assess the reproducible workflow and document format. This includes having a neatly organized document with readable code and your name and the date in the YAML.↩︎"
  },
  {
    "objectID": "hw/hw-08.html",
    "href": "hw/hw-08.html",
    "title": "HW 08: Lego Prices",
    "section": "",
    "text": "In this homework you will use multiple linear regression to fit and evaluate models using characteristics of LEGO sets to understand variability in the price.\n\n\nIn this assignment, you will…\n\nUse exploratory data analysis to inform feature engineering steps\nEvaluate and compare models\nAssess model conditions\nUse inference to draw conclusions\n\n\n\n\n\nGo to RStudio and login with your College of Idaho Email and Password.\nMake a subfolder in your hw directory to store this homework.\nLog into Canvas, navigate to Homework 8 and upload the hw-08.qmd and lego-sample.csv files into the folder your just made.\n\n\n\n\nThe following packages will be used in this assignment:\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(ggformula)\nlibrary(knitr) \nlibrary(rms)\nlibrary(patchwork)\n# add other packages as needed\n\n\n\n\n\n\n\nImportant\n\n\n\nAll narrative should be written in complete sentences, and all visualizations should have informative titles and axis labels.\n\n\n\n\n\nThe data for this analysis includes information about LEGO sets from themes produced January 1, 2018 and September 11, 2020. The data were originally scraped from Brickset.com, an online LEGO set guide and were obtained for this assignment from Peterson and Ziegla (2021).\nYou will work with data on about 400 randomly selected LEGO sets produced during this time period. The primary variables are interest in this analysis are\n\nPieces: Number of pieces in the set from brickset.com.\nMinifigures: Number of minifigures (LEGO people) in the set scraped from brickset.com. LEGO sets with no minifigures have been coded as NA. NA’s also represent missing data. This is due to how brickset.com reports their data.\nAmazon_Price: Amazon price of the set scraped from brickset.com (in U.S. dollars)\nSize: General size of the interlocking bricks (Large = LEGO Duplo sets - which include large brick pieces safe for children ages 1 to 5, Small = LEGO sets which- include the traditional smaller brick pieces created for age groups 5 and - older, e.g., City, Friends)\nTheme: Theme of the LEGO set\nYear : Year the LEGO set was produced\nPages: Number of pages in the instruction booklet"
  },
  {
    "objectID": "hw/hw-08.html#learning-goals",
    "href": "hw/hw-08.html#learning-goals",
    "title": "HW 08: Lego Prices",
    "section": "",
    "text": "In this assignment, you will…\n\nUse exploratory data analysis to inform feature engineering steps\nEvaluate and compare models\nAssess model conditions\nUse inference to draw conclusions"
  },
  {
    "objectID": "hw/hw-08.html#getting-started",
    "href": "hw/hw-08.html#getting-started",
    "title": "HW 08: Lego Prices",
    "section": "",
    "text": "Go to RStudio and login with your College of Idaho Email and Password.\nMake a subfolder in your hw directory to store this homework.\nLog into Canvas, navigate to Homework 8 and upload the hw-08.qmd and lego-sample.csv files into the folder your just made."
  },
  {
    "objectID": "hw/hw-08.html#packages",
    "href": "hw/hw-08.html#packages",
    "title": "HW 08: Lego Prices",
    "section": "",
    "text": "The following packages will be used in this assignment:\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(ggformula)\nlibrary(knitr) \nlibrary(rms)\nlibrary(patchwork)\n# add other packages as needed\n\n\n\n\n\n\n\nImportant\n\n\n\nAll narrative should be written in complete sentences, and all visualizations should have informative titles and axis labels."
  },
  {
    "objectID": "hw/hw-08.html#data-legos",
    "href": "hw/hw-08.html#data-legos",
    "title": "HW 08: Lego Prices",
    "section": "",
    "text": "The data for this analysis includes information about LEGO sets from themes produced January 1, 2018 and September 11, 2020. The data were originally scraped from Brickset.com, an online LEGO set guide and were obtained for this assignment from Peterson and Ziegla (2021).\nYou will work with data on about 400 randomly selected LEGO sets produced during this time period. The primary variables are interest in this analysis are\n\nPieces: Number of pieces in the set from brickset.com.\nMinifigures: Number of minifigures (LEGO people) in the set scraped from brickset.com. LEGO sets with no minifigures have been coded as NA. NA’s also represent missing data. This is due to how brickset.com reports their data.\nAmazon_Price: Amazon price of the set scraped from brickset.com (in U.S. dollars)\nSize: General size of the interlocking bricks (Large = LEGO Duplo sets - which include large brick pieces safe for children ages 1 to 5, Small = LEGO sets which- include the traditional smaller brick pieces created for age groups 5 and - older, e.g., City, Friends)\nTheme: Theme of the LEGO set\nYear : Year the LEGO set was produced\nPages: Number of pages in the instruction booklet"
  },
  {
    "objectID": "hw/hw-08.html#exercise-1",
    "href": "hw/hw-08.html#exercise-1",
    "title": "HW 08: Lego Prices",
    "section": "Exercise 1",
    "text": "Exercise 1\nThe data are contained in lego-sample.csv. Use the code below to read in the data, replace the NAs in Minifigure with 0, and remove any observations that have missing values for the relevant variables.\n\nlegos &lt;- read_csv(\"../data/lego-sample.csv\") |&gt;\n  select(Size, Pieces, Theme, Amazon_Price, Year, Pages, Minifigures) |&gt;\n  mutate(Minifigures = replace_na(Minifigures, 0)) |&gt;\n  drop_na()\n\nRows: 400 Columns: 15\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): Set_Name, Theme, Ages, Packaging, Weight, Availability, Size\ndbl (8): Item_Number, Pieces, Price, Amazon_Price, Year, Pages, Minifigures,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet’s think about the analysis decisions we just made. We decided to drop all observations that have missing values for any of the relevant variables we identified. What are these advantages and disadvantages of doing this? Feel free to Google “structurally missing data”. We also replaced ALL of the NAs in Minifigures with 0’s. What are the advantages and disadvantages of doing this? Make sure you read the description of Minifigures in the introduction before answering."
  },
  {
    "objectID": "hw/hw-08.html#exercise-2",
    "href": "hw/hw-08.html#exercise-2",
    "title": "HW 08: Lego Prices",
    "section": "Exercise 2",
    "text": "Exercise 2\nVisualize the distributions of the predictor variables Pieces, Size, Year, and Pages. Neatly arrange the plots using the patchwork package. Create univariate visualizations not bivariate visualizations."
  },
  {
    "objectID": "hw/hw-08.html#exercise-3",
    "href": "hw/hw-08.html#exercise-3",
    "title": "HW 08: Lego Prices",
    "section": "Exercise 3",
    "text": "Exercise 3\nThe distribution of Theme is shown below. The bars are ordered by the frequency they occur in the data set.\n\nlegos |&gt;\n  count(Theme) |&gt;\n  mutate(Theme = fct_reorder(Theme, n)) |&gt; \ngf_col(Theme ~ n) |&gt; \n    gf_labs(title = \"Lego Set Theme\", \n         y = \"Theme\", \n         x = \"Number of LEGO sets\")\n\n\n\n\n\n\n\n\nBased solely on information that is visible in this plot, what is one reason we should avoid putting the variable Theme in a model as is?"
  },
  {
    "objectID": "hw/hw-08.html#exercise-4",
    "href": "hw/hw-08.html#exercise-4",
    "title": "HW 08: Lego Prices",
    "section": "Exercise 4",
    "text": "Exercise 4\nCreate a new variable called Theme_coarse that collapses any levels of Theme with fewer than 20 observations into a single category called Other. Hint: Look into the function fct_lump_min. You’ll want to use it inside a mutate statement."
  },
  {
    "objectID": "hw/hw-08.html#exercise-5",
    "href": "hw/hw-08.html#exercise-5",
    "title": "HW 08: Lego Prices",
    "section": "Exercise 5",
    "text": "Exercise 5\nFit a model that uses the variables Size, Theme_coarse, and Pages to predict Amazon_Price. Call this model1."
  },
  {
    "objectID": "hw/hw-08.html#exercise-6",
    "href": "hw/hw-08.html#exercise-6",
    "title": "HW 08: Lego Prices",
    "section": "Exercise 6",
    "text": "Exercise 6\nCreate a new variable called since2018 that calculates the number of years since 2018 based on the variable Year."
  },
  {
    "objectID": "hw/hw-08.html#exercise-7",
    "href": "hw/hw-08.html#exercise-7",
    "title": "HW 08: Lego Prices",
    "section": "Exercise 7",
    "text": "Exercise 7\nNow let’s consider a new model that includes all the variables used in the model from Exercise 5 along with since2018, Pieces, and Minifigures. Call this model2"
  },
  {
    "objectID": "hw/hw-08.html#exercise-8",
    "href": "hw/hw-08.html#exercise-8",
    "title": "HW 08: Lego Prices",
    "section": "Exercise 8",
    "text": "Exercise 8\nCompute the \\(R^2\\), \\(R^2_{adj}\\), AIC, and BIC for model1 and model2. Based on your results, what model do you believe is the “best” model? Justify your answer citing specific evidence from the values you just generated. Note, they may not all be useful."
  },
  {
    "objectID": "hw/hw-08.html#exercise-9",
    "href": "hw/hw-08.html#exercise-9",
    "title": "HW 08: Lego Prices",
    "section": "Exercise 9",
    "text": "Exercise 9\nFor whichever model you chose above, determine whether the conditions for inference are met."
  },
  {
    "objectID": "hw/hw-08.html#exercise-10",
    "href": "hw/hw-08.html#exercise-10",
    "title": "HW 08: Lego Prices",
    "section": "Exercise 10",
    "text": "Exercise 10\nFor the same model, conduct an F-test and interpret your results in the context of the problem. Make sure to:\n\nIdentify the null and alternative hypotheses.\nCompute the F-statistic.\nCompute the p-value.\nInterpret the result of your test at a significance level of \\(\\alpha = 0.05\\)"
  },
  {
    "objectID": "hw/hw-08.html#footnotes",
    "href": "hw/hw-08.html#footnotes",
    "title": "HW 08: Lego Prices",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Grammar & Writing” grade is decided based on your grammar and writing. This is typically decided by choosing one of the questions and assessing the writing.↩︎\nThe “Workflow & formatting” grade is to assess the reproducible workflow and document format. This includes having a neatly organized document with readable code and your name and the date in the YAML.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH 2025: Multiple Regression Analysis",
    "section": "",
    "text": "An introduction to statistical modeling and methods. Topics may include model construction and analysis using multiple linear regression, analysis of variance, and logistic regression. The course makes substantial use of statistical software. Students who have taken more than one calculus course should generally take MAT-212 rather than MAT-125.\n\n\n\nWe will cover approaches for analyzing multivariate data sets, emphasizing linear and logistic regression. You will learn techniques for checking the appropriateness of proposed models, such as residual analyses, and techniques for selecting models. You will gain experience dealing with the challenges that arise in practice through assignments that utilize real-world data. This class emphasizes data analysis over mathematical theory."
  },
  {
    "objectID": "index.html#catalog-description",
    "href": "index.html#catalog-description",
    "title": "MATH 2025: Multiple Regression Analysis",
    "section": "",
    "text": "An introduction to statistical modeling and methods. Topics may include model construction and analysis using multiple linear regression, analysis of variance, and logistic regression. The course makes substantial use of statistical software. Students who have taken more than one calculus course should generally take MAT-212 rather than MAT-125."
  },
  {
    "objectID": "index.html#erics-addition",
    "href": "index.html#erics-addition",
    "title": "MATH 2025: Multiple Regression Analysis",
    "section": "",
    "text": "We will cover approaches for analyzing multivariate data sets, emphasizing linear and logistic regression. You will learn techniques for checking the appropriateness of proposed models, such as residual analyses, and techniques for selecting models. You will gain experience dealing with the challenges that arise in practice through assignments that utilize real-world data. This class emphasizes data analysis over mathematical theory."
  },
  {
    "objectID": "index.html#instructor",
    "href": "index.html#instructor",
    "title": "MATH 2025: Multiple Regression Analysis",
    "section": "Instructor",
    "text": "Instructor\nEric is an Assistant Professor in the Department of Mathematics and Physical Sciences at the College of Idaho. His expertise lies in the areas of probability, statistics, and statistics education. He joined the College of Idaho faculty in 2024 after spending three years as an Assistant Professor at St. Norbert College in De Pere, Wisconsin. He received his bachelor’s degree in mathematics and statistics from Rice University in 2011. After earning his degree, he worked for Capital One for two years in their home loans division before enrolling in graduate school. In 2018, Eric received his Ph.D. in statistics and operations research from the University of North Carolina at Chapel Hill studying under professor Amarjit Budhiraja. His dissertation work focused on modeling and analyzing large systems which arise in industrial engineering (e.g. large server and communication networks). Following his Ph.D., Eric did a postdoc in the Department of Ecology & Evolution at the University of Chicago under the direction of professor Matthias Steinrücken where he used stochastic processes to study natural selection and population genetics.\nOutside of school, Eric is an avid fan of the New York Giants, New York Knicks, and North Carolina Tarheels. In addition, he enjoys comic books, the Fast and the Furious franchise, and spending time with his lovely wife Maria and lovable dogs Allie, Tony, and Miriam.\n\n\n\nOffice hours (Boone 126B)\n\n\n\n\n\nMTWRF\n11:00am-12:00pm\n\n\nor by appointment."
  },
  {
    "objectID": "prepare/prep-02.html",
    "href": "prepare/prep-02.html",
    "title": "Preparation for Simple Linear Regression",
    "section": "",
    "text": "Review Syllabus\nRead Chapter 0 of Stat2\nStat2 Textbook: Chapter 1.1 The Simple Linear Regression Model"
  },
  {
    "objectID": "prepare/prep-02.html#assigned-reading",
    "href": "prepare/prep-02.html#assigned-reading",
    "title": "Preparation for Simple Linear Regression",
    "section": "",
    "text": "Review Syllabus\nRead Chapter 0 of Stat2\nStat2 Textbook: Chapter 1.1 The Simple Linear Regression Model"
  },
  {
    "objectID": "prepare/prep-02.html#additional-resources-optional",
    "href": "prepare/prep-02.html#additional-resources-optional",
    "title": "Preparation for Simple Linear Regression",
    "section": "Additional resources (optional)",
    "text": "Additional resources (optional)\n📖 Introduction to Regression Analysis: A Data Science Approach: Ch 4: Simple linear regression\n📖 Introduction to Modern Statistics (IMS) Ch 7: Linear regression with a single predictor"
  },
  {
    "objectID": "prepare/prep-04.html",
    "href": "prepare/prep-04.html",
    "title": "Preparation for Mathematical Models for Inference",
    "section": "",
    "text": "Stat2 Textbook: Chapter 2.1, 2.3, 2.3"
  },
  {
    "objectID": "prepare/prep-04.html#assigned-reading",
    "href": "prepare/prep-04.html#assigned-reading",
    "title": "Preparation for Mathematical Models for Inference",
    "section": "",
    "text": "Stat2 Textbook: Chapter 2.1, 2.3, 2.3"
  },
  {
    "objectID": "prepare/prep-06.html",
    "href": "prepare/prep-06.html",
    "title": "Preparation for Model Transformation and Outliers",
    "section": "",
    "text": "Stat2 Textbook: Chapter 1.4, 1.5"
  },
  {
    "objectID": "prepare/prep-06.html#assigned-reading",
    "href": "prepare/prep-06.html#assigned-reading",
    "title": "Preparation for Model Transformation and Outliers",
    "section": "",
    "text": "Stat2 Textbook: Chapter 1.4, 1.5"
  },
  {
    "objectID": "prepare/prep-08.html",
    "href": "prepare/prep-08.html",
    "title": "Preparation for Multiple Linear Regression Inference and Comparison",
    "section": "",
    "text": "Stat2 Textbook: Chapter 3.2"
  },
  {
    "objectID": "prepare/prep-08.html#assigned-reading",
    "href": "prepare/prep-08.html#assigned-reading",
    "title": "Preparation for Multiple Linear Regression Inference and Comparison",
    "section": "",
    "text": "Stat2 Textbook: Chapter 3.2"
  },
  {
    "objectID": "prepare/prep-10.html",
    "href": "prepare/prep-10.html",
    "title": "Preparation for Inference and Conditions for Logistic Regression",
    "section": "",
    "text": "Stat2 Textbook: Chapters 9.3, 9.4"
  },
  {
    "objectID": "prepare/prep-10.html#assigned-reading",
    "href": "prepare/prep-10.html#assigned-reading",
    "title": "Preparation for Inference and Conditions for Logistic Regression",
    "section": "",
    "text": "Stat2 Textbook: Chapters 9.3, 9.4"
  },
  {
    "objectID": "prepare/prep-12.html",
    "href": "prepare/prep-12.html",
    "title": "Preparation for Multiple Logistic Regression",
    "section": "",
    "text": "Stat2 Textbook: Chapter 10.1, 10.2"
  },
  {
    "objectID": "prepare/prep-12.html#assigned-reading",
    "href": "prepare/prep-12.html#assigned-reading",
    "title": "Preparation for Multiple Logistic Regression",
    "section": "",
    "text": "Stat2 Textbook: Chapter 10.1, 10.2"
  },
  {
    "objectID": "prepare/prep-14.html",
    "href": "prepare/prep-14.html",
    "title": "Preparation for Model Comparison",
    "section": "",
    "text": "Stat2 Textbook: Chapter 3.2"
  },
  {
    "objectID": "prepare/prep-14.html#assigned-reading",
    "href": "prepare/prep-14.html#assigned-reading",
    "title": "Preparation for Model Comparison",
    "section": "",
    "text": "Stat2 Textbook: Chapter 3.2"
  },
  {
    "objectID": "project/project-instructions.html",
    "href": "project/project-instructions.html",
    "title": "Final Project Instructions",
    "section": "",
    "text": "TL;DR: Pick a data set and do a regression analysis. That is your final project.\nIn this project, you will select a data set of interest to you, pose a research question that you will attempt to answer using multiple linear and/or multiple logistic regression, and write a paper in a formal scientific style. The data set can come from one of the pre-selected data sets made available on Canvas. However, if you have data from research you have conducted or another source which you are passionate about, you may use that data subject to instructor permission.\n\n\nThere are four major deliverables for the final project:\n\nA written, reproducible report detailing your analysis to be submitted in either Microsoft word or a PDF and uploaded to Canvas.\nThe .qmd and .html files from a Quarto appendix you create.\nFormal peer review on another team’s work and presentation feedback.\nA PDF of a poster to be uploaded to Canvas which you will present at during the last day of class.\n\nHowever, due to the condensed nature of winter term, you will have small deliverable due on most days of class to make sure no one get’s left behind.\n\n\n\nFrom Christine Schutz:\n\nAn essential (and favorite) aspect of a librarian’s work is to provide assistance and coaching to students doing research for their projects and papers. Our two librarians - Christine Schutz and Lance McGrath – can help students to focus their source quests and locate appropriate and useful sources. Since students don’t always realize that not everyone who works in a library is a librarian (or even what that means) and that librarians aren’t in the library at all hours of the night (even though the library is open), please nudge them to reach out to us by email or Teams chat to set up a time to meet (or we can sometimes help over email/chat). From the history of the sufganiyot (this was last Fall’s favorite) to the Lamington crayfish (Euastacus sulcatus) and why they are blue (an all-time favorite of mine), student research projects are the best part of the job (and I know Lance feels the same way), please send those students our way.\n\nYou are more than welcome to contact me with any questions about your project but please do not hesitate reaching out to Christine or Lance with any questions.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project/project-instructions.html#overview",
    "href": "project/project-instructions.html#overview",
    "title": "Final Project Instructions",
    "section": "",
    "text": "TL;DR: Pick a data set and do a regression analysis. That is your final project.\nIn this project, you will select a data set of interest to you, pose a research question that you will attempt to answer using multiple linear and/or multiple logistic regression, and write a paper in a formal scientific style. The data set can come from one of the pre-selected data sets made available on Canvas. However, if you have data from research you have conducted or another source which you are passionate about, you may use that data subject to instructor permission.\n\n\nThere are four major deliverables for the final project:\n\nA written, reproducible report detailing your analysis to be submitted in either Microsoft word or a PDF and uploaded to Canvas.\nThe .qmd and .html files from a Quarto appendix you create.\nFormal peer review on another team’s work and presentation feedback.\nA PDF of a poster to be uploaded to Canvas which you will present at during the last day of class.\n\nHowever, due to the condensed nature of winter term, you will have small deliverable due on most days of class to make sure no one get’s left behind.\n\n\n\nFrom Christine Schutz:\n\nAn essential (and favorite) aspect of a librarian’s work is to provide assistance and coaching to students doing research for their projects and papers. Our two librarians - Christine Schutz and Lance McGrath – can help students to focus their source quests and locate appropriate and useful sources. Since students don’t always realize that not everyone who works in a library is a librarian (or even what that means) and that librarians aren’t in the library at all hours of the night (even though the library is open), please nudge them to reach out to us by email or Teams chat to set up a time to meet (or we can sometimes help over email/chat). From the history of the sufganiyot (this was last Fall’s favorite) to the Lamington crayfish (Euastacus sulcatus) and why they are blue (an all-time favorite of mine), student research projects are the best part of the job (and I know Lance feels the same way), please send those students our way.\n\nYou are more than welcome to contact me with any questions about your project but please do not hesitate reaching out to Christine or Lance with any questions.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project/project-instructions.html#timeline-grading",
    "href": "project/project-instructions.html#timeline-grading",
    "title": "Final Project Instructions",
    "section": "Timeline & Grading",
    "text": "Timeline & Grading\nCheckpoint I: Select a partner (0%)\n\ndue Monday, January 6\n\nCheckpoint II: Select a data set and write a research question (1%)\n\ndue Tuesday, January 7\n\nCheckpoint III: Literature Review (1%)\n\ndue Wednesday, January 8\n\nCheckpoint IV: Annotated Bibliography (1%)\n\ndue Thursday, January 9\n\nCheckpoint V: Project Proposal (2%)\n\ndue Friday, January 10\n\nCheckpoint VI: Load and Clean the Data Set (1%)\n\ndue Monday, January 13\n\nCheckpoint VII: Generate Summaries (1%)\n\ndue Tuesday, January 14\n\nCheckpoint VIII: Analyze Plots and Tables (1%)\n\ndue Wednesday, January 15\n\nCheckpoint IX: Exploratory Data Analysis (2%)\n\ndue Thursday, January 16\n\n\n\n\n\nCheckpoint X: Poster Presentation (50%)\n\ndue in Canvas by Friday, January 24th, 11:59am\nposters presentations will be on TVs in CML (Library) 105 from 1pm-3:30pm on Friday, January 24th\n\nCheckpoint XI: Supplementary Report (40%)\n\ndue in Canvas by Friday, January 24th, 11:59pm",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project/project-instructions.html#partner",
    "href": "project/project-instructions.html#partner",
    "title": "Final Project Instructions",
    "section": "Checkpoint I: Select a partner",
    "text": "Checkpoint I: Select a partner\nYour project will be completed with a partner. Try and select a partner with similar interests to you. Anyone who has not selected a partner by the deadline will be assigned a partner.\nDeliverables: Put yourselves in a group on Canvas.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project/project-instructions.html#research-question",
    "href": "project/project-instructions.html#research-question",
    "title": "Final Project Instructions",
    "section": "Checkpoint II: Select a data set and write a research question",
    "text": "Checkpoint II: Select a data set and write a research question\nSelect one of the pre-approved data set and identify the research question which will guide your project (e.g. “Do youth who participate in physical exercise class have lower BMI?”, “Are males more likely to drink and drive after adjusting for confounding variables?”) and briefly describe why your chosen project is interesting to you.\nDeliverables: Word document identifying which data set you plan to use, the response variable of interest, and a brief description of why your chosen project is interesting to you.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project/project-instructions.html#lit-review",
    "href": "project/project-instructions.html#lit-review",
    "title": "Final Project Instructions",
    "section": "Checkpoint III: Literature Review",
    "text": "Checkpoint III: Literature Review\nMeet with Christine Schutz and find articles in the refereed literature that are relevant to your question of interest. You must find at least six. You should avoid articles that are too technical to be relevant to the project or to be informative for the non-specialist. You must read the abstracts for each of these papers. Be sure you obtain the entire paper and not just an abstract! You will eventually use these references in the introduction of your paper.\nDeliverables: Word document containing the citation for each reference (in a standard format) and a link, if appropriate. Note that you are REQUIRED to meet with Christine to receive credit.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project/project-instructions.html#annotated-bib",
    "href": "project/project-instructions.html#annotated-bib",
    "title": "Final Project Instructions",
    "section": "Checkpoint IV: Annotated Bibliography",
    "text": "Checkpoint IV: Annotated Bibliography\nBased on the abstracts, choose two of the papers you listed in your literature review. For these two papers, write a few sentences summarizing the primary findings and how they relate to your research question.\nDeliverables: Take the word document you submitted for Checkpoint III and, below your two chosen articles, add a few sentences that summarize the primary findings and how they relate to your proposal.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project/project-instructions.html#project-proposal",
    "href": "project/project-instructions.html#project-proposal",
    "title": "Final Project Instructions",
    "section": "Checkpoint V: Project proposal",
    "text": "Checkpoint V: Project proposal\nYour proposal, to be turned in as a Word document or PDF on Canvas, will include the following. Note that some of the information below can be copied from previous Checkpoints:\n\nIdentify the original data source. Include a brief summary of how, from whom, and by whom, the data were collected. Describe how the study design will impact the generalizability of any analysis.\nIdentify the research question which will guide your project (e.g. “Do youth who participate in physical exercise class have lower BMI?”, “Are males more likely to drink and drive after adjusting for confounding variables?”).\nProvide a list of variables of interest and their definitions (including units). You should also include rationale for inclusion for each variable and identify the variable type, and whether it may need recoding. You should include at least 4 variables. A table is a good way to summarize this information; for example:\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable Name\nOriginal Definition\nUnits\nRange or Levels\nPossible recoding\nRationale\n\n\nbmi\nBody mass index\nKg/m^2\n&gt;0\n\nResponse variable\n\n\npe\nHow many days per week attends Physical education class\nDays/week\n0-5, integers\nCurrently categorical var. Recode to same values but numeric\nMain explanatory var of interest\n\n\nage\nAge of student\nyears\n12-19, integers\n\nPossible con founding var\n\n\nlunch\nPercent of students at the school receiving free /reduced lunch\n%\n0-100\n\nPossible con founding var (soc io-econ status)\n\n\n\n\n\nYour partially annotated bibliography. There must be at least six articles and at least two must be annotated.\n\nDeliverables: Word document or PDF with the information above. You may receive points back on Checkpoints II-IV for your work on this assignment.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project/project-instructions.html#load-data",
    "href": "project/project-instructions.html#load-data",
    "title": "Final Project Instructions",
    "section": "Checkpoint VI: Load and Clean the Data Set",
    "text": "Checkpoint VI: Load and Clean the Data Set\nWe now begin the second stage of your project, the Exploratory Data Analysis portion.\nYou now need to load and “clean” your data set, make note of any problematic data and observations that need to be removed, determine whether you want to use the whole data set or a subset, and consider implications about any decisions you make about missing data. This website shares some simple approaches to missing data (and the relative advantages/disadvantages of each approach).\nDeliverables: A QMD and HTML file uploaded to Canvas containing the following:\n\nA chunk in which you load the data. See the Loading the Data section below. Note that you will be building on this document.\nAny filtering that is necessary. At the very least this should include a treatment of your missing data.\n\nA very brief discussion of whether your data represents a representative sample of your target population.\n\nThe number of variable and observations in your resulting data set.\nUnivariate plots of all your variables\n\nNo need to describe them yet but note any observations which seem troublesome.\n\nBivariate plots of your repose variables with all of your\n\nNo need to describe them yet but note any observations which seem troublesome.\n\n\n\nLoading the Data\nNation Financial Well-Being Survey: Simply download the csv from Canvas, upload it to Quarto, and read it using the read_csv function in the same way as we’ve been doing with activities and homeworks.\nBehavioral Risk Factor Surveillance Survey: The data set is extremely large which results in a few problems. It is too big for you to upload to RStudio yourself so I have already uploaded it to RStudio. Furthermore, if you load the entire data set into memory, RStudio will begin to run extremely slowly. As a result, it will be advantageous to only load the columns that you need. Use the code below to load only the columns that you need. Simply replace col_1, col_2, etc. with the names of your columns. You may add as many columns as you need, simple separate the names using commas.\n\nread_csv(\"/srv/R/MAT212_WIN25/brfss2023.csv\") |&gt; \n  select(col_1, col_2, col_3)",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project/project-instructions.html#plots-tables",
    "href": "project/project-instructions.html#plots-tables",
    "title": "Final Project Instructions",
    "section": "Checkpoint VII: Generate Plots and Tables",
    "text": "Checkpoint VII: Generate Plots and Tables\nGenerate univariate and bivariate summaries of your data.\nDeliverables: A QMD and HTML file uploaded to Canvas. You should start with the same file you used for your last checkpoint and add the following:\n\nUnivariate summaries:\n\nA plot for every variable you are considering\nA table of every variable you are considering\n\nBivariate summaries:\n\nA plot comparing your response variable to all of your explanatory variables\nUse the function ggpairs from the package GGally to display a grid of all bivariate plots for your data. Feel free to use any resource available to use without citing it (e.g. ChatGPT, tutor, Dr. F, etc…)\n\nNote that you don’t need to analyze the plots yet… just create them.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project/project-instructions.html#plots-tables-analysis",
    "href": "project/project-instructions.html#plots-tables-analysis",
    "title": "Final Project Instructions",
    "section": "Checkpoint VIII: Analyze Plots and Tables",
    "text": "Checkpoint VIII: Analyze Plots and Tables\nNow that you’ve created all of your plots and tables, it’s time to start figuring out which ones are important and which ones you need. Each plot and table you have in your final EDA should have a purpose. That is, there should be something in that plot for you to talk about and point to. If not, then it’s not worth including. For this checkpoint I will have you go through all of your plots and tables, determine whether they are worth including, and summarize why you are including each plot. In general, plots are better than tables for visualizing patters, whereas tables are better if you want to reference specific numbers.\nDeliverables: A QMD and HTML file uploaded to Canvas. You should start with the same file you used for your last checkpoint and add the following:\n\nUnder each plot and table:\n\nKeep: Do you plan to keep this plot in your EDA. (If no, skip the rest). The answer to most of these should be no.\nKey insights: What’s important piece(s) of information is in this plot/table and how does it relate back to your research question?\nRedundancy: Is there information in the plot/table that is not evident in any of the other plots and tables? If the answer to this is no, why are you including it?\nMedium: Why are you using a plot/table to display this information rather than a table/plot to display this information?",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project/project-instructions.html#exploratory-analysis",
    "href": "project/project-instructions.html#exploratory-analysis",
    "title": "Final Project Instructions",
    "section": "Checkpoint IX: Exploratory Data Analysis",
    "text": "Checkpoint IX: Exploratory Data Analysis\nYou will now be synthesizing what you did in the preview three checkpoint into an exploratory data analysis report. You may receive points back on the previous three checkpoints for your work here. Note that you should not be fitting any models at this point.\nDeliverables: Your EDA report, to be turned in on Canvas, will meet these guidelines:\nIn no more than 3 pages, summarize the main findings of your exploratory analysis, referring to specific plots and summary statistics where necessary. In addition, describe your plans for building models to address important research questions, including which variables will be important to consider in light of your exploratory analyses. This report should be meaningful and readable to someone familiar with statistics but unfamiliar with your particular research topic and dataset (i.e. your professor). Give concise but precise statements interpreting your plots, tables, summary statistics, etc. – in the context of your data set and research question you pose. Avoid vague terms like “this data”, “these results”, etc. Also avoid cryptic variable names that you may have used in R. A report like this might be something you’d share with collaborators or store as a reference as you proceed with your analysis.\n\nThe Main Body of your EDA report should follow these guidelines:\n\nNo more than 3 pages\nBe submitted as a word document or PDF and include no code.\nBegin with a short paragraph introducing your project and primary research questions. (This introduction will be expanded into several paragraphs for the final paper.)\nYour next paragraph should be about your data. You should address where it’s from, how the study was conducted, and whether it contains a representative sample of the population you’re interested in.\nUse your graphical and numerical summaries to tell a story, supporting your conclusions with plots, tables, and summary statistics. Weave numerical summaries and graphs seamlessly into your text.\nYou do not need to include EVERY plot or table you made in the report but include at least 2 interesting plots/tables (if not more!). Name each figure (e.g. Figure 1) so they are easily referred to in your report. These exploratory plots/tables don’t have to be perfect in terms of titles and labels, but for your final paper it is essential that your figures have (meaningful) captions and axis labels!\nPreview directions you plan to go with modeling. What models will you begin by fitting, and what variables will be involved. This should be the last paragraph of the report.\nWrite well! Complete sentences, good flow, proper grammar, etc.\n\nYour EDA report should also include an Annotated Appendix and References section (not included in the 3 page limit) which include these elements:\n\nClear definitions of important variables and the (properly cited) source of the data.\nTables and figures that are informative but were not referenced specifically in the main report. Include a short annotation – one or two sentences on what they show.\nA citation for each reference article (in a standard format) you included in your proposal. Also include a link, if appropriate.\n\nAll of the code you used to generate the plots and graphs for this EDA should be included in your previous checkpoint. However, you are welcome to make changes. If you do, please re-upload the code and HTML file to Checkpoint VIII so I can reference it if I need to.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project/project-instructions.html#poster-presentation",
    "href": "project/project-instructions.html#poster-presentation",
    "title": "Final Project Instructions",
    "section": "Checkpoint X: Poster Presentation",
    "text": "Checkpoint X: Poster Presentation\nThe following is from Passion-Driven Statistics:\nYou have conducted a quantitative research project. Now you will learn how to present your results as a research poster and presentation. Posters offer the opportunity to engage with an audience and to meaningfully disseminate your research findings to others.\nLearn keys to a successful poster presentation. Consider your audience and frame your research question and results in an understandable and interesting way. Understand you should be brief, use large font size, and incorporate graphics instead of text whenever possible. See how being clear and concise with a logical layout will ensure the viewing experience is intellectually and aesthetically satisfying for your audience. Click HERE to a watch the video lesson.\nYou will create a 3 column, 40” X 36” poster including an introduction, research questions, methods, results, and discussion.\nDeliverables: Your poster, to be turned in on Canvas, will be a PDF. A rubric can be found here and poster template can be found on Canvas.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project/project-instructions.html#supplementary-presentation",
    "href": "project/project-instructions.html#supplementary-presentation",
    "title": "Final Project Instructions",
    "section": "Checkpoint XI: Supplementary Report",
    "text": "Checkpoint XI: Supplementary Report\nYour supplementary report should… wait for it… supplment your poster presentation.\nDeliverables: Your Supplementary Report report, to be turned in on Canvas, should be created in Canvas. A rubric can be found here.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project/project-rubric.html",
    "href": "project/project-rubric.html",
    "title": "Supplementary report rubric",
    "section": "",
    "text": "Applied Stats Individual Project Evaluation\nPossible\nScore\n\n\n\n\nData import and data cleaning – 5 pts\n\n\n\n\nData is imported appropriately\n1\n\n\n\nMissing values are dealt with appropriately and intentionally\n1\n\n\n\nCategorical variables are recoded as factors\n1\n\n\n\nThe initial and final number of observations in the analysis reported and are those that were removed accounted for.\n1\n\n\n\nData preparation and cleaning procedures are clearly described so that the study could be replicated by another researcher.\n1\n\n\n\nModeling – 20 pts\n\n\n\n\nThoughtful variable selection employed to decide upon the final model. The the final model was compared with simpler options, and the form of the final model was justified.\n5\n\n\n\nThe conditions of the model were checked.\n5\n\n\n\nIf necessary, data transformations were explored.\n5\n\n\n\nInteraction were explored and were included in the model when appropriat.\n5\n\n\n\nResults – 15 pts\n\n\n\n\nFinal model is reported with parameter estimates, standard errors, p-values, and confidence intervals.\n5\n\n\n\nParameter estimates and CIs werecorrectly interpreted in the context of the research question.\n5\n\n\n\nImplications of the results are discussed in the context of the research question.\n2\n\n\n\nAa thoughtful discussion of generalizability of the results and limitations of the study are identified.\n1\n\n\n\nStrengths and weaknesses of the analysis are identified, including a discussion of model conditions.\n2",
    "crumbs": [
      "Project",
      "Supplementary Report Rubric"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses, with all changes documented here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlecture\ndow\ndate\ntopic\nprepare\nslides\nae\nhw\nproject\nnotes\n\n\n\n\n1\nF/M\nJan 3\nWelcome to MAT 212\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2\nM/T\nJan 6\nIntroduction to Simple Linear Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3\nT/W\nJan 7\nSimulation-Based Inference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4\nTH\nJan 9\nSLR: Mathematical models for inference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5\nF/M\nJan 10\nSLR: Conditions + Evaluation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6\nM\nJan 13\nSLR: Transformations and Outliers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7\nT/W\nJan 14\nIntro to Multiple Linear Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8\nW/TH\nJan 15\nMLR: Inference and Conditions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9\nF\nJan 16\nIntroduction to Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10\nT\nJan 21\nLogistic Regression: Inference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n11\nW\nJan 22\nModel Comparison\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12\nTH\nJan 23\nMultiple Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n13\nF\nJan 27\nPoster Presentations",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "slides/02-slr-intro.html#application-exercise",
    "href": "slides/02-slr-intro.html#application-exercise",
    "title": "Simple Linear Regression",
    "section": "Application exercise",
    "text": "Application exercise\n\n📋 AE 02 - DC Bikeshare\n\nComplete Exercises 0 and 1."
  },
  {
    "objectID": "slides/02-slr-intro.html#topics",
    "href": "slides/02-slr-intro.html#topics",
    "title": "Simple Linear Regression",
    "section": "Topics",
    "text": "Topics\n\nUse simple linear regression to describe the relationship between a quantitative predictor and quantitative response variable.\nEstimate the slope and intercept of the regression line using the least squares method.\nInterpret the slope and intercept of the regression line.\nUse R to fit and summarize regression models."
  },
  {
    "objectID": "slides/02-slr-intro.html#computation-set-up",
    "href": "slides/02-slr-intro.html#computation-set-up",
    "title": "Simple Linear Regression",
    "section": "Computation set up",
    "text": "Computation set up\n\n# load packages\nlibrary(tidyverse)       # for data wrangling\nlibrary(ggformula)       # for plotting\nlibrary(broom)           # for formatting model output\nlibrary(knitr)           # for formatting tables\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 16))\n\n# set default figure parameters for knitr\nknitr::opts_chunk$set(\n  fig.width = 8,\n  fig.asp = 0.618,\n  fig.retina = 3,\n  dpi = 300,\n  out.width = \"80%\"\n)"
  },
  {
    "objectID": "slides/02-slr-intro.html#dc-bikeshare",
    "href": "slides/02-slr-intro.html#dc-bikeshare",
    "title": "Simple Linear Regression",
    "section": "DC Bikeshare",
    "text": "DC Bikeshare\nOur data set contains daily rentals from the Capital Bikeshare in Washington, DC in 2011 and 2012. It was obtained from the dcbikeshare data set in the dsbox R package.\nWe will focus on the following variables in the analysis:\n\ncount: total bike rentals\ntemp_orig: Temperature in degrees Celsius\nseason: 1 - winter, 2 - spring, 3 - summer, 4 - fall\n\nClick here for the full list of variables and definitions.\nLet’s complete Exercises 2-6 together"
  },
  {
    "objectID": "slides/02-slr-intro.html#data-prep",
    "href": "slides/02-slr-intro.html#data-prep",
    "title": "Simple Linear Regression",
    "section": "Data prep",
    "text": "Data prep\n\nExercise 2: Recode season as a factor with names instead of numbers (livecode)\nRemember:\n\nThink of |&gt; as “and then”\nmutate creates new columns and changes (mutates) existing columns\nR calls categorical data “factors”\n\n\n\nbikeshare &lt;- read_csv(\"../data/dcbikeshare.csv\") |&gt; \n  mutate(season = case_when(\n    season == 1 ~ \"winter\",\n    season == 2 ~ \"spring\",\n    season == 3 ~ \"summer\",\n    season == 4 ~ \"fall\"\n  ),\n  season = factor(season))"
  },
  {
    "objectID": "slides/02-slr-intro.html#exploratory-data-analysis-exercise-3",
    "href": "slides/02-slr-intro.html#exploratory-data-analysis-exercise-3",
    "title": "Simple Linear Regression",
    "section": "Exploratory data analysis (Exercise 3)",
    "text": "Exploratory data analysis (Exercise 3)\n\ngf_point(count ~ temp_orig | season, data = bikeshare) |&gt; \n  gf_labs(x = \"Temperature (Celsius)\",\n          y = \"Daily bike rentals\")"
  },
  {
    "objectID": "slides/02-slr-intro.html#exploratory-data-analysis-exercise-3-1",
    "href": "slides/02-slr-intro.html#exploratory-data-analysis-exercise-3-1",
    "title": "Simple Linear Regression",
    "section": "Exploratory data analysis (Exercise 3)",
    "text": "Exploratory data analysis (Exercise 3)\n\ngf_point(count ~ temp_orig | season, data = bikeshare) |&gt; \n  gf_labs(x = \"Temperature (Celsius)\",\n          y = \"Daily bike rentals\")"
  },
  {
    "objectID": "slides/02-slr-intro.html#more-data-prep",
    "href": "slides/02-slr-intro.html#more-data-prep",
    "title": "Simple Linear Regression",
    "section": "More data prep",
    "text": "More data prep\n\n(Exercise 5) Filter your data for the season with the strongest relationship and give the resulting data set a new name\n\n\nwinter &lt;- bikeshare |&gt; \n  filter(season == \"winter\")"
  },
  {
    "objectID": "slides/02-slr-intro.html#rentals-vs-temperature",
    "href": "slides/02-slr-intro.html#rentals-vs-temperature",
    "title": "Simple Linear Regression",
    "section": "Rentals vs Temperature",
    "text": "Rentals vs Temperature\nGoal: Fit a line to describe the relationship between the temperature and the number of rentals in winter."
  },
  {
    "objectID": "slides/02-slr-intro.html#why-fit-a-line",
    "href": "slides/02-slr-intro.html#why-fit-a-line",
    "title": "Simple Linear Regression",
    "section": "Why fit a line?",
    "text": "Why fit a line?\nWe fit a line to accomplish one or both of the following:\n\n\nPrediction\n\nHow many rentals are expected when it’s 10 degrees out?\n\n\n\nInference\n\nIs temperature a useful predictor of the number of rentals? By how much is the number of rentals expected to change for each degree Celsius?"
  },
  {
    "objectID": "slides/02-slr-intro.html#population-vs.-sample",
    "href": "slides/02-slr-intro.html#population-vs.-sample",
    "title": "Simple Linear Regression",
    "section": "Population vs. Sample",
    "text": "Population vs. Sample\nPopulation: The set of items or events that you’re interested in and hoping (able) to generalize the results of your analysis to.\nSample: The set of items that you have data for.\nRepresentative Sample: A sample that looks like a small version of your population.\nGoal: Build a model from your sample which generalizes to your population."
  },
  {
    "objectID": "slides/02-slr-intro.html#terminology",
    "href": "slides/02-slr-intro.html#terminology",
    "title": "Simple Linear Regression",
    "section": "Terminology",
    "text": "Terminology\n\n\n\nResponse, Y: variable describing the outcome of interest\nPredictor, X: variable we use to help understand the variability in the response"
  },
  {
    "objectID": "slides/02-slr-intro.html#regression-model",
    "href": "slides/02-slr-intro.html#regression-model",
    "title": "Simple Linear Regression",
    "section": "Regression model",
    "text": "Regression model\nRegression model: a function that describes the relationship between a quantitative response, \\(Y\\), and the predictor, \\(X\\) (or many predictors).\n\\[\\begin{aligned} Y &= \\color{black}{\\textbf{Model}} + \\text{Error} \\\\[8pt]\n&= \\color{black}{\\mathbf{f(X)}} + \\epsilon \\\\[8pt]\n&= \\color{black}{\\boldsymbol{\\mu_{Y|X}}} + \\epsilon \\end{aligned}\\]"
  },
  {
    "objectID": "slides/02-slr-intro.html#regression-model-1",
    "href": "slides/02-slr-intro.html#regression-model-1",
    "title": "Simple Linear Regression",
    "section": "Regression model",
    "text": "Regression model\n\n\n\\[\\begin{aligned} Y &= \\color{purple}{\\textbf{Model}} + \\text{Error} \\\\[8pt]\n&= \\color{purple}{\\mathbf{f(X)}} + \\epsilon \\\\[8pt]\n&= \\color{purple}{\\boldsymbol{\\mu_{Y|X}}} + \\epsilon \\end{aligned}\\]\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mu_{Y|X}\\) is the mean value of \\(Y\\) given a particular value of \\(X\\)."
  },
  {
    "objectID": "slides/02-slr-intro.html#regression-model-2",
    "href": "slides/02-slr-intro.html#regression-model-2",
    "title": "Simple Linear Regression",
    "section": "Regression model",
    "text": "Regression model\n\n\n\\[\n\\begin{aligned} Y &= \\color{purple}{\\textbf{Model}} + \\color{blue}{\\textbf{Error}} \\\\[5pt]\n&= \\color{purple}{\\mathbf{f(X)}} + \\color{blue}{\\boldsymbol{\\epsilon}} \\\\[5pt]\n&= \\color{purple}{\\boldsymbol{\\mu_{Y|X}}} + \\color{blue}{\\boldsymbol{\\epsilon}} \\\\[5pt]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/02-slr-intro.html#slr-statistical-model",
    "href": "slides/02-slr-intro.html#slr-statistical-model",
    "title": "Simple Linear Regression",
    "section": "SLR: Statistical model",
    "text": "SLR: Statistical model\n\nSimple linear regression: model to describe the relationship between \\(Y\\) and \\(X\\) where:\n\n\\(Y\\) is a quantitative/numerical response\n\\(X\\) is a single quantitative predictor\n\\[\\Large{Y = \\mathbf{\\beta_0 + \\beta_1 X} + \\epsilon}\\]\n\n\n\n\n\\(\\beta_1\\): True slope of the relationship between \\(X\\) and \\(Y\\)\n\\(\\beta_0\\): True intercept of the relationship between \\(X\\) and \\(Y\\)\n\\(\\epsilon\\): Error"
  },
  {
    "objectID": "slides/02-slr-intro.html#slr-regression-equation",
    "href": "slides/02-slr-intro.html#slr-regression-equation",
    "title": "Simple Linear Regression",
    "section": "SLR: Regression equation",
    "text": "SLR: Regression equation\n\\[\\Large{\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X}\\]\n\n\\(\\hat{\\beta}_1\\): Estimated slope of the relationship between \\(X\\) and \\(Y\\)\n\\(\\hat{\\beta}_0\\): Estimated intercept of the relationship between \\(X\\) and \\(Y\\)\n\\(\\hat{Y}\\): Predicted value of \\(Y\\) for a given \\(X\\)\nNo error term!"
  },
  {
    "objectID": "slides/02-slr-intro.html#choosing-values-for-hatbeta_1-and-hatbeta_0",
    "href": "slides/02-slr-intro.html#choosing-values-for-hatbeta_1-and-hatbeta_0",
    "title": "Simple Linear Regression",
    "section": "Choosing values for \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\)",
    "text": "Choosing values for \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\)"
  },
  {
    "objectID": "slides/02-slr-intro.html#residuals",
    "href": "slides/02-slr-intro.html#residuals",
    "title": "Simple Linear Regression",
    "section": "Residuals",
    "text": "Residuals\n\n\\[\\text{residual} = \\text{observed} - \\text{predicted} = y_i - \\hat{y}_i\\]"
  },
  {
    "objectID": "slides/02-slr-intro.html#least-squares-line",
    "href": "slides/02-slr-intro.html#least-squares-line",
    "title": "Simple Linear Regression",
    "section": "Least squares line",
    "text": "Least squares line\n\nResidual for the \\(i^{th}\\) observation:\n\n\\[e_i = \\text{observed} - \\text{predicted}\n= y_i - \\hat{y}_i\\]\n\nSum of squared residuals:\n\n\\[e^2_1 + e^2_2 + \\dots + e^2_n\\]\n\nLeast squares line is the one that minimizes the sum of squared residuals"
  },
  {
    "objectID": "slides/02-slr-intro.html#properties-of-least-squares-regression",
    "href": "slides/02-slr-intro.html#properties-of-least-squares-regression",
    "title": "Simple Linear Regression",
    "section": "Properties of least squares regression",
    "text": "Properties of least squares regression\n\nPasses through center of mass point, the coordinates corresponding to average \\(X\\) and average \\(Y\\): \\(\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X}\\)\nSlope has same sign as the correlation coefficient: \\(\\hat{\\beta}_1 = r \\frac{s_Y}{s_X}\\)\n\n\\(r\\): correlation coefficient\n\\(s_Y, s_X\\): sample standard deviations of \\(X\\) and \\(Y\\)\n\nSum of the residuals is zero: \\(\\sum_{i = 1}^n e_i \\approx 0\\)\n\nIntuition: Residuals are “balanced”\n\nThe residuals and \\(X\\) values are uncorrelated"
  },
  {
    "objectID": "slides/02-slr-intro.html#estimating-the-slope",
    "href": "slides/02-slr-intro.html#estimating-the-slope",
    "title": "Simple Linear Regression",
    "section": "Estimating the slope",
    "text": "Estimating the slope\n\\[\\large{\\hat{\\beta}_1 = r \\frac{s_Y}{s_X}}\\]\n\n\n\\[\\begin{aligned}\ns_X &= 4.2121 \\\\\ns_Y &= 1399.942 \\\\\nr &= 0.6692\n\\end{aligned}\\]\n\n\\[\\begin{aligned}\n\\hat{\\beta}_1 &= 0.6692 \\times \\frac{1399.942}{4.2121} \\\\\n&= 222.417\\end{aligned}\\]\n\n\n\nClick here for details on deriving the equations for slope and intercept which is easy if you know multivariate calculus."
  },
  {
    "objectID": "slides/02-slr-intro.html#estimating-the-intercept",
    "href": "slides/02-slr-intro.html#estimating-the-intercept",
    "title": "Simple Linear Regression",
    "section": "Estimating the intercept",
    "text": "Estimating the intercept\n\\[\\large{\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X}}\\]\n\n\n\\[\\begin{aligned}\n&\\bar{x} = 12.2076 \\\\\n&\\bar{y} = 2604.133 \\\\\n&\\hat{\\beta}_1 = 222.4167\n\\end{aligned}\\]\n\n\\[\\begin{aligned}\\hat{\\beta}_0 &= 2604.133 - 222.4167 \\times 12.2076 \\\\\n&= -111.0411\n\\end{aligned}\\]\n\n\n\nClick here for details on deriving the equations for slope and intercept."
  },
  {
    "objectID": "slides/02-slr-intro.html#interpretation",
    "href": "slides/02-slr-intro.html#interpretation",
    "title": "Simple Linear Regression",
    "section": "Interpretation",
    "text": "Interpretation\n\nSlope: For each additional unit of \\(X\\) we expect the \\(Y\\) to increase by \\(\\hat{\\beta}_1\\), on average.\nIntercept: If \\(X\\) were 0, we predict \\(Y\\) to be \\(\\hat{\\beta}_0\\)"
  },
  {
    "objectID": "slides/02-slr-intro.html#does-it-make-sense-to-interpret-the-intercept",
    "href": "slides/02-slr-intro.html#does-it-make-sense-to-interpret-the-intercept",
    "title": "Simple Linear Regression",
    "section": "Does it make sense to interpret the intercept?",
    "text": "Does it make sense to interpret the intercept?\n✅ The intercept is meaningful in the context of the data if\n\nthe predictor can feasibly take values equal to or near zero, or\nthere are values near zero in the observed data.\n\n\n🛑 Otherwise, the intercept may not be meaningful!"
  },
  {
    "objectID": "slides/02-slr-intro.html#estimating-the-regression-line-in-r",
    "href": "slides/02-slr-intro.html#estimating-the-regression-line-in-r",
    "title": "Simple Linear Regression",
    "section": "Estimating the regression line in R",
    "text": "Estimating the regression line in R\n\nLet’s complete Exercises 7-11"
  },
  {
    "objectID": "slides/02-slr-intro.html#fit-model-estimate-parameters",
    "href": "slides/02-slr-intro.html#fit-model-estimate-parameters",
    "title": "Simple Linear Regression",
    "section": "Fit model & estimate parameters",
    "text": "Fit model & estimate parameters\n\nwinter_fit &lt;- lm(count ~ temp_orig, data = winter)\nwinter_fit\n\n\nCall:\nlm(formula = count ~ temp_orig, data = winter)\n\nCoefficients:\n(Intercept)    temp_orig  \n     -111.0        222.4"
  },
  {
    "objectID": "slides/02-slr-intro.html#look-at-the-regression-output",
    "href": "slides/02-slr-intro.html#look-at-the-regression-output",
    "title": "Simple Linear Regression",
    "section": "Look at the regression output",
    "text": "Look at the regression output\n\nwinter_fit &lt;- lm(count ~ temp_orig, data = winter)\nwinter_fit\n\n\nCall:\nlm(formula = count ~ temp_orig, data = winter)\n\nCoefficients:\n(Intercept)    temp_orig  \n     -111.0        222.4  \n\n\n\\[\\widehat{\\text{count}} = -111.0 + 222.4 \\times \\text{temp_orig}\\]\n\n\nNote: The intercept is off by a tiny bit from the hand-calculated intercept, this is just due to rounding in the hand calculation."
  },
  {
    "objectID": "slides/02-slr-intro.html#the-regression-output",
    "href": "slides/02-slr-intro.html#the-regression-output",
    "title": "Simple Linear Regression",
    "section": "The regression output",
    "text": "The regression output\nWe’ll focus on the first column for now…\n\nwinter_fit |&gt; \n  tidy() \n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    -111.     238.     -0.466 6.42e- 1\n2 temp_orig       222.      18.5    12.0   7.28e-25"
  },
  {
    "objectID": "slides/02-slr-intro.html#format-output-with-kable",
    "href": "slides/02-slr-intro.html#format-output-with-kable",
    "title": "Simple Linear Regression",
    "section": "Format output with kable",
    "text": "Format output with kable\nUse the kable function from the knitr package to produce a table and specify number of significant digits\n\nwinter_fit |&gt; \n  tidy() |&gt;\n  kable(digits = 4)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-111.0380\n238.3124\n-0.4659\n0.6418\n\n\ntemp_orig\n222.4155\n18.4594\n12.0489\n0.0000"
  },
  {
    "objectID": "slides/02-slr-intro.html#visualize-model",
    "href": "slides/02-slr-intro.html#visualize-model",
    "title": "Simple Linear Regression",
    "section": "Visualize Model",
    "text": "Visualize Model\n\nwinter |&gt; \n  gf_point(count ~ temp_orig) |&gt; \n  gf_lm()"
  },
  {
    "objectID": "slides/02-slr-intro.html#our-model",
    "href": "slides/02-slr-intro.html#our-model",
    "title": "Simple Linear Regression",
    "section": "Our Model",
    "text": "Our Model\n\\[\\begin{aligned}\n\\widehat{Y} &=  -111.0 + 222.4 \\times X\\\\\n\n\\widehat{\\text{count}} &=  -111.0 + 222.4 \\times \\text{temp_orig}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/02-slr-intro.html#making-a-prediction",
    "href": "slides/02-slr-intro.html#making-a-prediction",
    "title": "Simple Linear Regression",
    "section": "Making a prediction",
    "text": "Making a prediction\nSuppose that it’s 15 degrees Celsius outside. According to this model, how many bike rentals should we expect if it’s winter?\n\\[\\begin{aligned}\n\\widehat{\\text{count}} &= -111.0 + 222.4 \\times \\text{temp_orig} \\\\\n&= -111.0 + 222.4 \\times 15 \\\\\n&= 3225\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/02-slr-intro.html#prediction-in-r",
    "href": "slides/02-slr-intro.html#prediction-in-r",
    "title": "Simple Linear Regression",
    "section": "Prediction in R",
    "text": "Prediction in R\n\n# create a data frame for a new temperature\nnew_day &lt;- tibble(temp_orig = 15)\n\n# predict the outcome for a new day\npredict(winter_fit, new_day)\n\n       1 \n3225.195 \n\n\nComplete Exercises 12-13."
  },
  {
    "objectID": "slides/02-slr-intro.html#recap",
    "href": "slides/02-slr-intro.html#recap",
    "title": "Simple Linear Regression",
    "section": "Recap",
    "text": "Recap\n\n\nUsed simple linear regression to describe the relationship between a quantitative predictor and quantitative response variable.\nUsed the least squares method to estimate the slope and intercept.\nInterpreted the slope and intercept.\n\nSlope: For every one unit increase in \\(x\\), we expect y to change by \\(\\hat{\\beta}_1\\) units, on average.\nIntercept: If \\(x\\) is 0, then we expect \\(y\\) to be \\(\\hat{\\beta}_0\\) units\n\nPredicted the response given a value of the predictor variable.\nUsed lm and the broom package to fit and summarize regression models in R."
  },
  {
    "objectID": "slides/04-slr-math-models.html#topics",
    "href": "slides/04-slr-math-models.html#topics",
    "title": "SLR: Mathematical models for inference",
    "section": "Topics",
    "text": "Topics\n\nDefine mathematical models to conduct inference for the slope\nUse mathematical models to\n\ncalculate confidence interval for the slope\nconduct a hypothesis test for the slope\ncalculate confidence intervals for predictions"
  },
  {
    "objectID": "slides/04-slr-math-models.html#computational-setup",
    "href": "slides/04-slr-math-models.html#computational-setup",
    "title": "SLR: Mathematical models for inference",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(broom)       # for formatting model output\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(kableExtra)  # also for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# Spotify Dataset\nspotify &lt;- read_csv(\"../data/spotify-popular.csv\")\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/04-slr-math-models.html#the-regression-model-revisited",
    "href": "slides/04-slr-math-models.html#the-regression-model-revisited",
    "title": "SLR: Mathematical models for inference",
    "section": "The regression model, revisited",
    "text": "The regression model, revisited\n\nspotify_fit &lt;- lm(danceability ~ duration_ms, data = spotify)\n\ntidy(spotify_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.781\n0.028\n28.351\n0.000\n\n\nduration_ms\n0.000\n0.000\n-3.151\n0.002"
  },
  {
    "objectID": "slides/04-slr-math-models.html#inference-revisited",
    "href": "slides/04-slr-math-models.html#inference-revisited",
    "title": "SLR: Mathematical models for inference",
    "section": "Inference, revisited",
    "text": "Inference, revisited\n\nEarlier we computed a confidence interval and conducted a hypothesis test via simulation:\n\nCI: Bootstrap the observed sample to simulate the distribution of the slope\nHT: Permute the observed sample to simulate the distribution of the slope under the assumption that the null hypothesis is true\n\nNow we’ll do these based on theoretical results, i.e., by using the Central Limit Theorem to define the distribution of the slope and use features (shape, center, spread) of this distribution to compute bounds of the confidence interval and the p-value for the hypothesis test"
  },
  {
    "objectID": "slides/04-slr-math-models.html#mathematical-representation-of-the-model",
    "href": "slides/04-slr-math-models.html#mathematical-representation-of-the-model",
    "title": "SLR: Mathematical models for inference",
    "section": "Mathematical representation of the model",
    "text": "Mathematical representation of the model\n\\[\n\\begin{aligned}\nY &= Model + Error \\\\\n&= f(X) + \\epsilon \\\\\n&= \\mu_{Y|X} + \\epsilon \\\\\n&= \\beta_0 + \\beta_1 X + \\epsilon\n\\end{aligned}\n\\]\nwhere the errors are independent and normally distributed:\n\n\nindependent: Knowing the error term for one observation doesn’t tell you anything about the error term for another observation\nnormally distributed: \\(\\epsilon \\sim N(0, \\sigma_\\epsilon^2)\\)"
  },
  {
    "objectID": "slides/04-slr-math-models.html#mathematical-representation-visualized",
    "href": "slides/04-slr-math-models.html#mathematical-representation-visualized",
    "title": "SLR: Mathematical models for inference",
    "section": "Mathematical representation, visualized",
    "text": "Mathematical representation, visualized\n\\[\nY|X \\sim N(\\beta_0 + \\beta_1 X, \\sigma_\\epsilon^2)\n\\]\n\n\n\n\n\nImage source: Introduction to the Practice of Statistics (5th ed)\n\n\n\n\nMean: \\(\\beta_0 + \\beta_1 X\\), the predicted value based on the regression model\nVariance: \\(\\sigma_\\epsilon^2\\), constant across the range of \\(X\\)\n\nHow do we estimate \\(\\sigma_\\epsilon^2\\)?"
  },
  {
    "objectID": "slides/04-slr-math-models.html#regression-standard-error",
    "href": "slides/04-slr-math-models.html#regression-standard-error",
    "title": "SLR: Mathematical models for inference",
    "section": "Regression standard error",
    "text": "Regression standard error\nOnce we fit the model, we can use the residuals to estimate the regression standard error, the average distance between the observed values and the regression line\n\\[\n\\hat{\\sigma}_\\epsilon = \\sqrt{\\frac{\\sum_\\limits{i=1}^n(y_i - \\hat{y}_i)^2}{n-2}} = \\sqrt{\\frac{\\sum_\\limits{i=1}^ne_i^2}{n-2}}\n\\]\n\n\nWe divide by \\(n - 2\\) because we have \\(n-2\\) degrees of freedom\n\n\nWhy do we care about the value of the regression standard error?"
  },
  {
    "objectID": "slides/04-slr-math-models.html#standard-error-of-hatbeta_1",
    "href": "slides/04-slr-math-models.html#standard-error-of-hatbeta_1",
    "title": "SLR: Mathematical models for inference",
    "section": "Standard error of \\(\\hat{\\beta}_1\\)",
    "text": "Standard error of \\(\\hat{\\beta}_1\\)\nThe standard error of \\(\\hat{\\beta}_1\\) quantifies the sampling variability in the estimated slopes\n\\[\nSE_{\\hat{\\beta}_1} = \\hat{\\sigma}_\\epsilon\\sqrt{\\frac{1}{(n-1)s_X^2}}\n\\]\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n7.809197e-01\n2.754508e-02\n2.835061e+01\n0.000000e+00\n\n\nduration_ms\n-4.039000e-07\n1.282000e-07\n-3.150955e+00\n1.723739e-03"
  },
  {
    "objectID": "slides/04-slr-math-models.html#hypothesis-test-for-the-slope",
    "href": "slides/04-slr-math-models.html#hypothesis-test-for-the-slope",
    "title": "SLR: Mathematical models for inference",
    "section": "Hypothesis test for the slope",
    "text": "Hypothesis test for the slope\nHypotheses: \\(H_0: \\beta_1 = 0\\) vs. \\(H_A: \\beta_1 \\ne 0\\)\n\nTest statistic: Number of standard errors the estimate is away from the null\n\\[\nT = \\frac{\\text{Estimate - Null Value}}{\\text{Standard error}} \\\\\n\\]\n\n\np-value: Probability of observing a test statistic at least as extreme (in the direction of the alternative hypothesis) from the null value as the one observed\n\\[\n\\text{p-value} = P(|T| &gt; |\\text{test statistic}|),\n\\]\ncalculated from a \\(t\\) distribution with \\(n - 2\\) degrees of freedom"
  },
  {
    "objectID": "slides/04-slr-math-models.html#hypothesis-test-test-statistic",
    "href": "slides/04-slr-math-models.html#hypothesis-test-test-statistic",
    "title": "SLR: Mathematical models for inference",
    "section": "Hypothesis test: Test statistic",
    "text": "Hypothesis test: Test statistic\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n7.809197e-01\n2.754508e-02\n2.835061e+01\n0.000000e+00\n\n\nduration_ms\n-4.039000e-07\n1.282000e-07\n-3.150955e+00\n1.723739e-03\n\n\n\n\n\n\\[\nT = \\frac{\\hat{\\beta}_1 - 0}{SE_{\\hat{\\beta}_1}} = \\frac{-4.04\\times 10^{-7} - 0}{1.28\\times 10^{-7}} \\approx -3.15\n\\]\nOur observed slope, \\(\\hat{\\beta_1} = -4.04\\times 10^{-7}\\), is \\(3.15\\) standard errors below what we would expect if there were no linear relationship between duration_ms and danceability.\n\nComplete Exercise 2 and 3."
  },
  {
    "objectID": "slides/04-slr-math-models.html#hypothesis-test-p-value",
    "href": "slides/04-slr-math-models.html#hypothesis-test-p-value",
    "title": "SLR: Mathematical models for inference",
    "section": "Hypothesis test: p-value",
    "text": "Hypothesis test: p-value\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n7.809197e-01\n2.754508e-02\n2.835061e+01\n0.000000e+00\n\n\nduration_ms\n-4.039000e-07\n1.282000e-07\n-3.150955e+00\n1.723739e-03"
  },
  {
    "objectID": "slides/04-slr-math-models.html#hypothesis-test-p-value-1",
    "href": "slides/04-slr-math-models.html#hypothesis-test-p-value-1",
    "title": "SLR: Mathematical models for inference",
    "section": "Hypothesis test: p-value",
    "text": "Hypothesis test: p-value\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n7.809197e-01\n2.754508e-02\n2.835061e+01\n0.000000e+00\n\n\nduration_ms\n-4.039000e-07\n1.282000e-07\n-3.150955e+00\n1.723739e-03\n\n\n\n\n\n\nThe probability of obtaining an observed slope providing stronger evidence for the alternative hypothesis, if we assume that the null hypothesis is true, is \\(1.72\\times 10^{-3}\\)."
  },
  {
    "objectID": "slides/04-slr-math-models.html#understanding-the-p-value",
    "href": "slides/04-slr-math-models.html#understanding-the-p-value",
    "title": "SLR: Mathematical models for inference",
    "section": "Understanding the p-value",
    "text": "Understanding the p-value\n\n\n\nMagnitude of p-value\nInterpretation\n\n\n\n\np-value &lt; 0.01\nstrong evidence against \\(H_0\\)\n\n\n0.01 &lt; p-value &lt; 0.05\nmoderate evidence against \\(H_0\\)\n\n\n0.05 &lt; p-value &lt; 0.1\nweak evidence against \\(H_0\\)\n\n\np-value &gt; 0.1\neffectively no evidence against \\(H_0\\)\n\n\n\n\n\n\n\n\n\nImportant\n\n\nThese are general guidelines. The strength of evidence depends on the context of the problem."
  },
  {
    "objectID": "slides/04-slr-math-models.html#hypothesis-test-conclusion-in-context",
    "href": "slides/04-slr-math-models.html#hypothesis-test-conclusion-in-context",
    "title": "SLR: Mathematical models for inference",
    "section": "Hypothesis test: Conclusion, in context",
    "text": "Hypothesis test: Conclusion, in context\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n7.809197e-01\n2.754508e-02\n2.835061e+01\n0.000000e+00\n\n\nduration_ms\n-4.039000e-07\n1.282000e-07\n-3.150955e+00\n1.723739e-03\n\n\n\n\n\n\nThe data provide convincing evidence that the population slope \\(\\beta_1\\) is different from 0.\nThe data provide convincing evidence of a linear relationship between the duration of a song and it’s danceability.\n\n\nComplete Exercises 4-7."
  },
  {
    "objectID": "slides/04-slr-math-models.html#confidence-interval-for-the-slope",
    "href": "slides/04-slr-math-models.html#confidence-interval-for-the-slope",
    "title": "SLR: Mathematical models for inference",
    "section": "Confidence interval for the slope",
    "text": "Confidence interval for the slope\n\\[\n\\text{Estimate} \\pm \\text{ (critical value) } \\times \\text{SE}\n\\]\n\n\\[\n\\hat{\\beta}_1 \\pm t^* \\times SE_{\\hat{\\beta}_1}\n\\]\nwhere \\(t^*\\) is calculated from a \\(t\\) distribution with \\(n-2\\) degrees of freedom"
  },
  {
    "objectID": "slides/04-slr-math-models.html#confidence-interval-critical-value",
    "href": "slides/04-slr-math-models.html#confidence-interval-critical-value",
    "title": "SLR: Mathematical models for inference",
    "section": "Confidence interval: Critical value",
    "text": "Confidence interval: Critical value\n\n\n\n\n# confidence level: 95%\nqt(0.975, df = nrow(spotify) - 2)\n\n[1] 1.964663\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# confidence level: 90%\nqt(0.95, df = nrow(spotify) - 2)\n\n[1] 1.647871\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# confidence level: 99%\nqt(0.995, df = nrow(spotify) - 2)\n\n[1] 2.58558"
  },
  {
    "objectID": "slides/04-slr-math-models.html#ci-for-the-slope-calculation",
    "href": "slides/04-slr-math-models.html#ci-for-the-slope-calculation",
    "title": "SLR: Mathematical models for inference",
    "section": "95% CI for the slope: Calculation",
    "text": "95% CI for the slope: Calculation\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n7.809197e-01\n2.754508e-02\n2.835061e+01\n0.000000e+00\n\n\nduration_ms\n-4.039000e-07\n1.282000e-07\n-3.150955e+00\n1.723739e-03\n\n\n\n\n\n\\[\\hat{\\beta}_1 = -4.04\\times 10^{-7} \\hspace{15mm} t^* = 1.96  \\hspace{15mm} SE_{\\hat{\\beta}_1} = 1.28\\times 10^{-7}\\]\n\n\\[\n-4.04\\times 10^{-7} \\pm 1.96 \\times 1.28\\times 10^{-7} = (-6.55\\times 10^{-7}, -1.53\\times 10^{-7})\n\\]"
  },
  {
    "objectID": "slides/04-slr-math-models.html#ci-for-the-slope-computation",
    "href": "slides/04-slr-math-models.html#ci-for-the-slope-computation",
    "title": "SLR: Mathematical models for inference",
    "section": "95% CI for the slope: Computation",
    "text": "95% CI for the slope: Computation\n\ntidy(spotify_fit, conf.int = TRUE, conf.level = 0.95) |&gt;\n   kable(digits = 10, format.args = list(scientifi = TRUE))|&gt;\n  row_spec(2, background = \"#D9E3E4\")\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n7.809197e-01\n2.754508e-02\n2.835061e+01\n0.000000e+00\n7.268029e-01\n8.350365e-01\n\n\nduration_ms\n-4.039000e-07\n1.282000e-07\n-3.150955e+00\n1.723739e-03\n-6.557000e-07\n-1.520000e-07\n\n\n\n\n\nWe are 95% confident that, as the duration of a song increases by 1 millisecond, the danceability of that song will decrease by \\(1.53\\times 10^{-7}\\) to \\(6.56\\times 10^{-7}\\) units.\n\nComplete Exercises 7-10."
  },
  {
    "objectID": "slides/04-slr-math-models.html#intervals-for-predictions-1",
    "href": "slides/04-slr-math-models.html#intervals-for-predictions-1",
    "title": "SLR: Mathematical models for inference",
    "section": "Intervals for predictions",
    "text": "Intervals for predictions\n\nQuestion: “What is the predicted danceability for a 3 minute (180,000 ms) song?”\nWe said reporting a single estimate for the slope is not wise, and we should report a plausible range instead\nSimilarly, reporting a single prediction for a new value is not wise, and we should report a plausible range instead"
  },
  {
    "objectID": "slides/04-slr-math-models.html#two-types-of-predictions",
    "href": "slides/04-slr-math-models.html#two-types-of-predictions",
    "title": "SLR: Mathematical models for inference",
    "section": "Two types of predictions",
    "text": "Two types of predictions\n\nPrediction for the mean: “What is the average predicted danceability for three minute songs?”\nPrediction for an individual observation: “What is the predicted danceability for the new Magdala Bay song which is exactly three minutes long?”\n\n\n\nWhich would you expect to be more variable? The average prediction or the prediction for an individual observation? Based on your answer, how would you expect the widths of plausible ranges for these two predictions to compare?"
  },
  {
    "objectID": "slides/04-slr-math-models.html#uncertainty-in-predictions",
    "href": "slides/04-slr-math-models.html#uncertainty-in-predictions",
    "title": "SLR: Mathematical models for inference",
    "section": "Uncertainty in predictions",
    "text": "Uncertainty in predictions\nConfidence interval for the mean outcome: \\[\\large{\\hat{y} \\pm t_{n-2}^* \\times \\color{purple}{\\mathbf{SE}_{\\hat{\\boldsymbol{\\mu}}}}}\\]\n\nPrediction interval for an individual observation: \\[\\large{\\hat{y} \\pm t_{n-2}^* \\times \\color{purple}{\\mathbf{SE_{\\hat{y}}}}}\\]"
  },
  {
    "objectID": "slides/04-slr-math-models.html#standard-errors",
    "href": "slides/04-slr-math-models.html#standard-errors",
    "title": "SLR: Mathematical models for inference",
    "section": "Standard errors",
    "text": "Standard errors\nStandard error of the mean outcome: \\[SE_{\\hat{\\mu}} = \\hat{\\sigma}_\\epsilon\\sqrt{\\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]\n\nStandard error of an individual outcome: \\[SE_{\\hat{y}} = \\hat{\\sigma}_\\epsilon\\sqrt{1 + \\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]"
  },
  {
    "objectID": "slides/04-slr-math-models.html#standard-errors-1",
    "href": "slides/04-slr-math-models.html#standard-errors-1",
    "title": "SLR: Mathematical models for inference",
    "section": "Standard errors",
    "text": "Standard errors\nStandard error of the mean outcome: \\[SE_{\\hat{\\mu}} = \\hat{\\sigma}_\\epsilon\\sqrt{\\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]\nStandard error of an individual outcome: \\[SE_{\\hat{y}} = \\hat{\\sigma}_\\epsilon\\sqrt{\\mathbf{\\color{purple}{\\Large{1}}} + \\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]"
  },
  {
    "objectID": "slides/04-slr-math-models.html#confidence-interval",
    "href": "slides/04-slr-math-models.html#confidence-interval",
    "title": "SLR: Mathematical models for inference",
    "section": "Confidence interval",
    "text": "Confidence interval\nThe 95% confidence interval for the mean outcome:\n\nnew_songs &lt;- tibble(duration_ms = 180000)\n\npredict(spotify_fit, newdata = new_songs, interval = \"confidence\", \n        level = 0.95) |&gt;\n  kable()\n\n\n\n\nfit\nlwr\nupr\n\n\n\n\n0.7082244\n0.6945675\n0.7218813\n\n\n\n\n\n\nWe are 95% confident that the mean danceability for new popular three minute songs on Spotify will be between 0.69 and 0.72."
  },
  {
    "objectID": "slides/04-slr-math-models.html#prediction-interval",
    "href": "slides/04-slr-math-models.html#prediction-interval",
    "title": "SLR: Mathematical models for inference",
    "section": "Prediction interval",
    "text": "Prediction interval\nThe 95% prediction interval for an individual outcome:\n\nnew_song &lt;- tibble(duration_ms = 180000)\n\npredict(spotify_fit, newdata = new_song, interval = \"prediction\", \n        level = 0.95) |&gt;\n  kable()\n\n\n\n\nfit\nlwr\nupr\n\n\n\n\n0.7082244\n0.4518935\n0.9645553\n\n\n\n\n\n\nWe are 95% confident that the new Magdala Bay song that is exactly three minutes long will have a danceability between 0.45 and 0.96."
  },
  {
    "objectID": "slides/04-slr-math-models.html#comparing-intervals",
    "href": "slides/04-slr-math-models.html#comparing-intervals",
    "title": "SLR: Mathematical models for inference",
    "section": "Comparing intervals",
    "text": "Comparing intervals\n\n\nComplete Exercises 11 and 12."
  },
  {
    "objectID": "slides/04-slr-math-models.html#extrapolation",
    "href": "slides/04-slr-math-models.html#extrapolation",
    "title": "SLR: Mathematical models for inference",
    "section": "Extrapolation",
    "text": "Extrapolation\nUsing the model to predict for values outside the range of the original data is extrapolation.\n\n\n\n\nCalculate the prediction interval for the danceability of a song which is 20 minutes (1.2 Million milliseconds) song.\n\nNo, thanks!\n\n\n\n\nAI Generated Image of “Funny people dancing. Add in silly dance moves and make them look tired.”"
  },
  {
    "objectID": "slides/04-slr-math-models.html#extrapolation-1",
    "href": "slides/04-slr-math-models.html#extrapolation-1",
    "title": "SLR: Mathematical models for inference",
    "section": "Extrapolation",
    "text": "Extrapolation\n\nWhy do we want to avoid extrapolation?"
  },
  {
    "objectID": "slides/04-slr-math-models.html#be-like-omg-housing-prices-are-going-to-increase-forever-your-credit-doesnt-matter",
    "href": "slides/04-slr-math-models.html#be-like-omg-housing-prices-are-going-to-increase-forever-your-credit-doesnt-matter",
    "title": "SLR: Mathematical models for inference",
    "section": "2005 be like: OMG HOUSING PRICES ARE GOING TO INCREASE FOREVER, YOUR CREDIT DOESN’T MATTER",
    "text": "2005 be like: OMG HOUSING PRICES ARE GOING TO INCREASE FOREVER, YOUR CREDIT DOESN’T MATTER"
  },
  {
    "objectID": "slides/04-slr-math-models.html#be-like-lol-no",
    "href": "slides/04-slr-math-models.html#be-like-lol-no",
    "title": "SLR: Mathematical models for inference",
    "section": "2007 be like: LOL NO",
    "text": "2007 be like: LOL NO"
  },
  {
    "objectID": "slides/04-slr-math-models.html#recap",
    "href": "slides/04-slr-math-models.html#recap",
    "title": "SLR: Mathematical models for inference",
    "section": "Recap",
    "text": "Recap\n\nLearned how to use mathematical models to\n\ncalculate confidence interval for the slope\nconduct a hypothesis test for the slope\ncalculate confidence intervals for predictions\n\npredictions of averages: “confidence intervals”\npredictions for single observations: “prediction intervals”"
  },
  {
    "objectID": "slides/06-slr-transformations.html#application-exercise",
    "href": "slides/06-slr-transformations.html#application-exercise",
    "title": "SLR: Transformations",
    "section": "Application exercise",
    "text": "Application exercise\n\n📋 AE 06 - Transformations + Outliers\n\nComplete Exercises 0 and 1."
  },
  {
    "objectID": "slides/06-slr-transformations.html#computational-set-up",
    "href": "slides/06-slr-transformations.html#computational-set-up",
    "title": "SLR: Transformations",
    "section": "Computational set up",
    "text": "Computational set up\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(broom)       # for formatting model output\nlibrary(ggformula)   # for creating plots using formulas\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(moderndive)  # for house_price dataset\nlibrary(fivethirtyeight)   # for fandango dataset\nlibrary(kableExtra)  # also for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/06-slr-transformations.html#data-house_prices",
    "href": "slides/06-slr-transformations.html#data-house_prices",
    "title": "SLR: Transformations",
    "section": "Data: house_prices",
    "text": "Data: house_prices\n\nContains house sale prices for King County, which includes Seattle, from homes sold between May 2014 and May 2015\nObtained from Kaggle.com\nImported from the moderndive package\n\n\nglimpse(house_prices)\n\nRows: 21,613\nColumns: 21\n$ id            &lt;chr&gt; \"7129300520\", \"6414100192\", \"5631500400\", \"2487200875\", …\n$ date          &lt;date&gt; 2014-10-13, 2014-12-09, 2015-02-25, 2014-12-09, 2015-02…\n$ price         &lt;dbl&gt; 221900, 538000, 180000, 604000, 510000, 1225000, 257500,…\n$ bedrooms      &lt;int&gt; 3, 3, 2, 4, 3, 4, 3, 3, 3, 3, 3, 2, 3, 3, 5, 4, 3, 4, 2,…\n$ bathrooms     &lt;dbl&gt; 1.00, 2.25, 1.00, 3.00, 2.00, 4.50, 2.25, 1.50, 1.00, 2.…\n$ sqft_living   &lt;int&gt; 1180, 2570, 770, 1960, 1680, 5420, 1715, 1060, 1780, 189…\n$ sqft_lot      &lt;int&gt; 5650, 7242, 10000, 5000, 8080, 101930, 6819, 9711, 7470,…\n$ floors        &lt;dbl&gt; 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1…\n$ waterfront    &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ view          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0,…\n$ condition     &lt;fct&gt; 3, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3, 3, 4, 4,…\n$ grade         &lt;fct&gt; 7, 7, 6, 7, 8, 11, 7, 7, 7, 7, 8, 7, 7, 7, 7, 9, 7, 7, 7…\n$ sqft_above    &lt;int&gt; 1180, 2170, 770, 1050, 1680, 3890, 1715, 1060, 1050, 189…\n$ sqft_basement &lt;int&gt; 0, 400, 0, 910, 0, 1530, 0, 0, 730, 0, 1700, 300, 0, 0, …\n$ yr_built      &lt;int&gt; 1955, 1951, 1933, 1965, 1987, 2001, 1995, 1963, 1960, 20…\n$ yr_renovated  &lt;int&gt; 0, 1991, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ zipcode       &lt;fct&gt; 98178, 98125, 98028, 98136, 98074, 98053, 98003, 98198, …\n$ lat           &lt;dbl&gt; 47.5112, 47.7210, 47.7379, 47.5208, 47.6168, 47.6561, 47…\n$ long          &lt;dbl&gt; -122.257, -122.319, -122.233, -122.393, -122.045, -122.0…\n$ sqft_living15 &lt;int&gt; 1340, 1690, 2720, 1360, 1800, 4760, 2238, 1650, 1780, 23…\n$ sqft_lot15    &lt;int&gt; 5650, 7639, 8062, 5000, 7503, 101930, 6819, 9711, 8113, …"
  },
  {
    "objectID": "slides/06-slr-transformations.html#variables",
    "href": "slides/06-slr-transformations.html#variables",
    "title": "SLR: Transformations",
    "section": "Variables",
    "text": "Variables\n\nOutcome\n\nprice: the sale price in dollars\n\nPredictor\n\nsqft_living: the square footage of the home"
  },
  {
    "objectID": "slides/06-slr-transformations.html#recap-fit-the-model",
    "href": "slides/06-slr-transformations.html#recap-fit-the-model",
    "title": "SLR: Transformations",
    "section": "Recap: Fit the model",
    "text": "Recap: Fit the model\n\nhp_fit &lt;- lm(price ~ sqft_living, data = house_prices)\ntidy(hp_fit) |&gt;  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-43580.74\n4402.69\n-9.90\n0\n\n\nsqft_living\n280.62\n1.94\n144.92\n0\n\n\n\n\n\n\nWrite down the model:\n\nModel: \\(\\hat{\\text{price}} = -43580.74 + 280.62\\times\\text{sqft_living}\\)\n\nInterpret the slope and intercept in the context of this problem:\n\nInterpretation: If the square footage of the house increases by 1, the price increases by and average of $280.62 and a (theoretical) house with 0 square feet with cost -$43,580.74."
  },
  {
    "objectID": "slides/06-slr-transformations.html#recap-fit-the-model-1",
    "href": "slides/06-slr-transformations.html#recap-fit-the-model-1",
    "title": "SLR: Transformations",
    "section": "Recap: Fit the model",
    "text": "Recap: Fit the model\n\n\nCode\ngf_point(price ~ sqft_living, data = house_prices,\n         alpha = 0.25, size = 0.01) |&gt; \n  gf_smooth(method = \"lm\", color = \"red\") |&gt; \n  gf_labs(x = \"Square Footage\", \n       y = \"Sale Price\")  |&gt; \n  gf_refine(scale_y_continuous(labels = label_dollar()),\n  scale_x_continuous(labels = label_number()))"
  },
  {
    "objectID": "slides/06-slr-transformations.html#recap-model-conditions",
    "href": "slides/06-slr-transformations.html#recap-model-conditions",
    "title": "SLR: Transformations",
    "section": "Recap: Model conditions",
    "text": "Recap: Model conditions\n\nLinearity: There is a linear relationship between the outcome and predictor variables\nConstant variance: The variability of the errors is equal for all values of the predictor variable\nNormality: The errors follow a normal distribution\nIndependence: The errors are independent from each other\n\n\n\nHow should we check these assumptions?"
  },
  {
    "objectID": "slides/06-slr-transformations.html#recap-residual-histogram",
    "href": "slides/06-slr-transformations.html#recap-residual-histogram",
    "title": "SLR: Transformations",
    "section": "Recap: Residual Histogram",
    "text": "Recap: Residual Histogram\n\n\nCode\nhp_aug &lt;- augment(hp_fit)\n\ngf_histogram(~.resid, data = hp_aug, bins = 100) |&gt; \n  gf_labs(x = \"Residual\", \n       y = \"Count\", \n       title = \"Residual Histogram\")"
  },
  {
    "objectID": "slides/06-slr-transformations.html#recap-qq-plot-of-residuals",
    "href": "slides/06-slr-transformations.html#recap-qq-plot-of-residuals",
    "title": "SLR: Transformations",
    "section": "Recap: QQ-Plot of Residuals",
    "text": "Recap: QQ-Plot of Residuals\n\n\nCode\ngf_qq(~.resid, data = hp_aug) |&gt; \n  gf_qqline() |&gt;\n  gf_labs(x = \"Theoretical quantile\", \n       y = \"Observed quantile\", \n       title = \"Normal QQ-plot of residuals\")"
  },
  {
    "objectID": "slides/06-slr-transformations.html#recap-residuals-vs.-fitted-values",
    "href": "slides/06-slr-transformations.html#recap-residuals-vs.-fitted-values",
    "title": "SLR: Transformations",
    "section": "Recap: Residuals vs. Fitted Values",
    "text": "Recap: Residuals vs. Fitted Values\n\n\nCode\ngf_point(.resid ~ .fitted, data = hp_aug, \n         alpha = 0.25, size = 0.01) |&gt; \n  gf_hline(yintercept = 0, linetype = \"dashed\") |&gt; \n  gf_labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Residuals vs. fitted values\"\n  )"
  },
  {
    "objectID": "slides/06-slr-transformations.html#are-model-conditions-satisfied",
    "href": "slides/06-slr-transformations.html#are-model-conditions-satisfied",
    "title": "SLR: Transformations",
    "section": "Are model conditions satisfied?",
    "text": "Are model conditions satisfied?\n\nLinearity: ❓\nConstant variance: ❌\nNormality: ❌\nIndependence: ✅"
  },
  {
    "objectID": "slides/06-slr-transformations.html#what-to-do-when-regression-conditions-are-violated",
    "href": "slides/06-slr-transformations.html#what-to-do-when-regression-conditions-are-violated",
    "title": "SLR: Transformations",
    "section": "What to do when regression conditions are violated",
    "text": "What to do when regression conditions are violated\nExamples:\n\nLack of normality in residuals\nPatterns in residuals\nHeteroscedasticity (non-constant variance)\nOutliers: influential points, large residuals"
  },
  {
    "objectID": "slides/06-slr-transformations.html#data-transformations",
    "href": "slides/06-slr-transformations.html#data-transformations",
    "title": "SLR: Transformations",
    "section": "Data Transformations",
    "text": "Data Transformations\nCan be used to:\n\nAddress nonlinear patterns\nStabilize variance\nRemove skewness from residuals\nMinimize effects of outliers"
  },
  {
    "objectID": "slides/06-slr-transformations.html#common-transformations",
    "href": "slides/06-slr-transformations.html#common-transformations",
    "title": "SLR: Transformations",
    "section": "Common Transformations",
    "text": "Common Transformations\nFor either the response \\(Y\\) or predictor \\(X\\):\n\nLogarithm \\(Z \\to \\log(Z)\\)\n\nNote: “log” means “log base \\(e\\)”\n\nSquare Root \\(Z \\to \\sqrt{Z}\\)\nExponential \\(Z \\to e^Z\\)\nPower functions \\(Z \\to Z^2, Z^3, Z^4, \\ldots\\)\nReciprocal \\(Z \\to 1/Z\\)"
  },
  {
    "objectID": "slides/06-slr-transformations.html#general-approach",
    "href": "slides/06-slr-transformations.html#general-approach",
    "title": "SLR: Transformations",
    "section": "General Approach",
    "text": "General Approach\n\nFix non-constant variance be transforming \\(Y\\) (do this first)\n\nFan Pattern: Log-Transform \\(Y\\)\n\nFix non-linearity by transforming \\(X\\)"
  },
  {
    "objectID": "slides/06-slr-transformations.html#why-a-log-transformation",
    "href": "slides/06-slr-transformations.html#why-a-log-transformation",
    "title": "SLR: Transformations",
    "section": "Why a Log Transformation?",
    "text": "Why a Log Transformation?\n\nSome relationship are multiplicative (not linear)\n\nExample: Area of a circle\n\\[\n\\begin{aligned}\nA &= \\pi r^2 \\text{ (not linear)}\\\\\n\\log(A) &= \\log(\\pi r^2)\n= \\log(\\pi) + 2\\log(r)\\\\\n\\log(A) &= \\beta_0 + \\beta_1\\times \\log(r)\\\\\n\\implies & \\log(A) \\text{ is a linear function of } \\log(r)\n\\end{aligned}\n\\]\nLook for:\n\nIncreasing variability in scatterplot\nStrongly right-skewed residual distributions\nComplete Exercise 2"
  },
  {
    "objectID": "slides/06-slr-transformations.html#fixing-non-linearity",
    "href": "slides/06-slr-transformations.html#fixing-non-linearity",
    "title": "SLR: Transformations",
    "section": "Fixing non-linearity",
    "text": "Fixing non-linearity\n\nMany departures from linearity can be solved with power transformations (e.g. \\(X^{power}\\))\n\nFor technical reasons, \\(power = 0\\) corresponds to \\(\\log\\)\n\nConcave down pattern \\(\\Rightarrow\\) transform down (i.e. \\(power &lt; 1\\))\n\n\\(\\log\\) is typically a good first choice\n\nConcave up pattern \\(\\Rightarrow\\) transform up (i.e. \\(power &gt; 1\\))\nComplete Exercises 3-5."
  },
  {
    "objectID": "slides/06-slr-transformations.html#back-to-house_sales",
    "href": "slides/06-slr-transformations.html#back-to-house_sales",
    "title": "SLR: Transformations",
    "section": "Back to house_sales",
    "text": "Back to house_sales\n\n\nCode\np1 &lt;- gf_point(price ~ sqft_living, data = house_prices,\n         alpha = 0.25, size = 0.01) |&gt; \n  gf_smooth(method = \"lm\", color = \"red\") |&gt; \n  gf_labs(x = \"Square Footage\", \n       y = \"Sale Price\")  |&gt; \n  gf_refine(scale_y_continuous(labels = label_dollar()),\n  scale_x_continuous(labels = label_number()))\n\np2 &lt;- gf_point(log(price) ~ sqft_living, data = house_prices,\n         alpha = 0.25, size = 0.01) |&gt; \n  gf_smooth(method = \"lm\", color = \"red\") |&gt; \n  gf_labs(x = \"Square Footage\", \n       y = \"log(Sale Price)\")  |&gt; \n  gf_refine(scale_y_continuous(labels = label_dollar()),\n  scale_x_continuous(labels = label_number()))\n\np3 &lt;- gf_point(price ~ log(sqft_living), data = house_prices,\n         alpha = 0.25, size = 0.01) |&gt; \n  gf_smooth(method = \"lm\", color = \"red\") |&gt; \n  gf_labs(x = \"log(Square Footage)\", \n       y = \"Sale Price\")  |&gt; \n  gf_refine(scale_y_continuous(labels = label_dollar()),\n  scale_x_continuous(labels = label_number()))\n\np4 &lt;- gf_point(log(price) ~ log(sqft_living), data = house_prices,\n         alpha = 0.25, size = 0.01) |&gt; \n  gf_smooth(method = \"lm\", color = \"red\") |&gt; \n  gf_labs(x = \"log(Square Footage)\", \n       y = \"log(Sale Price)\")  |&gt; \n  gf_refine(scale_y_continuous(labels = label_dollar()),\n  scale_x_continuous(labels = label_number()))\n\n(p1 + p2)/ (p3 + p4)"
  },
  {
    "objectID": "slides/06-slr-transformations.html#fitting-transformed-models",
    "href": "slides/06-slr-transformations.html#fitting-transformed-models",
    "title": "SLR: Transformations",
    "section": "Fitting Transformed Models",
    "text": "Fitting Transformed Models\n\n\n\nlogprice_model &lt;- lm(log(price) ~ sqft_living, data = house_prices)\ntidy(logprice_model) |&gt; kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n12.2184641\n0.0063741\n1916.8830\n0\n\n\nsqft_living\n0.0003987\n0.0000028\n142.2326\n0\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\log(Y) &= 12.22  + 3.99\\times 10^{-4}\\times X\\\\\nY &= e^{12.22 + 3.99\\times 10^{-4}\\times X}\\\\\n&= 202805\\times e^{3.99\\times 10^{-4}\\times X}\n\\end{aligned}\n\\]\n\n\nloglog_model &lt;- lm(log(price) ~ log(sqft_living), data = house_prices)\ntidy(loglog_model) |&gt; kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n6.729916\n0.0470620\n143.0011\n0\n\n\nlog(sqft_living)\n0.836771\n0.0062233\n134.4587\n0\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\log(Y) &=6.73 + 0.837\\times \\log(X)\\\\\n\\log(Y) &= \\log(e^{6.73})  + \\log(X^{0.837})\\\\\nY &= 873.15\\times X^{0.837}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/06-slr-transformations.html#residuals-histograms",
    "href": "slides/06-slr-transformations.html#residuals-histograms",
    "title": "SLR: Transformations",
    "section": "Residuals Histograms",
    "text": "Residuals Histograms\n\n\nCode\nlp_aug &lt;- augment(logprice_model)\nll_aug &lt;- augment(loglog_model)\n\np1 &lt;- gf_histogram(~.resid, data = lp_aug, bins = 100) |&gt; \n  gf_labs(x = \"Residual\", \n       y = \"Count\", \n       title = \"Log Price Residuals\")\n\np2 &lt;- gf_histogram(~.resid, data = ll_aug, bins = 100) |&gt; \n  gf_labs(x = \"Residual\", \n       y = \"Count\", \n       title = \"Log-Log Residuals\")\n\n(p1 + p2)"
  },
  {
    "objectID": "slides/06-slr-transformations.html#qq-plots-of-residuals",
    "href": "slides/06-slr-transformations.html#qq-plots-of-residuals",
    "title": "SLR: Transformations",
    "section": "QQ-Plots of Residuals",
    "text": "QQ-Plots of Residuals\n\n\nCode\np1 &lt;- gf_qq(~.resid, data = lp_aug) |&gt; \n  gf_qqline() |&gt;\n  gf_labs(x = \"Theoretical quantile\", \n       y = \"Observed quantile\", \n       title = \"Log Price QQ\")\n\np2 &lt;- gf_qq(~.resid, data = ll_aug) |&gt; \n  gf_qqline() |&gt;\n  gf_labs(x = \"Theoretical quantile\", \n       y = \"Observed quantile\", \n       title = \"Log-Log QQ\")\n\np1 + p2"
  },
  {
    "objectID": "slides/06-slr-transformations.html#residuals-vs.-fitted-values",
    "href": "slides/06-slr-transformations.html#residuals-vs.-fitted-values",
    "title": "SLR: Transformations",
    "section": "Residuals vs. Fitted Values",
    "text": "Residuals vs. Fitted Values\n\n\nCode\np1 &lt;- gf_point(.resid ~ .fitted, data = lp_aug, \n         alpha = 0.25, size = 0.01) |&gt; \n  gf_hline(yintercept = 0, linetype = \"dashed\") |&gt; \n  gf_labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Log Price Model\"\n  )\n\np2 &lt;- gf_point(.resid ~ .fitted, data = ll_aug, \n         alpha = 0.25, size = 0.01) |&gt; \n  gf_hline(yintercept = 0, linetype = \"dashed\") |&gt; \n  gf_labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Log-Log Model\"\n  )\n\np1 + p2"
  },
  {
    "objectID": "slides/06-slr-transformations.html#a-note-on-evaluation",
    "href": "slides/06-slr-transformations.html#a-note-on-evaluation",
    "title": "SLR: Transformations",
    "section": "A note on evaluation",
    "text": "A note on evaluation\nIf you are computing your evaluation metrics (e.g. \\(R^2\\) or RMSE), you should transform your predictions BACK to their original scale, especially if you’re trying to choose the best model\n\n\nWhy do we need to undo the transformation for evaluation metrics by not residuals plots?\nWhy don’t we need to worry about the predictors?"
  },
  {
    "objectID": "slides/06-slr-transformations.html#types-of-unusual-points-in-slr",
    "href": "slides/06-slr-transformations.html#types-of-unusual-points-in-slr",
    "title": "SLR: Transformations",
    "section": "Types of “Unusual” Points in SLR",
    "text": "Types of “Unusual” Points in SLR\n\nOutlier: a data point that is far from the regression line\nInfluential point: a data point that has a large effect on the regression fit\n\n\n\n\nHow do we measure “far”?\nHow do we measure “effect on the fit”?"
  },
  {
    "objectID": "slides/06-slr-transformations.html#detecting-unusual-cases-overview",
    "href": "slides/06-slr-transformations.html#detecting-unusual-cases-overview",
    "title": "SLR: Transformations",
    "section": "Detecting Unusual Cases: Overview",
    "text": "Detecting Unusual Cases: Overview\n\nCompute residuals\n\n“raw”, standardized, studentized\n\nPlots of residuals\n\nboxplot, scatterplot, normal plot\n\nLeverage\n\nunusual values for the predictors"
  },
  {
    "objectID": "slides/06-slr-transformations.html#example-movie-scores",
    "href": "slides/06-slr-transformations.html#example-movie-scores",
    "title": "SLR: Transformations",
    "section": "Example: Movie scores",
    "text": "Example: Movie scores\n\n\n\nData behind the FiveThirtyEight story Be Suspicious Of Online Movie Ratings, Especially Fandango’s\nIn the fivethirtyeight package: fandango\nContains every film released in 2014 and 2015 that has at least 30 fan reviews on Fandango, an IMDb score, Rotten Tomatoes critic and user ratings, and Metacritic critic and user scores"
  },
  {
    "objectID": "slides/06-slr-transformations.html#data-prep",
    "href": "slides/06-slr-transformations.html#data-prep",
    "title": "SLR: Transformations",
    "section": "Data prep",
    "text": "Data prep\n\nRename Rotten Tomatoes columns as critics and audience\nRename the dataset as movie_scores\n\n\ndata(\"fandango\")\n\nmovie_scores &lt;- fandango |&gt;\n  rename(critics = rottentomatoes, \n         audience = rottentomatoes_user)"
  },
  {
    "objectID": "slides/06-slr-transformations.html#example-movie-scores-1",
    "href": "slides/06-slr-transformations.html#example-movie-scores-1",
    "title": "SLR: Transformations",
    "section": "Example: Movie Scores",
    "text": "Example: Movie Scores\n\n\nCode\nmovie_scores |&gt; \n  gf_point(audience ~ critics) |&gt; \n  gf_lm() |&gt; \n  gf_labs(x = \"Critics Score\", \n       y = \"Audience Score\")"
  },
  {
    "objectID": "slides/06-slr-transformations.html#boxplot-of-residuals",
    "href": "slides/06-slr-transformations.html#boxplot-of-residuals",
    "title": "SLR: Transformations",
    "section": "Boxplot of Residuals",
    "text": "Boxplot of Residuals\n\n\n\nmovie_fit &lt;- lm(audience ~ critics, data = movie_scores)\nmovie_fit_aug &lt;- augment(movie_fit)\n\ngf_boxplot(.resid ~ \"\", data = movie_fit_aug, \n           fill = \"salmon\", ylab = \"Residuals\", xlab = \"\")\n\n\n\n\n\n\n\n\n\n\n\nDots (outliers) indicate data points more than 1.5 IQRs above (or below) quartiles"
  },
  {
    "objectID": "slides/06-slr-transformations.html#standardized-residuals",
    "href": "slides/06-slr-transformations.html#standardized-residuals",
    "title": "SLR: Transformations",
    "section": "Standardized Residuals",
    "text": "Standardized Residuals\n\nRecall: Z-scores\nFact: If \\(X\\) has mean \\(\\mu\\) and standard deviation \\(\\sigma\\), then \\((X-\\mu)/\\sigma\\) has mean 0 and standard deviation 1\nFor residuals: mean 0 and standard deviation \\(\\hat{\\sigma}_\\epsilon\\)\nStandardized residuals: \\(\\frac{y_i-\\hat{y}_i}{\\hat{\\sigma}_\\epsilon}\\)\n\nLook for values beyond \\(\\pm 2\\) or \\(\\pm 3\\)"
  },
  {
    "objectID": "slides/06-slr-transformations.html#recap-augment-function",
    "href": "slides/06-slr-transformations.html#recap-augment-function",
    "title": "SLR: Transformations",
    "section": "Recap: Augment function",
    "text": "Recap: Augment function\n\nmovie_fit_aug |&gt; \n  head() |&gt; \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\naudience\ncritics\n.fitted\n.resid\n.hat\n.sigma\n.cooksd\n.std.resid\n\n\n\n\n86\n74\n70.69768\n15.302321\n0.0081597\n12.51615\n0.0061774\n1.2254688\n\n\n80\n85\n76.40313\n3.596866\n0.0112688\n12.57830\n0.0004743\n0.2885034\n\n\n90\n80\n73.80975\n16.190255\n0.0096283\n12.50817\n0.0081839\n1.2975389\n\n\n84\n18\n41.65173\n42.348272\n0.0207618\n12.06226\n0.1234982\n3.4131653\n\n\n28\n14\n39.57702\n-11.577018\n0.0234805\n12.54373\n0.0104964\n-0.9343768\n\n\n62\n63\n64.99222\n-2.992225\n0.0068844\n12.57943\n0.0001988\n-0.2394750"
  },
  {
    "objectID": "slides/06-slr-transformations.html#example-movie-scores-2",
    "href": "slides/06-slr-transformations.html#example-movie-scores-2",
    "title": "SLR: Transformations",
    "section": "Example: Movie Scores",
    "text": "Example: Movie Scores\n\n\nCode\np1 &lt;- movie_fit_aug |&gt;  # Augmented data\n  gf_boxplot(\"\" ~ .std.resid, \n           xlab = \"Standardized Residual\")\n\np2 &lt;- movie_fit_aug |&gt;  # Augmented data\n  gf_point(.std.resid ~ .fitted, \n           xlab = \"Predicted\", ylab = \"Standardized Residual\")\n\np1 + p2"
  },
  {
    "objectID": "slides/06-slr-transformations.html#externally-studentized-residuals",
    "href": "slides/06-slr-transformations.html#externally-studentized-residuals",
    "title": "SLR: Transformations",
    "section": "(Externally) Studentized Residuals",
    "text": "(Externally) Studentized Residuals\n\nConcern: An unusual value may exert great influence on the fit\n\nIts residual might be underestimated because the model “moves” a lot to fit it\nThe standard error may also be inflated due to the outlier error\n\nStudentize: Fit the model without that case, then find new \\(\\hat{\\sigma}_\\epsilon\\)"
  },
  {
    "objectID": "slides/06-slr-transformations.html#example-movie-scores-3",
    "href": "slides/06-slr-transformations.html#example-movie-scores-3",
    "title": "SLR: Transformations",
    "section": "Example: Movie Scores",
    "text": "Example: Movie Scores\n\nmovie_fit_aug |&gt;  # Augmented data\n  mutate(studentized_residual = rstudent(movie_fit)) |&gt; \n  gf_point(studentized_residual ~ .fitted, \n           xlab = \"Predicted\", ylab = \"Studentized Residual\")"
  },
  {
    "objectID": "slides/06-slr-transformations.html#what-to-do-with-an-outlier",
    "href": "slides/06-slr-transformations.html#what-to-do-with-an-outlier",
    "title": "SLR: Transformations",
    "section": "What to do with an outlier?",
    "text": "What to do with an outlier?\n\nLook into it\nIf something is unusual about it and you can make a case that it is not a good representation of the population you can throw it out\nIf not and the value is just unusual, keep it"
  },
  {
    "objectID": "slides/06-slr-transformations.html#influence-vs.-leverage",
    "href": "slides/06-slr-transformations.html#influence-vs.-leverage",
    "title": "SLR: Transformations",
    "section": "Influence vs. Leverage",
    "text": "Influence vs. Leverage\n\nHigh Influence Point: point that DOES impact the regression line\nHigh Leverage Point: point with “potential” to impact regression line because \\(X\\)-value is unusual"
  },
  {
    "objectID": "slides/06-slr-transformations.html#high-leverage-low-influence",
    "href": "slides/06-slr-transformations.html#high-leverage-low-influence",
    "title": "SLR: Transformations",
    "section": "High Leverage, Low Influence",
    "text": "High Leverage, Low Influence"
  },
  {
    "objectID": "slides/06-slr-transformations.html#high-leverage-high-influence",
    "href": "slides/06-slr-transformations.html#high-leverage-high-influence",
    "title": "SLR: Transformations",
    "section": "High Leverage, High Influence",
    "text": "High Leverage, High Influence"
  },
  {
    "objectID": "slides/06-slr-transformations.html#low-leverage-low-influence",
    "href": "slides/06-slr-transformations.html#low-leverage-low-influence",
    "title": "SLR: Transformations",
    "section": "Low Leverage, Low Influence",
    "text": "Low Leverage, Low Influence"
  },
  {
    "objectID": "slides/06-slr-transformations.html#low-leverage-high-influence",
    "href": "slides/06-slr-transformations.html#low-leverage-high-influence",
    "title": "SLR: Transformations",
    "section": "Low Leverage, High Influence",
    "text": "Low Leverage, High Influence"
  },
  {
    "objectID": "slides/06-slr-transformations.html#low-leverage-high-influence-1",
    "href": "slides/06-slr-transformations.html#low-leverage-high-influence-1",
    "title": "SLR: Transformations",
    "section": "Low Leverage, High Influence",
    "text": "Low Leverage, High Influence"
  },
  {
    "objectID": "slides/06-slr-transformations.html#recap",
    "href": "slides/06-slr-transformations.html#recap",
    "title": "SLR: Transformations",
    "section": "Recap",
    "text": "Recap\n\nTransformations\n\nTransform \\(Y\\) to fix non-constant variance (and non-normality)\nTransform \\(X\\) to fix non-linearity\nPower transformations are powerful (concave up/drown \\(\\Rightarrow\\) power up/down)\nlogs allow us to model a lot of non-linear relationships with a linear model\n\nOutliers\n\nLeverage\nInfluence\nUsed plots of residuals, standardized residuals, and studentized residuals to diagnose outliers\n\nSpend the rest of class working on Exercise 6."
  },
  {
    "objectID": "slides/08-mlr-inference.html#application-exercise",
    "href": "slides/08-mlr-inference.html#application-exercise",
    "title": "MLR: Inference and Conditions",
    "section": "Application Exercise",
    "text": "Application Exercise\n\n📋 AE 08 - Inference for MLR\n\nOpen up AE 08 and complete Exercises 0-2"
  },
  {
    "objectID": "slides/08-mlr-inference.html#topics",
    "href": "slides/08-mlr-inference.html#topics",
    "title": "MLR: Inference and Conditions",
    "section": "Topics",
    "text": "Topics\n\nInference for multiple linear regression\nChecking model conditions"
  },
  {
    "objectID": "slides/08-mlr-inference.html#computational-setup",
    "href": "slides/08-mlr-inference.html#computational-setup",
    "title": "MLR: Inference and Conditions",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(mosaic)\nlibrary(mosaicData)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(scales)\nlibrary(GGally)\nlibrary(countdown)\nlibrary(rms)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))"
  },
  {
    "objectID": "slides/08-mlr-inference.html#data-rail_trail",
    "href": "slides/08-mlr-inference.html#data-rail_trail",
    "title": "MLR: Inference and Conditions",
    "section": "Data: rail_trail",
    "text": "Data: rail_trail\n\nThe Pioneer Valley Planning Commission (PVPC) collected data for ninety days from April 5, 2005 to November 15, 2005.\nData collectors set up a laser sensor, with breaks in the laser beam recording when a rail-trail user passed the data collection station.\n\n\n\nRows: 90\nColumns: 7\n$ volume     &lt;dbl&gt; 501, 419, 397, 385, 200, 375, 417, 629, 533, 547, 432, 418,…\n$ hightemp   &lt;dbl&gt; 83, 73, 74, 95, 44, 69, 66, 66, 80, 79, 78, 65, 41, 59, 50,…\n$ avgtemp    &lt;dbl&gt; 66.5, 61.0, 63.0, 78.0, 48.0, 61.5, 52.5, 52.0, 67.5, 62.0,…\n$ season     &lt;chr&gt; \"Summer\", \"Summer\", \"Spring\", \"Summer\", \"Spring\", \"Spring\",…\n$ cloudcover &lt;dbl&gt; 7.6, 6.3, 7.5, 2.6, 10.0, 6.6, 2.4, 0.0, 3.8, 4.1, 8.5, 7.2…\n$ precip     &lt;dbl&gt; 0.00, 0.29, 0.32, 0.00, 0.14, 0.02, 0.00, 0.00, 0.00, 0.00,…\n$ day_type   &lt;chr&gt; \"Weekday\", \"Weekday\", \"Weekday\", \"Weekend\", \"Weekday\", \"Wee…\n\n\nSource: Pioneer Valley Planning Commission via the mosaicData package."
  },
  {
    "objectID": "slides/08-mlr-inference.html#visualizing-the-data",
    "href": "slides/08-mlr-inference.html#visualizing-the-data",
    "title": "MLR: Inference and Conditions",
    "section": "Visualizing the data",
    "text": "Visualizing the data\n\nrail_trail |&gt; ggpairs()"
  },
  {
    "objectID": "slides/08-mlr-inference.html#review-simple-linear-regression-slr",
    "href": "slides/08-mlr-inference.html#review-simple-linear-regression-slr",
    "title": "MLR: Inference and Conditions",
    "section": "Review: Simple linear regression (SLR)",
    "text": "Review: Simple linear regression (SLR)\n\ngf_point(volume ~ avgtemp, data = rail_trail, alpha = 0.5) |&gt;\n  gf_lm()  |&gt;\n  gf_labs(x = \"rail_trail avgtemp\", y = \"avgtemp\")"
  },
  {
    "objectID": "slides/08-mlr-inference.html#slr-model-summary",
    "href": "slides/08-mlr-inference.html#slr-model-summary",
    "title": "MLR: Inference and Conditions",
    "section": "SLR model summary",
    "text": "SLR model summary\n\navgtemp_slr_fit &lt;- lm(volume ~ avgtemp, data = rail_trail)\n\ntidy(avgtemp_slr_fit) |&gt; kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n99.60227\n63.473587\n1.569192\n0.1201920\n\n\navgtemp\n4.80205\n1.084499\n4.427899\n0.0000272"
  },
  {
    "objectID": "slides/08-mlr-inference.html#slr-hypothesis-test",
    "href": "slides/08-mlr-inference.html#slr-hypothesis-test",
    "title": "MLR: Inference and Conditions",
    "section": "SLR hypothesis test",
    "text": "SLR hypothesis test\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n99.60227\n63.473587\n1.569192\n0.1201920\n\n\navgtemp\n4.80205\n1.084499\n4.427899\n0.0000272\n\n\n\n\n\n\nSet hypotheses: \\(H_0: \\beta_1 = 0\\) vs. \\(H_A: \\beta_1 \\ne 0\\)\n\n\n\nCalculate test statistic and p-value: The test statistic is \\(t= 4.43\\) . The p-value is calculated using a \\(t\\) distribution with 88 degrees of freedom. The p-value is \\(2.72\\times 10^{-5}\\).\n\n\n\n\nState the conclusion: The p-value is small, so we reject \\(H_0\\). The data provide strong evidence that avgtemp is a helpful predictor for a rail_trail card holder’s rail_trail volume, i.e. there is a linear relationship between avgtemp and rail_trail volume."
  },
  {
    "objectID": "slides/08-mlr-inference.html#multiple-linear-regression",
    "href": "slides/08-mlr-inference.html#multiple-linear-regression",
    "title": "MLR: Inference and Conditions",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\n\nrail_trail_fit &lt;- lm(volume ~ avgtemp + hightemp, data = rail_trail)\n\ntidy(rail_trail_fit) |&gt; kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n1.667237\n56.424299\n0.0295482\n0.9764951\n\n\navgtemp\n-7.942489\n2.346235\n-3.3852060\n0.0010689\n\n\nhightemp\n12.056606\n2.041231\n5.9065368\n0.0000001"
  },
  {
    "objectID": "slides/08-mlr-inference.html#multiple-linear-regression-1",
    "href": "slides/08-mlr-inference.html#multiple-linear-regression-1",
    "title": "MLR: Inference and Conditions",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nThe multiple linear regression model assumes \\[Y|X_1, X_2,  \\ldots, X_p \\sim N(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p, \\sigma_\\epsilon^2)\\]\n\nFor a given observation \\((x_{i1}, x_{i2}, \\ldots, x_{ip}, y_i)\\), we can rewrite the previous statement as\n\\[y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\epsilon_{i}, \\hspace{10mm} \\epsilon_i \\sim N(0,\\sigma_{\\epsilon}^2)\\]"
  },
  {
    "objectID": "slides/08-mlr-inference.html#estimating-sigma_epsilon",
    "href": "slides/08-mlr-inference.html#estimating-sigma_epsilon",
    "title": "MLR: Inference and Conditions",
    "section": "Estimating \\(\\sigma_\\epsilon\\)",
    "text": "Estimating \\(\\sigma_\\epsilon\\)\nFor a given observation \\((x_{i1}, x_{i2}, \\ldots,x_{ip}, y_i)\\) the residual is \\[\n\\begin{aligned}\ne_i &= y_{i} - \\hat{y_i}\\\\\n&= y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i1} + \\hat{\\beta}_{2} x_{i2} + \\dots + \\hat{\\beta}_p x_{ip})\n\\end{aligned}\n\\]\n\nThe estimated value of the regression standard error , \\(\\sigma_{\\epsilon}\\), is\n\\[\\hat{\\sigma}_\\epsilon  = \\sqrt{\\frac{\\sum_{i=1}^ne_i^2}{n-p-1}}\\]\n\n\nAs with SLR, we use \\(\\hat{\\sigma}_{\\epsilon}\\) to calculate \\(SE_{\\hat{\\beta}_j}\\), the standard error of each coefficient. See Matrix Form of Linear Regression for more detail."
  },
  {
    "objectID": "slides/08-mlr-inference.html#mlr-hypothesis-test-avgtemp",
    "href": "slides/08-mlr-inference.html#mlr-hypothesis-test-avgtemp",
    "title": "MLR: Inference and Conditions",
    "section": "MLR hypothesis test: avgtemp",
    "text": "MLR hypothesis test: avgtemp\n\nSet hypotheses: \\(H_0: \\beta_{avgtemp} = 0\\) vs. \\(H_A: \\beta_{avgtemp} \\ne 0\\), given hightemp is in the model\n\n\n\nCalculate test statistic and p-value: The test statistic is \\(t = -3.39\\). The p-value is calculated using a \\(t\\) distribution with \\[(n - p - 1) = 90 - 2 -1 = 87\\] degrees of freedom. The p-value is \\(\\approx 0.0011\\).\n\nNote that \\(p\\) counts all non-intercept \\(\\beta\\)’s\n\n\n\n\n\nState the conclusion: The p-value is small, so we reject \\(H_0\\). The data provides convincing evidence that a avgtemp is a useful predictor in a model that already contains rail_trail hightemp as a predictor for the rail_trail volume."
  },
  {
    "objectID": "slides/08-mlr-inference.html#mlr-hypothesis-test-interaction-terms",
    "href": "slides/08-mlr-inference.html#mlr-hypothesis-test-interaction-terms",
    "title": "MLR: Inference and Conditions",
    "section": "MLR hypothesis test: interaction terms",
    "text": "MLR hypothesis test: interaction terms\n\nFramework: Same as previous slide except use \\(\\beta\\) of interaction term\n\\(p\\) (the number of predictors) should include the interaction term\nConclusion: tells you whether interaction term is a useful predictor\nWarning: if \\(X_1\\) and \\(X_2\\) have an interaction term, don’t try to interpret their individual p-values… if interaction term is significant, then both variables are important\n\n\nComplete Exercises 3 and 4."
  },
  {
    "objectID": "slides/08-mlr-inference.html#confidence-interval-for-beta_j-1",
    "href": "slides/08-mlr-inference.html#confidence-interval-for-beta_j-1",
    "title": "MLR: Inference and Conditions",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\n\nThe \\(C\\%\\) confidence interval for \\(\\beta_j\\) \\[\\hat{\\beta}_j \\pm t^* SE(\\hat{\\beta}_j)\\] where \\(t^*\\) follows a \\(t\\) distribution with \\(n - p - 1\\) degrees of freedom.\nGenerically: We are \\(C\\%\\) confident that the interval LB to UB contains the population coefficient of \\(x_j\\).\nIn context: We are \\(C\\%\\) confident that for every one unit increase in \\(x_j\\), we expect \\(y\\) to change by LB to UB units, holding all else constant.\n\n\nComplete Exercise 5."
  },
  {
    "objectID": "slides/08-mlr-inference.html#confidence-interval-for-beta_j-2",
    "href": "slides/08-mlr-inference.html#confidence-interval-for-beta_j-2",
    "title": "MLR: Inference and Conditions",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n1.67\n56.42\n0.03\n0.98\n-110.48\n113.82\n\n\navgtemp\n-7.94\n2.35\n-3.39\n0.00\n-12.61\n-3.28\n\n\nhightemp\n12.06\n2.04\n5.91\n0.00\n8.00\n16.11"
  },
  {
    "objectID": "slides/08-mlr-inference.html#large-sample-sizes",
    "href": "slides/08-mlr-inference.html#large-sample-sizes",
    "title": "MLR: Inference and Conditions",
    "section": "Large sample sizes",
    "text": "Large sample sizes\n\n\n\n\n\n\nCaution\n\n\nIf the sample size is large enough, the test will likely result in rejecting \\(H_0: \\beta_j = 0\\) even \\(x_j\\) has a very small effect on \\(y\\).\n\nConsider the practical significance of the result not just the statistical significance.\nInstead of saying statistically significant say statistically detectable\nUse confidence intervals to draw conclusions instead of relying only on p-values."
  },
  {
    "objectID": "slides/08-mlr-inference.html#small-sample-sizes",
    "href": "slides/08-mlr-inference.html#small-sample-sizes",
    "title": "MLR: Inference and Conditions",
    "section": "Small sample sizes",
    "text": "Small sample sizes\n\n\n\n\n\n\nCaution\n\n\nIf the sample size is small, there may not be enough evidence to reject \\(H_0: \\beta_j=0\\).\n\nWhen you fail to reject the null hypothesis, DON’T immediately conclude that the variable has no association with the response.\nThere may be a linear association that is just not strong enough to detect given your data, or there may be a non-linear association."
  },
  {
    "objectID": "slides/08-mlr-inference.html#effect-size-and-power",
    "href": "slides/08-mlr-inference.html#effect-size-and-power",
    "title": "MLR: Inference and Conditions",
    "section": "Effect Size and Power",
    "text": "Effect Size and Power\n\nEffect Size:\n\nGeneral Idea: how big is your parameter compared to the variability of your data\nWhy is it important: if parameter has small effect size, it may not be a useful parameter to include. However, what effect size is useful depends on your problem\nIn practice: many formal ways of determining effects size which we won’t talk about\n\nStatistical Power: the ability of a hypothesis test to detect a given effect size\n\nmore data = more power\ntons of data = tons of power\nIn practice: think about how what effect size you’d like to be able to detect and design your study to collect enough data to do so\n\n\n\nComplete Exercise 6"
  },
  {
    "objectID": "slides/08-mlr-inference.html#model-conditions",
    "href": "slides/08-mlr-inference.html#model-conditions",
    "title": "MLR: Inference and Conditions",
    "section": "Model conditions",
    "text": "Model conditions\nOur model conditions are the same as they were with SLR:\n\nLinearity: There is a linear relationship between the response and predictor variables.\nConstant Variance: The variability about the least squares line is generally constant.\nNormality: The distribution of the residuals is approximately normal.\nIndependence: The residuals are independent from each other."
  },
  {
    "objectID": "slides/08-mlr-inference.html#checking-linearity",
    "href": "slides/08-mlr-inference.html#checking-linearity",
    "title": "MLR: Inference and Conditions",
    "section": "Checking Linearity",
    "text": "Checking Linearity\n\nLook at a plot of the residuals vs. predicted values\nLook at a plot of the residuals vs. each predictor\n\nUse scatter plots for quantitative and boxplots of categorical predictors\n\nLinearity is met if there is no discernible pattern in each of these plots\n\nComplete Exercises 7-10"
  },
  {
    "objectID": "slides/08-mlr-inference.html#fitting-the-full-model",
    "href": "slides/08-mlr-inference.html#fitting-the-full-model",
    "title": "MLR: Inference and Conditions",
    "section": "Fitting the Full Model",
    "text": "Fitting the Full Model\n\nfull_model &lt;- lm(volume ~ . , data = rail_trail)\nfull_model |&gt; tidy() |&gt;  kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n17.622161\n76.582860\n0.2301058\n0.8185826\n\n\nhightemp\n7.070528\n2.420523\n2.9210743\n0.0045045\n\n\navgtemp\n-2.036685\n3.142113\n-0.6481896\n0.5186733\n\n\nseasonSpring\n35.914983\n32.992762\n1.0885716\n0.2795319\n\n\nseasonSummer\n24.153571\n52.810486\n0.4573632\n0.6486195\n\n\ncloudcover\n-7.251776\n3.843071\n-1.8869743\n0.0627025\n\n\nprecip\n-95.696525\n42.573359\n-2.2478030\n0.0272735\n\n\nday_typeWeekend\n35.903750\n22.429056\n1.6007696\n0.1132738"
  },
  {
    "objectID": "slides/08-mlr-inference.html#residuals-vs.-predicted-values",
    "href": "slides/08-mlr-inference.html#residuals-vs.-predicted-values",
    "title": "MLR: Inference and Conditions",
    "section": "Residuals vs. predicted values",
    "text": "Residuals vs. predicted values\n\n\n\n\n\n\n\n\n\n\nDoes the constant variance condition appear to be satisfied?"
  },
  {
    "objectID": "slides/08-mlr-inference.html#checking-constant-variance",
    "href": "slides/08-mlr-inference.html#checking-constant-variance",
    "title": "MLR: Inference and Conditions",
    "section": "Checking constant variance",
    "text": "Checking constant variance\n\nThe vertical spread of the residuals is not constant across the plot.\nThe constant variance condition is not satisfied.\n\n\n\nGiven this conclusion, what might be a next step in the analysis?"
  },
  {
    "objectID": "slides/08-mlr-inference.html#residuals-vs.-each-predictor",
    "href": "slides/08-mlr-inference.html#residuals-vs.-each-predictor",
    "title": "MLR: Inference and Conditions",
    "section": "Residuals vs. each predictor",
    "text": "Residuals vs. each predictor"
  },
  {
    "objectID": "slides/08-mlr-inference.html#checking-linearity-1",
    "href": "slides/08-mlr-inference.html#checking-linearity-1",
    "title": "MLR: Inference and Conditions",
    "section": "Checking linearity",
    "text": "Checking linearity\n\nThe plot of the residuals vs. predicted values looked OK\nThe plots of residuals vs. hightemp and avgtemp appear to have a parabolic pattern.\nThe linearity condition does not appear to be satisfied given these plots.\n\n\n\nGiven this conclusion, what might be a next step in the analysis?\n\nComplete Exercises 11-12."
  },
  {
    "objectID": "slides/08-mlr-inference.html#checking-normality",
    "href": "slides/08-mlr-inference.html#checking-normality",
    "title": "MLR: Inference and Conditions",
    "section": "Checking normality",
    "text": "Checking normality\n\nThe distribution of the residuals is approximately unimodal and symmetric but the qqplot shows that tails are too heavy, so the normality condition is likely not satisfied. The sample size 90 is sufficiently large to relax this condition."
  },
  {
    "objectID": "slides/08-mlr-inference.html#checking-independence",
    "href": "slides/08-mlr-inference.html#checking-independence",
    "title": "MLR: Inference and Conditions",
    "section": "Checking independence",
    "text": "Checking independence\n\nWe can often check the independence condition based on the context of the data and how the observations were collected.\nIf the data were collected in a particular order, examine a scatter plot of the residuals versus order in which the data were collected.\nIf there is a grouping variable lurking in the background, check the residuals based on that grouping variable."
  },
  {
    "objectID": "slides/08-mlr-inference.html#recap",
    "href": "slides/08-mlr-inference.html#recap",
    "title": "MLR: Inference and Conditions",
    "section": "Recap",
    "text": "Recap\n\nInference for individual predictors\n\nHypothesis testing: same as SLR but add if all other terms are in the model\nConfidence intervals: same as SLR but add holding all other variables in the model constant\n\nConditions: Same as for SLR\n\nFor linearity, also check fitter values vs. each predictor"
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#application-exercise",
    "href": "slides/10-logistic-estimation-inference.html#application-exercise",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Application Exercise",
    "text": "Application Exercise\n\n📋 AE 10 - Logistic Regression Inference\n\nOpen up AE 10 and complete Exercise 0."
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#topics",
    "href": "slides/10-logistic-estimation-inference.html#topics",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Topics",
    "text": "Topics\n\nEstimating coefficients in logistic regression\nChecking model conditions for logistic regression\nInference for coefficients in logistic regression"
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#computational-setup",
    "href": "slides/10-logistic-estimation-inference.html#computational-setup",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(ggformula)\nlibrary(openintro)\nlibrary(knitr)\nlibrary(kableExtra)  # for table embellishments\nlibrary(Stat2Data)   # for empirical logit\nlibrary(countdown)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#risk-of-coronary-heart-disease",
    "href": "slides/10-logistic-estimation-inference.html#risk-of-coronary-heart-disease",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Risk of coronary heart disease",
    "text": "Risk of coronary heart disease\nThis data set is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to examine the relationship between various health characteristics and the risk of having heart disease.\n\nTenYearCHD:\n\n1: High risk of having heart disease in next 10 years\n0: Not high risk of having heart disease in next 10 years\n\nage: Age at exam time (in years)\ncurrentSmoker: 0 = nonsmoker, 1 = smoker"
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#data-prep",
    "href": "slides/10-logistic-estimation-inference.html#data-prep",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Data prep",
    "text": "Data prep\n\nheart_disease &lt;- read_csv(\"../data/framingham.csv\") |&gt;\n  select(TenYearCHD, age, currentSmoker) |&gt;\n  drop_na() |&gt;\n  mutate(currentSmoker = as.factor(currentSmoker))\n\nheart_disease |&gt; head() |&gt; kable()\n\n\n\n\nTenYearCHD\nage\ncurrentSmoker\n\n\n\n\n0\n39\n0\n\n\n0\n46\n0\n\n\n0\n48\n1\n\n\n1\n61\n1\n\n\n0\n46\n1\n\n\n0\n43\n0"
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#statistical-model",
    "href": "slides/10-logistic-estimation-inference.html#statistical-model",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Statistical model",
    "text": "Statistical model\nThe form of the statistical model for logistic regression is\n\\[\n\\log\\Big(\\frac{\\pi}{1-\\pi}\\Big) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p\n\\]\nwhere \\(\\pi\\) is the probability \\(Y = 1\\).\n\nNotice there is no error term when writing the statistical model for logistic regression. Why?\n\nThe statistical model is the “data-generating” model\nEach individual observed \\(Y\\) is generated from a Bernoulli distribution, \\(Bernoulli(\\pi)\\)\nTherefore, the randomness is not produced by an error term but rather in the distribution used to generate \\(Y\\)"
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#bernoulli-distribution",
    "href": "slides/10-logistic-estimation-inference.html#bernoulli-distribution",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Bernoulli Distribution",
    "text": "Bernoulli Distribution\n\nThink of two possible outcomes:\n1 = “Success” which occurs with probability \\(\\pi\\)\n0 = “Failure” which occurs with probability \\(1-\\pi\\)\nWe can think of each of our observations as having a Bernoulli distribution with mean \\(\\pi_i\\)\nOur logistic regression model is changing \\(\\pi_i\\) (the probability of success) for each new observation\nThe probability that we got our data, given our model is the truth, is then called the Likelihood \\[L = \\prod_{i=1}^n \\pi_i^{y_i}(1-\\pi_i)^{1-y_i}\\]"
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#log-likelihood-function",
    "href": "slides/10-logistic-estimation-inference.html#log-likelihood-function",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Log Likelihood Function",
    "text": "Log Likelihood Function\nLog-Likelihood Function: the log of the likelihood function is easier to work with and has the same maxima and minima!\n\\[\n\\log L = \\sum\\limits_{i=1}^n[y_i \\log(\\hat{\\pi}_i) + (1 - y_i)\\log(1 - \\hat{\\pi}_i)]\n\\]\nwhere\n\\[\\hat{\\pi} = \\frac{\\exp\\{\\hat{\\beta}_0 + \\hat{\\beta}_1X_1 + \\dots + \\hat{\\beta}_pX_p\\}}{1 + \\exp\\{\\hat{\\beta}_0 + \\hat{\\beta}_1X_1 + \\dots + \\hat{\\beta}_pX_p\\}}\\]\n\n\nThe coefficients \\(\\hat{\\beta}_0, \\ldots, \\hat{\\beta}_p\\) are estimated using maximum likelihood estimation\nBasic idea: Find the values of \\(\\hat{\\beta}_0, \\ldots, \\hat{\\beta}_p\\) that give the observed data the maximum probability of occurring"
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#maximum-likelihood-estimation",
    "href": "slides/10-logistic-estimation-inference.html#maximum-likelihood-estimation",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation\n\nThis is called maximum likelihood estimation and is EXTREMELY common in statistics and data science\nNeed a strong foundation in probability and applied mathematics to fully understand\nLogistic regression: maximum found through numerical methods (clever computer algorithms that approximate the maximum)\nLinear regression: maximum found through calculus\n\n\nComplete Exercise 1.\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#the-models",
    "href": "slides/10-logistic-estimation-inference.html#the-models",
    "title": "Logistic Regression Estimation and Inference",
    "section": "The models",
    "text": "The models\nModel 1: Let’s predict TenYearCHD from currentSmoker:\n\nrisk_fit &lt;- glm(TenYearCHD ~ currentSmoker, \n      data = heart_disease, family = \"binomial\")\n\ntidy(risk_fit, conf.int = TRUE) |&gt; \n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-1.774\n0.061\n-28.936\n0.000\n-1.896\n-1.656\n\n\ncurrentSmoker1\n0.108\n0.086\n1.266\n0.206\n-0.059\n0.276\n\n\n\n\n\nModel 2: Let’s predict TenYearCHD from age:\n\nrisk_fit &lt;- glm(TenYearCHD ~ age, \n      data = heart_disease, family = \"binomial\")\n\ntidy(risk_fit, conf.int = TRUE) |&gt; \n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-5.561\n0.284\n-19.599\n0\n-6.124\n-5.011\n\n\nage\n0.075\n0.005\n14.178\n0\n0.064\n0.085"
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#conditions-for-logistic-regression",
    "href": "slides/10-logistic-estimation-inference.html#conditions-for-logistic-regression",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Conditions for logistic regression",
    "text": "Conditions for logistic regression\n\nLinearity: The log-odds have a linear relationship with the predictors.\nRandomness: The data were obtained from a random process.\nIndependence: The observations are independent from one another."
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#empirical-logit",
    "href": "slides/10-logistic-estimation-inference.html#empirical-logit",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Empirical logit",
    "text": "Empirical logit\nThe empirical logit is the log of the observed odds:\n\\[\n\\text{logit}(\\hat{p}) = \\log\\Big(\\frac{\\hat{p}}{1 - \\hat{p}}\\Big) = \\log\\Big(\\frac{\\# \\text{Yes}}{\\# \\text{No}}\\Big)\n\\]"
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#calculating-empirical-logit-categorical-predictor",
    "href": "slides/10-logistic-estimation-inference.html#calculating-empirical-logit-categorical-predictor",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Calculating empirical logit (categorical predictor)",
    "text": "Calculating empirical logit (categorical predictor)\nIf the predictor is categorical, we can calculate the empirical logit for each level of the predictor.\n\n\n\n\n\ncurrentSmoker\nTenYearCHD\nn\nprop\nemp_logit\n\n\n\n\n0\n1\n311\n0.1449883\n-1.774462\n\n\n1\n1\n333\n0.1589499\n-1.666062\n\n\n\n\n\n\nComplete Exercise 2."
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#calculating-empirical-logit-quantitative-predictor",
    "href": "slides/10-logistic-estimation-inference.html#calculating-empirical-logit-quantitative-predictor",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Calculating empirical logit (quantitative predictor)",
    "text": "Calculating empirical logit (quantitative predictor)\n\nDivide the range of the predictor into intervals with approximately equal number of cases. (If you have enough observations, use 5 - 10 intervals.)\nCompute the empirical logit for each interval\n\n\nYou can then calculate the mean value of the predictor in each interval and create a plot of the empirical logit versus the mean value of the predictor in each interval."
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#empirical-logit-plot-in-r-quantitative-predictor",
    "href": "slides/10-logistic-estimation-inference.html#empirical-logit-plot-in-r-quantitative-predictor",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Empirical logit plot in R (quantitative predictor)",
    "text": "Empirical logit plot in R (quantitative predictor)\nCreated using dplyr and ggplot functions."
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#empirical-logit-plot-in-r-quantitative-predictor-1",
    "href": "slides/10-logistic-estimation-inference.html#empirical-logit-plot-in-r-quantitative-predictor-1",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Empirical logit plot in R (quantitative predictor)",
    "text": "Empirical logit plot in R (quantitative predictor)\nCreated using dplyr and ggformula functions.\n\nheart_disease |&gt; \n  mutate(age_bin = cut_interval(age, n = 10)) |&gt;\n  group_by(age_bin) |&gt;\n  mutate(mean_age = mean(age)) |&gt;\n  count(mean_age, TenYearCHD) |&gt;\n  mutate(prop = n/sum(n)) |&gt;\n  filter(TenYearCHD == \"1\") |&gt;\n  mutate(emp_logit = log(prop/(1-prop))) |&gt;\n  gf_point(emp_logit ~ mean_age)  |&gt;  \n  gf_smooth(method = \"lm\", se = FALSE) |&gt; \n  gf_labs(x = \"Mean Age\", \n       y = \"Empirical logit\")"
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#empirical-logit-plot-in-r-quantitative-predictor-2",
    "href": "slides/10-logistic-estimation-inference.html#empirical-logit-plot-in-r-quantitative-predictor-2",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Empirical logit plot in R (quantitative predictor)",
    "text": "Empirical logit plot in R (quantitative predictor)\nUsing the emplogitplot1 function from the Stat2Data R package\n\nemplogitplot1(TenYearCHD ~ age, \n              data = heart_disease, \n              ngroups = 10)"
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#checking-linearity",
    "href": "slides/10-logistic-estimation-inference.html#checking-linearity",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Checking linearity",
    "text": "Checking linearity\n✅ The linearity condition is satisfied. There is a linear relationship between the empirical logit and the predictor variables.\n\nComplete Exercise 3."
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#checking-randomness",
    "href": "slides/10-logistic-estimation-inference.html#checking-randomness",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Checking randomness",
    "text": "Checking randomness\nWe can check the randomness condition based on the context of the data and how the observations were collected.\n\nWas the sample randomly selected?\nDid the successes and failures occur from a random process?\n\n\n✅ The randomness condition is satisfied. Who does and does not develop heart disease occurs from a random process."
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#checking-independence",
    "href": "slides/10-logistic-estimation-inference.html#checking-independence",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Checking independence",
    "text": "Checking independence\n\nWe can check the independence condition based on the context of the data and how the observations were collected.\nIndependence is most often violated if the data were collected over time or there is a strong spatial relationship between the observations.\n\n\n✅ The independence condition is satisfied. It is reasonable to conclude that the participants’ health characteristics are independent of one another."
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#modeling-risk-of-coronary-heart-disease",
    "href": "slides/10-logistic-estimation-inference.html#modeling-risk-of-coronary-heart-disease",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Modeling risk of coronary heart disease",
    "text": "Modeling risk of coronary heart disease\n\nWhat’s wrong with this code?\n\n\nrisk_fit &lt;- glm(TenYearCHD ~ age, data = heart_disease, \n                family = \"binomial\") |&gt; \n  tidy() |&gt; \n  kable()"
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#modeling-risk-of-coronary-heart-disease-1",
    "href": "slides/10-logistic-estimation-inference.html#modeling-risk-of-coronary-heart-disease-1",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Modeling risk of coronary heart disease",
    "text": "Modeling risk of coronary heart disease\nUsing age:\n\nrisk_fit &lt;- glm(TenYearCHD ~ age, data = heart_disease, \n                family = \"binomial\")\n\nrisk_fit |&gt; tidy() |&gt; kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.5610898\n0.2837460\n-19.59883\n0\n\n\nage\n0.0746501\n0.0052651\n14.17821\n0"
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#recall-inference-for-linear-regression",
    "href": "slides/10-logistic-estimation-inference.html#recall-inference-for-linear-regression",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Recall: Inference for Linear Regression",
    "text": "Recall: Inference for Linear Regression\n\nt-test: determine whether \\(\\beta_1\\) (the slope) is different than zero\nANOVA/F-Test: To test the full model or to compare nested models\nSSModel/SSE/ \\(R^2\\) / \\(\\hat{\\sigma}_{\\epsilon}\\) : metrics to try a measure the amount of variability explained by competing models"
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#hypothesis-test-for-beta_1",
    "href": "slides/10-logistic-estimation-inference.html#hypothesis-test-for-beta_1",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Hypothesis test for \\(\\beta_1\\)",
    "text": "Hypothesis test for \\(\\beta_1\\)\nHypotheses: \\(H_0: \\beta_1 = 0 \\hspace{2mm} \\text{ vs } \\hspace{2mm} H_a: \\beta_1 \\neq 0\\)\n\nTest Statistic: \\[z = \\frac{\\hat{\\beta}_1 - 0}{SE_{\\hat{\\beta}_1}}\\]\n\\(z\\) is sometimes called a Wald statistic and this test is sometimes called a Wald Hypothesis Test.\n\n\nP-value: \\(P(|Z| &gt; |z|)\\), where \\(Z \\sim N(0, 1)\\), the Standard Normal distribution"
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#confidence-interval-for-beta_1",
    "href": "slides/10-logistic-estimation-inference.html#confidence-interval-for-beta_1",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Confidence interval for \\(\\beta_1\\)",
    "text": "Confidence interval for \\(\\beta_1\\)\nWe can calculate the C% confidence interval for \\(\\beta_1\\) as the following:\n\\[\n\\Large{\\hat{\\beta}_1 \\pm z^* SE_{\\hat{\\beta}_1}}\n\\]\nwhere \\(z^*\\) is calculated from the \\(N(0,1)\\) distribution\n\n\n\n\n\n\n\nNote\n\n\nThis is an interval for the change in the log-odds for every one unit increase in \\(x\\)"
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#interpretation-in-terms-of-the-odds",
    "href": "slides/10-logistic-estimation-inference.html#interpretation-in-terms-of-the-odds",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Interpretation in terms of the odds",
    "text": "Interpretation in terms of the odds\nThe change in odds for every one unit increase in \\(x_1\\).\n\\[\n\\Large{\\exp\\{\\hat{\\beta}_1 \\pm z^* SE_{\\hat{\\beta}_1}\\}}\n\\]\n\nInterpretation: We are \\(C\\%\\) confident that for every one unit increase in \\(x_1\\), the odds multiply by a factor of \\(\\exp\\{\\hat{\\beta}_1 - z^* SE_{\\hat{\\beta}_1}\\}\\) to \\(\\exp\\{\\hat{\\beta}_1 + z^* SE_{\\hat{\\beta}_1}\\}\\)."
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#coefficient-for-age",
    "href": "slides/10-logistic-estimation-inference.html#coefficient-for-age",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Coefficient for age",
    "text": "Coefficient for age\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-5.561\n0.284\n-19.599\n0\n-6.124\n-5.011\n\n\nage\n0.075\n0.005\n14.178\n0\n0.064\n0.085\n\n\n\n\n\n\nHypotheses:\n\\[\nH_0: \\beta_{age} = 0 \\hspace{2mm} \\text{ vs } \\hspace{2mm} H_a: \\beta_{age} \\neq 0\n\\]"
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#coefficient-for-age-1",
    "href": "slides/10-logistic-estimation-inference.html#coefficient-for-age-1",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Coefficient for age",
    "text": "Coefficient for age\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-5.561\n0.284\n-19.599\n0\n-6.124\n-5.011\n\n\nage\n0.075\n0.005\n14.178\n0\n0.064\n0.085\n\n\n\n\n\nTest statistic:\n\\[z = \\frac{0.0747 - 0}{0.00527} \\approx 14.178\\]\nNote: rounding errors!"
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#coefficient-for-age-2",
    "href": "slides/10-logistic-estimation-inference.html#coefficient-for-age-2",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Coefficient for age",
    "text": "Coefficient for age\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-5.561\n0.284\n-19.599\n0\n-6.124\n-5.011\n\n\nage\n0.075\n0.005\n14.178\n0\n0.064\n0.085\n\n\n\n\n\nP-value:\n\\[\nP(|Z| &gt; |14.178|) \\approx 0\n\\]\n\n\n2 * pnorm(14.178,lower.tail = FALSE)\n\n[1] 1.253689e-45"
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#coefficient-for-age-3",
    "href": "slides/10-logistic-estimation-inference.html#coefficient-for-age-3",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Coefficient for age",
    "text": "Coefficient for age\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-5.561\n0.284\n-19.599\n0\n-6.124\n-5.011\n\n\nage\n0.075\n0.005\n14.178\n0\n0.064\n0.085\n\n\n\n\n\nConclusion:\nThe p-value is very small, so we reject \\(H_0\\). The data provide sufficient evidence that age is a statistically significant predictor of whether someone will develop heart disease in the next 10 years."
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#ci-for-age",
    "href": "slides/10-logistic-estimation-inference.html#ci-for-age",
    "title": "Logistic Regression Estimation and Inference",
    "section": "CI for age",
    "text": "CI for age\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-5.561\n0.284\n-19.599\n0\n-6.124\n-5.011\n\n\nage\n0.075\n0.005\n14.178\n0\n0.064\n0.085\n\n\n\n\n\nWe are 95% confident that for each additional year of age, the change in the log-odds of someone developing heart disease in the next 10 years is between 0.064 and 0.085.\n\nWe are 95% confident that for each additional year of age, the odds of someone developing heart disease in the next 10 years will increase by a factor of \\(\\exp(0.064) \\approx 1.077\\) to \\(\\exp(0.085)\\approx 1.089\\).\n\n\nComplete Exercises 4-7."
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#recall-inference-for-linear-regression-1",
    "href": "slides/10-logistic-estimation-inference.html#recall-inference-for-linear-regression-1",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Recall: Inference for Linear Regression",
    "text": "Recall: Inference for Linear Regression\n\nt-test: determine whether \\(\\beta_1\\) (the slope) is different than zero\nANOVA/F-Test: To test the full model or to compare nested models\nSSModel/SSE/ \\(R^2\\) / \\(\\hat{\\sigma}_{\\epsilon}\\) : metrics to try a measure the amount of variability explained by competing models"
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#recall-likelihood",
    "href": "slides/10-logistic-estimation-inference.html#recall-likelihood",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Recall: Likelihood",
    "text": "Recall: Likelihood\n\\[\nL = \\prod_{i=1}^n\\hat{\\pi}_i^{y_i}(1 - \\hat{\\pi}_i)^{1 - y_i}\n\\]\n\nIntuition: probability of obtaining our data given a certain set of parameters\n\n\\[\nL(\\hat{\\beta}_0, \\ldots, \\hat{\\beta}_p) = \\prod_{i=1}^n\\hat{\\pi}_i(\\hat{\\beta}_0, \\ldots, \\hat{\\beta}_p)^{y_i}(1 - \\hat{\\pi}_i(\\hat{\\beta}_0, \\ldots, \\hat{\\beta}_p))^{1 - y_i}\n\\]"
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#recall-log-likelihood",
    "href": "slides/10-logistic-estimation-inference.html#recall-log-likelihood",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Recall: Log-Likelihood",
    "text": "Recall: Log-Likelihood\nTaking the log makes the likelihood easier to work with and doesn’t change which \\(\\beta\\)’s maximize it.\n\\[\n\\log L = \\sum\\limits_{i=1}^n[y_i \\log(\\hat{\\pi}_i) + (1 - y_i)\\log(1 - \\hat{\\pi}_i)]\n\\]"
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#log-likelihood-to-deviance",
    "href": "slides/10-logistic-estimation-inference.html#log-likelihood-to-deviance",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Log-Likelihood to Deviance",
    "text": "Log-Likelihood to Deviance\n\nThe log-likelihood measures of how well the model fits the data\nHigher values of \\(\\log L\\) are better\nDeviance = \\(-2 \\log L\\)\n\n\\(-2 \\log L\\) follows a \\(\\chi^2\\) distribution with 1 degree of freedom\nThink of deviace as the analog of the residual sum of squares (SSE) in linear regression"
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#calculate-deviance-for-our-model",
    "href": "slides/10-logistic-estimation-inference.html#calculate-deviance-for-our-model",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Calculate deviance for our model:",
    "text": "Calculate deviance for our model:\nWe can use our trusty ol’ glance function\n\nrisk_fit |&gt;  glance() |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnull.deviance\ndf.null\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n3612.209\n4239\n-1698.305\n3400.61\n3413.314\n3396.61\n4238\n4240\n\n\n\n\n\n\nComplete Exercise 8."
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#comparing-nested-models",
    "href": "slides/10-logistic-estimation-inference.html#comparing-nested-models",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Comparing nested models",
    "text": "Comparing nested models\n\nSuppose there are two models:\n\nReduced Model includes only an intercept \\(\\beta_0\\)\nFull Model includes \\(\\beta_1\\)\n\nWe want to test the hypotheses\n\\[\n\\begin{aligned}\nH_0&: \\beta_{1} = 0\\\\\nH_A&: \\beta_1 \\neq 0\n\\end{aligned}\n\\]\nTo do so, we will use something called a Likelihood Ratio test (LRT), also known as the Drop-in-deviance test"
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#likelihood-ratio-test-lrt",
    "href": "slides/10-logistic-estimation-inference.html#likelihood-ratio-test-lrt",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Likelihood Ratio Test (LRT)",
    "text": "Likelihood Ratio Test (LRT)\nHypotheses:\n\\[\n\\begin{aligned}\nH_0&: \\beta_1 = 0 \\\\\nH_A&: \\beta_1 \\neq 0\n\\end{aligned}\n\\]\n\nTest Statistic: \\[G = (-2 \\log L_{reduced}) - (-2 \\log L_{full})\\]\nSometimes written as \\[G = (-2 \\log L_0) - (-2 \\log L)\\]\n\n\nP-value: \\(P(\\chi^2 &gt; G)\\), calculated using a \\(\\chi^2\\) distribution with 1 degree of freedom"
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#chi2-distribution",
    "href": "slides/10-logistic-estimation-inference.html#chi2-distribution",
    "title": "Logistic Regression Estimation and Inference",
    "section": "\\(\\chi^2\\) distribution",
    "text": "\\(\\chi^2\\) distribution"
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#reduced-model",
    "href": "slides/10-logistic-estimation-inference.html#reduced-model",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Reduced model",
    "text": "Reduced model\nFirst model, reduced:\n\nrisk_fit_reduced &lt;- glm(TenYearCHD ~ 1, \n      data = heart_disease, family = \"binomial\",\n      control = glm.control(epsilon = 1e-20)) # Ignore this line\n\nrisk_fit_reduced |&gt; tidy() |&gt; kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-1.719879\n0.0427888\n-40.19459\n0\n\n\n\n\n\n\nSide bar…\n\n\nProbability predicted by model\n\nexp(coef(risk_fit_reduced)[1])/(1 + exp(coef(risk_fit_reduced)[1]))\n\n(Intercept) \n  0.1518868 \n\n\n\n\nlibrary(mosaic)\ntally(~TenYearCHD, data = heart_disease, format = \"proportion\") |&gt; kable()\n\n\n\n\nTenYearCHD\nFreq\n\n\n\n\n0\n0.8481132\n\n\n1\n0.1518868"
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#should-we-add-age-to-the-model",
    "href": "slides/10-logistic-estimation-inference.html#should-we-add-age-to-the-model",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Should we add age to the model?",
    "text": "Should we add age to the model?\nSecond model, full:\n\nrisk_fit_full &lt;- glm(TenYearCHD ~ age, \n      data = heart_disease, family = \"binomial\")\n\nrisk_fit_full |&gt;  tidy() |&gt; kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.5610898\n0.2837460\n-19.59883\n0\n\n\nage\n0.0746501\n0.0052651\n14.17821\n0"
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#should-we-add-age-to-the-model-1",
    "href": "slides/10-logistic-estimation-inference.html#should-we-add-age-to-the-model-1",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Should we add age to the model?",
    "text": "Should we add age to the model?\nCalculate deviance for each model:\n\ndev_reduced &lt;- glance(risk_fit_reduced)$deviance #Use $ instead of select\n\ndev_full &lt;- glance(risk_fit_full)$deviance\n\ndev_reduced\n\n[1] 3612.209\n\ndev_full\n\n[1] 3396.61"
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#should-we-add-age-to-the-model-2",
    "href": "slides/10-logistic-estimation-inference.html#should-we-add-age-to-the-model-2",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Should we add age to the model?",
    "text": "Should we add age to the model?\nDrop-in-deviance test statistic:\n\ntest_stat &lt;- dev_reduced - dev_full"
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#should-we-add-age-to-the-model-3",
    "href": "slides/10-logistic-estimation-inference.html#should-we-add-age-to-the-model-3",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Should we add age to the model?",
    "text": "Should we add age to the model?\nCalculate the p-value using a pchisq(), with 1 degree of freedom:\n\npchisq(test_stat, 1, lower.tail = FALSE)\n\n[1] 8.249288e-49\n\n\n\nConclusion: The p-value is very small, so we reject \\(H_0\\). The data provide sufficient evidence that the coefficient of age is not equal to 0. Therefore, we should add it to the model.\n\n\nComplete Exercises 9-10."
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#drop-in-deviance-test-in-r",
    "href": "slides/10-logistic-estimation-inference.html#drop-in-deviance-test-in-r",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Drop-in-Deviance test in R",
    "text": "Drop-in-Deviance test in R\n\nWe can use the anova function to conduct this test\nAdd test = \"Chisq\" to conduct the drop-in-deviance test\n\n\n\nanova(risk_fit_reduced, risk_fit_full, test = \"Chisq\") |&gt;\n  tidy() |&gt; kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\nterm\ndf.residual\nresidual.deviance\ndf\ndeviance\np.value\n\n\n\n\nTenYearCHD ~ 1\n4239\n3612.209\nNA\nNA\nNA\n\n\nTenYearCHD ~ age\n4238\n3396.610\n1\n215.599\n0\n\n\n\n\n\nComplete Exercises 11-12."
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#recap-1",
    "href": "slides/10-logistic-estimation-inference.html#recap-1",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Recap",
    "text": "Recap\n\nHow do we fit a logistic regression model?\n\nMaximum likelihood estimation\n\nLogistic regression conditions\n\nLinearity\nRandomness\nIndependence"
  },
  {
    "objectID": "slides/10-logistic-estimation-inference.html#inference-for-beta_1",
    "href": "slides/10-logistic-estimation-inference.html#inference-for-beta_1",
    "title": "Logistic Regression Estimation and Inference",
    "section": "Inference for \\(\\beta_1\\)",
    "text": "Inference for \\(\\beta_1\\)\n\nWald Test\nLikelihood Ratio Test\n\nMore reliable than Wald\nMore computationally taxing\nDeviance: think of like SSE\n\nNext time: Multiple predictors!"
  },
  {
    "objectID": "slides/12-logistic-multiple.html#topics",
    "href": "slides/12-logistic-multiple.html#topics",
    "title": "Multiple Logistic Regression",
    "section": "Topics",
    "text": "Topics\n\nExtending what we’ve learned to logistic regression models with multiple predictors\n\n\n\n📋 AE 12 - Multiple Logistic Regression\n\nComplete all of AE 12 using slides as needed."
  },
  {
    "objectID": "slides/12-logistic-multiple.html#computational-setup",
    "href": "slides/12-logistic-multiple.html#computational-setup",
    "title": "Multiple Logistic Regression",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(openintro)\nlibrary(knitr)\nlibrary(kableExtra)  # for table embellishments\nlibrary(Stat2Data)   # for empirical logit\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/12-logistic-multiple.html#risk-of-coronary-heart-disease",
    "href": "slides/12-logistic-multiple.html#risk-of-coronary-heart-disease",
    "title": "Multiple Logistic Regression",
    "section": "Risk of coronary heart disease",
    "text": "Risk of coronary heart disease\nThis data set is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to examine the relationship between various health characteristics and the risk of having heart disease.\n\nTenYearCHD:\n\n1: Developed heart disease in next 10 years\n0: Did not develop heart disease in next 10 years\n\nage: Age at exam time (in years)\neducation: 1 = Some High School, 2 = High School or GED, 3 = Some College or Vocational School, 4 = College"
  },
  {
    "objectID": "slides/12-logistic-multiple.html#data-prep",
    "href": "slides/12-logistic-multiple.html#data-prep",
    "title": "Multiple Logistic Regression",
    "section": "Data prep",
    "text": "Data prep\n\nheart_disease &lt;- read_csv(\"../data/framingham.csv\") |&gt;\n  select(TenYearCHD, age, education) |&gt;\n  drop_na() |&gt; \n  mutate(education = as.factor(education))\n\nheart_disease |&gt; head() |&gt; kable()\n\n\n\n\nTenYearCHD\nage\neducation\n\n\n\n\n0\n39\n4\n\n\n0\n46\n2\n\n\n0\n48\n1\n\n\n1\n61\n3\n\n\n0\n46\n3\n\n\n0\n43\n2"
  },
  {
    "objectID": "slides/12-logistic-multiple.html#model-output",
    "href": "slides/12-logistic-multiple.html#model-output",
    "title": "Multiple Logistic Regression",
    "section": "Model output",
    "text": "Model output\n\nrisk_fit &lt;- glm(TenYearCHD ~ age + education, \n      data = heart_disease, family = \"binomial\")\n\nrisk_fit |&gt; tidy() |&gt; kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n\n\nage\n0.073\n0.005\n13.385\n0.000\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n\n\n\n\n\n\\[\n\\small{\\log\\Big(\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}\\Big) = -5.385 + 0.073 ~ \\text{age} - 0.242 ~ \\text{ed2} - 0.235 ~ \\text{ed3} - 0.020 ~ \\text{ed4}}\n\\]"
  },
  {
    "objectID": "slides/12-logistic-multiple.html#model-interpretation",
    "href": "slides/12-logistic-multiple.html#model-interpretation",
    "title": "Multiple Logistic Regression",
    "section": "Model Interpretation",
    "text": "Model Interpretation\n\nrisk_fit &lt;- glm(TenYearCHD ~ age + education, \n      data = heart_disease, family = \"binomial\")\n\nrisk_fit |&gt; tidy() |&gt; kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n\n\nage\n0.073\n0.005\n13.385\n0.000\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n\n\n\n\n\nAs age increases by a year, the typical log-odds of developing coronary heart disease within the next 10 years increases by 0.073 for patients with the same level of education.\n\nAs age increases by a year, the typical odds of developing coronary heart disease within the next 10 years increases by a factor of \\(\\exp(0.073)\\approx 1.08\\) (i.e. 8%) for patients with the same level of education."
  },
  {
    "objectID": "slides/12-logistic-multiple.html#model-interpretation-1",
    "href": "slides/12-logistic-multiple.html#model-interpretation-1",
    "title": "Multiple Logistic Regression",
    "section": "Model Interpretation",
    "text": "Model Interpretation\n\nrisk_fit &lt;- glm(TenYearCHD ~ age + education, \n      data = heart_disease, family = \"binomial\")\n\nrisk_fit |&gt; tidy() |&gt; kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n\n\nage\n0.073\n0.005\n13.385\n0.000\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n\n\n\n\n\nPatients of the same age who have [a High School diploma or GED/Some College or Vocational School/a College education], the typical log-odds of developing coronary heart disease within the next 10 years is 0.242/0.235/0.020 lower than patients with only some high school.\n\nPatients of the same age who have [a High School diploma or GED/Some College or Vocational School/a College education], the typical odds of developing coronary heart disease within the next 10 years is 79.5%/79.1%/98.0% of the odds for patients with only some high school."
  },
  {
    "objectID": "slides/12-logistic-multiple.html#hypothesis-test-for-beta_j",
    "href": "slides/12-logistic-multiple.html#hypothesis-test-for-beta_j",
    "title": "Multiple Logistic Regression",
    "section": "Hypothesis test for \\(\\beta_j\\)",
    "text": "Hypothesis test for \\(\\beta_j\\)\nHypotheses: \\(H_0: \\beta_j = 0 \\hspace{2mm} \\text{ vs } \\hspace{2mm} H_a: \\beta_j \\neq 0\\), given the other variables in the model\n\nTest Statistic: \\[z = \\frac{\\hat{\\beta}_j - 0}{SE_{\\hat{\\beta}_j}}\\]\n\n\nP-value: \\(P(|Z| &gt; |z|)\\), where \\(Z \\sim N(0, 1)\\), the Standard Normal distribution"
  },
  {
    "objectID": "slides/12-logistic-multiple.html#confidence-interval-for-beta_j",
    "href": "slides/12-logistic-multiple.html#confidence-interval-for-beta_j",
    "title": "Multiple Logistic Regression",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\nWe can calculate the C% confidence interval for \\(\\beta_j\\) as the following:\n\\[\n\\Large{\\hat{\\beta}_j \\pm z^* SE_{\\hat{\\beta}_j}}\n\\]\nwhere \\(z^*\\) is calculated from the \\(N(0,1)\\) distribution\n\n\n\n\n\n\n\nNote\n\n\nThis is an interval for the change in the log-odds for every one unit increase in \\(x_j\\)"
  },
  {
    "objectID": "slides/12-logistic-multiple.html#interpretation-in-terms-of-the-odds",
    "href": "slides/12-logistic-multiple.html#interpretation-in-terms-of-the-odds",
    "title": "Multiple Logistic Regression",
    "section": "Interpretation in terms of the odds",
    "text": "Interpretation in terms of the odds\nThe change in odds for every one unit increase in \\(x_j\\).\n\\[\n\\Large{\\exp\\{\\hat{\\beta}_j \\pm z^* SE_{\\hat{\\beta}_j}\\}}\n\\]\n\nInterpretation: We are \\(C\\%\\) confident that for every one unit increase in \\(x_j\\), the odds multiply by a factor of \\(\\exp\\{\\hat{\\beta}_j - z^* SE_{\\hat{\\beta}_j}\\}\\) to \\(\\exp\\{\\hat{\\beta}_j + z^* SE_{\\hat{\\beta}_j}\\}\\), holding all else constant."
  },
  {
    "objectID": "slides/12-logistic-multiple.html#coefficient-for-age",
    "href": "slides/12-logistic-multiple.html#coefficient-for-age",
    "title": "Multiple Logistic Regression",
    "section": "Coefficient for age",
    "text": "Coefficient for age\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n-5.995\n-4.788\n\n\nage\n0.073\n0.005\n13.385\n0.000\n0.063\n0.084\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n-0.463\n-0.024\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n-0.501\n0.023\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n-0.317\n0.266\n\n\n\n\n\n\nHypotheses:\n\\[\nH_0: \\beta_{age} = 0 \\hspace{2mm} \\text{ vs } \\hspace{2mm} H_a: \\beta_{age} \\neq 0\n\\] given education is in the model"
  },
  {
    "objectID": "slides/12-logistic-multiple.html#coefficient-for-age-1",
    "href": "slides/12-logistic-multiple.html#coefficient-for-age-1",
    "title": "Multiple Logistic Regression",
    "section": "Coefficient for age",
    "text": "Coefficient for age\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n-5.995\n-4.788\n\n\nage\n0.073\n0.005\n13.385\n0.000\n0.063\n0.084\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n-0.463\n-0.024\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n-0.501\n0.023\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n-0.317\n0.266\n\n\n\n\n\nTest statistic:\n\\[z = \\frac{0.07328 - 0}{0.00547} = 13.39\\]"
  },
  {
    "objectID": "slides/12-logistic-multiple.html#coefficient-for-age-2",
    "href": "slides/12-logistic-multiple.html#coefficient-for-age-2",
    "title": "Multiple Logistic Regression",
    "section": "Coefficient for age",
    "text": "Coefficient for age\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n-5.995\n-4.788\n\n\nage\n0.073\n0.005\n13.385\n0.000\n0.063\n0.084\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n-0.463\n-0.024\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n-0.501\n0.023\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n-0.317\n0.266\n\n\n\n\n\nP-value:\n\\[\nP(|Z| &gt; |13.39|) \\approx 0\n\\]\n\n\n2 * pnorm(13.64,lower.tail = FALSE)\n\n[1] 2.315606e-42"
  },
  {
    "objectID": "slides/12-logistic-multiple.html#coefficient-for-age-3",
    "href": "slides/12-logistic-multiple.html#coefficient-for-age-3",
    "title": "Multiple Logistic Regression",
    "section": "Coefficient for age",
    "text": "Coefficient for age\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n-5.995\n-4.788\n\n\nage\n0.073\n0.005\n13.385\n0.000\n0.063\n0.084\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n-0.463\n-0.024\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n-0.501\n0.023\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n-0.317\n0.266\n\n\n\n\n\nConclusion:\nThe p-value is very small, so we reject \\(H_0\\). The data provide sufficient evidence that age is a statistically significant predictor of whether someone is will develop heart disease in the next year, after accounting for education."
  },
  {
    "objectID": "slides/12-logistic-multiple.html#comparing-nested-models-1",
    "href": "slides/12-logistic-multiple.html#comparing-nested-models-1",
    "title": "Multiple Logistic Regression",
    "section": "Comparing nested models",
    "text": "Comparing nested models\n\nSuppose there are two models:\n\nReduced Model includes predictors \\(x_1, \\ldots, x_q\\)\nFull Model includes predictors \\(x_1, \\ldots, x_q, x_{q+1}, \\ldots, x_p\\)\n\nWe want to test the hypotheses\n\\[\n\\begin{aligned}\nH_0&: \\beta_{q+1} = \\dots = \\beta_p = 0 \\\\\nH_A&: \\text{ at least one }\\beta_j \\text{ is not } 0\n\\end{aligned}\n\\]\nTo do so, we will use the Nested Likelihood Ratio test (LRT), also known as the Drop-in-deviance test,"
  },
  {
    "objectID": "slides/12-logistic-multiple.html#likelihood-ratio-test",
    "href": "slides/12-logistic-multiple.html#likelihood-ratio-test",
    "title": "Multiple Logistic Regression",
    "section": "Likelihood Ratio test",
    "text": "Likelihood Ratio test\nHypotheses:\n\\[\n\\begin{aligned}\nH_0&: \\beta_{q+1} = \\dots = \\beta_p = 0 \\\\\nH_A&: \\text{ at least 1 }\\beta_j \\text{ is not } 0\n\\end{aligned}\n\\]\n\nTest Statistic: \\[G = (-2 \\log L_{reduced}) - (-2 \\log L_{full})\\]\nor sometimes\nTest Statistic: \\[G = (-2 \\log L_{0}) - (-2 \\log L)\\]\n\n\nP-value: \\(P(\\chi^2 &gt; G)\\), calculated using a \\(\\chi^2\\) distribution with degrees of freedom equal to the difference in the number of parameters in the full and reduced models"
  },
  {
    "objectID": "slides/12-logistic-multiple.html#chi2-distribution",
    "href": "slides/12-logistic-multiple.html#chi2-distribution",
    "title": "Multiple Logistic Regression",
    "section": "\\(\\chi^2\\) distribution",
    "text": "\\(\\chi^2\\) distribution"
  },
  {
    "objectID": "slides/12-logistic-multiple.html#should-we-add-education-to-a-model-with-only-age",
    "href": "slides/12-logistic-multiple.html#should-we-add-education-to-a-model-with-only-age",
    "title": "Multiple Logistic Regression",
    "section": "Should we add education to a model with only age?",
    "text": "Should we add education to a model with only age?\nFirst model, reduced:\n\nrisk_fit_reduced &lt;- glm(TenYearCHD ~ age, \n      data = heart_disease, family = \"binomial\")\n\n\n\nSecond model, full:\n\nrisk_fit_full &lt;- glm(TenYearCHD ~ age + education, \n      data = heart_disease, family = \"binomial\")"
  },
  {
    "objectID": "slides/12-logistic-multiple.html#should-we-add-education-to-the-model",
    "href": "slides/12-logistic-multiple.html#should-we-add-education-to-the-model",
    "title": "Multiple Logistic Regression",
    "section": "Should we add education to the model?",
    "text": "Should we add education to the model?\nCalculate deviance for each model:\n\ndev_reduced &lt;- glance(risk_fit_reduced)$deviance\ndev_reduced\n\n[1] 3306.68\n\ndev_full &lt;- glance(risk_fit_full)$deviance\ndev_full\n\n[1] 3300.135"
  },
  {
    "objectID": "slides/12-logistic-multiple.html#should-we-add-education-to-the-model-1",
    "href": "slides/12-logistic-multiple.html#should-we-add-education-to-the-model-1",
    "title": "Multiple Logistic Regression",
    "section": "Should we add education to the model?",
    "text": "Should we add education to the model?\nDrop-in-deviance test statistic:\n\ntest_stat &lt;- dev_reduced - dev_full\ntest_stat\n\n[1] 6.544306"
  },
  {
    "objectID": "slides/12-logistic-multiple.html#should-we-add-education-to-the-model-2",
    "href": "slides/12-logistic-multiple.html#should-we-add-education-to-the-model-2",
    "title": "Multiple Logistic Regression",
    "section": "Should we add education to the model?",
    "text": "Should we add education to the model?\nCalculate the p-value using a pchisq(), with degrees of freedom equal to the number of new model terms in the second model:\n\npchisq(test_stat, 3, lower.tail = FALSE) \n\n[1] 0.08793145\n\n\n\nConclusion: The p-value is between 0.1 and 0.05 indicating mild but not strong evidence that a model with education is a useful predictor when age is already in the model."
  },
  {
    "objectID": "slides/12-logistic-multiple.html#drop-in-deviance-test-in-r",
    "href": "slides/12-logistic-multiple.html#drop-in-deviance-test-in-r",
    "title": "Multiple Logistic Regression",
    "section": "Drop-in-Deviance test in R",
    "text": "Drop-in-Deviance test in R\n\nWe can use the anova function to conduct this test\nAdd test = \"Chisq\" to conduct the drop-in-deviance test\n\n\n\nanova(risk_fit_reduced, risk_fit_full, test = \"Chisq\") |&gt;\n  tidy() |&gt; kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\nterm\ndf.residual\nresidual.deviance\ndf\ndeviance\np.value\n\n\n\n\nTenYearCHD ~ age\n4133\n3306.680\nNA\nNA\nNA\n\n\nTenYearCHD ~ age + education\n4130\n3300.135\n3\n6.544\n0.088"
  },
  {
    "objectID": "slides/12-logistic-multiple.html#recap",
    "href": "slides/12-logistic-multiple.html#recap",
    "title": "Multiple Logistic Regression",
    "section": "Recap",
    "text": "Recap\n\nToday: Simple logistic to Multiple logistic\n\nCoefficient interpretations\nWald test interpretations\nLRTs and Drop-in-Deviance tests\n\nBasic idea add “holding all else constant”\nLRT allows you to compare nested models and requires a degree of free for each extra parameter in the full model"
  },
  {
    "objectID": "support.html",
    "href": "support.html",
    "title": "Course support",
    "section": "",
    "text": "We expect everyone will have questions at some point in the semester, so we want to make sure you can identify when that is and feel comfortable seeking help.",
    "crumbs": [
      "Course support"
    ]
  },
  {
    "objectID": "support.html#lectures",
    "href": "support.html#lectures",
    "title": "Course support",
    "section": "Lectures",
    "text": "Lectures\nIf you have a question during lecture, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.",
    "crumbs": [
      "Course support"
    ]
  },
  {
    "objectID": "support.html#office-hours",
    "href": "support.html#office-hours",
    "title": "Course support",
    "section": "Office hours",
    "text": "Office hours\nI am here to help you be successful in the course. You are encouraged to attend office hours during the times posted on the home page to ask questions about the course content and assignments. A lot of questions are most effectively answered in-person, so office hours are a valuable resource. I encourage you to take advantage of them!\nMake a pledge to stop by office hours at least once before Wednesday’s (1/9) class. If you truly have no questions to ask, just stop by and say hi and introduce yourself.",
    "crumbs": [
      "Course support"
    ]
  },
  {
    "objectID": "support.html#teams-discussion-forum",
    "href": "support.html#teams-discussion-forum",
    "title": "Course support",
    "section": "Teams Discussion Forum",
    "text": "Teams Discussion Forum\nOutside of class and office hours, any general questions about course content or assignments should be posted on the Teams Discussion Forum. If you know the answer to a question that is posted, I encourage you to respond!",
    "crumbs": [
      "Course support"
    ]
  },
  {
    "objectID": "support.html#tutoring",
    "href": "support.html#tutoring",
    "title": "Course support",
    "section": "Tutoring",
    "text": "Tutoring\nBrayden Banks will be tutoring for MAT-212 this winter term. His hours are Monday and Wednesday from 10 am - 11 am in Jewett 100. If this time doesn’t work for you, feel free email him to meet outside these hours.",
    "crumbs": [
      "Course support"
    ]
  },
  {
    "objectID": "support.html#email",
    "href": "support.html#email",
    "title": "Course support",
    "section": "Email",
    "text": "Email\nIf you have questions about personal matters that are not appropriate for the class discussion forum (e.g. illness, accommodations, etc.), you may email Professor Friedlander at efriedlander@collegeofidaho.edu. If you email me, please include “MAT 212” in the subject line. Barring extenuating circumstances, I will respond to MAT 212 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday.",
    "crumbs": [
      "Course support"
    ]
  }
]